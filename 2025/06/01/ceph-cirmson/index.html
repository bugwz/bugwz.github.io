<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Ceph Crimson 设计实现深入解析 | 咕咕</title><meta name="author" content="bugwz"><meta name="copyright" content="bugwz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Crimson 是 Crimson OSD 的代码名称，它是下一代用于多核心可扩展性的 OSD 。它通过快速网络和存储设备提高性能，采用包括 DPDK 和 SPDK 的顶级技术。BlueStore 继续支持 HDD 和 SSD。Crimson 旨在与早期版本的 OSD 守护进程与类 Ceph OSD 兼容。Crimson 基于 SeaStar C++ 框架构建，是核心 Ceph 对象存储守护进程">
<meta property="og:type" content="article">
<meta property="og:title" content="Ceph Crimson 设计实现深入解析">
<meta property="og:url" content="https://bugwz.com/2025/06/01/ceph-cirmson/index.html">
<meta property="og:site_name" content="咕咕">
<meta property="og:description" content="Crimson 是 Crimson OSD 的代码名称，它是下一代用于多核心可扩展性的 OSD 。它通过快速网络和存储设备提高性能，采用包括 DPDK 和 SPDK 的顶级技术。BlueStore 继续支持 HDD 和 SSD。Crimson 旨在与早期版本的 OSD 守护进程与类 Ceph OSD 兼容。Crimson 基于 SeaStar C++ 框架构建，是核心 Ceph 对象存储守护进程">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bugwz.com/assets/images/bg/ceph.png">
<meta property="article:published_time" content="2025-05-31T16:00:00.000Z">
<meta property="article:modified_time" content="2025-06-13T15:22:15.212Z">
<meta property="article:author" content="bugwz">
<meta property="article:tag" content="Ceph">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bugwz.com/assets/images/bg/ceph.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Ceph Crimson 设计实现深入解析",
  "url": "https://bugwz.com/2025/06/01/ceph-cirmson/",
  "image": "https://bugwz.com/assets/images/bg/ceph.png",
  "datePublished": "2025-05-31T16:00:00.000Z",
  "dateModified": "2025-06-13T15:22:15.212Z",
  "author": [
    {
      "@type": "Person",
      "name": "bugwz",
      "url": "https://bugwz.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/assets/images/bg/favicon.png"><link rel="canonical" href="https://bugwz.com/2025/06/01/ceph-cirmson/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ceph Crimson 设计实现深入解析',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">126</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">134</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/assets/images/bg/ceph.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">咕咕</span></a><a class="nav-page-title" href="/"><span class="site-name">Ceph Crimson 设计实现深入解析</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Ceph Crimson 设计实现深入解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-31T16:00:00.000Z" title="发表于 2025-06-01 00:00:00">2025-06-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-13T15:22:15.212Z" title="更新于 2025-06-13 23:22:15">2025-06-13</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">9.2k</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>Crimson 是 Crimson OSD 的代码名称，它是下一代用于多核心可扩展性的 OSD 。它通过快速网络和存储设备提高性能，采用包括 DPDK 和 SPDK 的顶级技术。BlueStore 继续支持 HDD 和 SSD。Crimson 旨在与早期版本的 OSD 守护进程与类 Ceph OSD 兼容。</p>
<p>Crimson 基于 SeaStar C++ 框架构建，是核心 Ceph 对象存储守护进程 OSD 组件的新实现，并替换了 Ceph OSD 。Crimson OSD 最小化延迟并增加 CPU 处理器用量。它使用高性能异步 IO 和新的线程架构，旨在最小化上下文切换和用于跨通信的操作间的线程通信。</p>
<p><strong>以下分析基于 v19.2.1 进行分析。</strong></p>
<h1 id="一、架构对比"><a href="#一、架构对比" class="headerlink" title="一、架构对比"></a>一、架构对比</h1><p>Ceph OSD 是 Ceph 集群的一部分，负责通过网络提供对象访问、维护冗余和高可用性，并将对象持久化到本地存储设备。作为 Classic OSD 的重写版本，Crimson OSD 从客户端和其他 OSD 的角度兼容现有的 RADOS 协议，提供相同的接口和功能。Ceph OSD 的模块（例如 Messenger、OSD 服务和 ObjectStore）在其职责上保持不变，但跨组件交互的形式和内部资源管理经过了大幅重构，以应用无共享设计和自下而上的用户空间任务调度。</p>
<p>经典 OSD 的架构对多核处理器并不友好，因为每个组件都包含工作线程池，并且每个组件之间共享队列。举个简单的例子，一个 PG 操作首先需要由一个 Messenger 工作线程处理，将原始数据流组装或解码成一条消息，然后放入消息队列进行调度。之后， PG 工作线程获取该消息，经过必要的处理后，将请求以事务的形式交给 ObjectStore 。事务提交后， PG 将完成操作，并通过发送队列和 Messenger 工作线程再次发送回复。虽然可以通过向池中添加更多线程将工作负载扩展到多个 CPU ，但这些线程默认共享资源，因此需要使用锁，从而引入争用。实际情况会更加复杂，因为每个组件内部都会实现更多的线程池，并且如果跨 OSD 进行复制，数据路径也会更长。</p>
<p><img src="/assets/images/ceph-crimson-old-arch.png" alt="经典 OSD 架构" loading="lazy"></p>
<p>经典架构面临的一个主要挑战是，锁争用开销会随着任务和核心数量的增加而迅速增长，并且每个锁定点在某些情况下都可能成为扩展瓶颈。此外，即使在无争用的情况下，这些锁和队列也会产生延迟成本。多年来，人们在分析和优化更细粒度的资源管理和快速路径实现以跳过排队方面付出了巨大的努力。未来唾手可得的成果将会减少，在类似的设计下，可扩展性似乎正在收敛到某个乘数。此外，还存在其他挑战。由于簿记工作会在工作线程之间委派任务，延迟问题将随着线程池和任务队列的出现而恶化。锁可能会强制上下文切换，这会使情况更加糟糕。</p>
<p>Crimson 项目希望通过无共享设计和运行至完成模型来解决 CPU 的可扩展性问题。该设计的基本原理是强制每个核心（或 CPU）运行一个固定线程，并在用户空间中调度非阻塞任务。请求及其资源按核心进行分片，因此它们可以在同一核心中处理直至完成。理想情况下，所有锁和上下文切换都不再需要，因为每个正在运行的非阻塞任务都拥有 CPU，直到其完成或协同让出。没有其他线程可以同时抢占该任务。如果无需与数据路径中的其他分片通信，则理想的性能将随着核心数量线性扩展，直到 IO 设备达到其极限。这种设计非常适合 Ceph OSD，因为在 OSD 级别，所有 IO 都已按 PG 分片。</p>
<p><img src="/assets/images/ceph-crimson-new-arch.png" alt="Crimson OSD 架构" loading="lazy"></p>
<h1 id="二、配置解析流程"><a href="#二、配置解析流程" class="headerlink" title="二、配置解析流程"></a>二、配置解析流程</h1><p>配置解析的代码位于 <code>src/crimson/osd/main.cc</code> 文件中的 <code>auto early_config_result = crimson::osd::get_early_config(argc, argv);</code> 函数，该函数主要逻辑如下:</p>
<ul>
<li>创建一个子进程，在子进程中尝试解析参数后，将参数编码后通过管道传递给父进程；</li>
<li>父进程解析并返回参数给 main 函数中；</li>
</ul>
<p>子进程在 <code>_get_early_config</code> 函数中解析参数，其中 ceph 相关的参数使用 <code>ceph_argparse_early_args</code> 函数解析，并且根据 ceph 的 <code>crimson_seastar_cpu_cores</code> 参数来设置 <code>--cpuset $cpu_cores --thread-affinity 1</code> ；或者根据 ceph 的 <code>crimson_seastar_num_threads</code> 参数来设置 <code>--smp $smp --thread-affinity 0</code>。注意 <code>crimson_seastar_cpu_cores</code> 参数的优先级高于 <code>crimson_seastar_num_threads</code> 参数。</p>
<p>之后 <code>main</code> 函数中通过 <code>app.run</code> 函数调用，将解析到的参数传递给 <code>seastar</code> ，进而设置了 <code>seastar</code> 要启动的 <code>shard</code> 的数量及绑定 <code>cpu</code> 的配置。但是由于目前 <code>main</code> 中的 <code>seastar::async</code> 函数逻辑中没有显示的使用 <code>seastar::smp::count</code> 来将任务分发给多个 <code>shard</code> 执行，因此关于日志的配置，<code>prometheus</code> 的配置，<code>crimson osd</code> 的对象均是在 <code>shard 0</code> （即 <code>PRIMARY_CORE</code> ）上执行的。</p>
<h1 id="三、网络通信流程"><a href="#三、网络通信流程" class="headerlink" title="三、网络通信流程"></a>三、网络通信流程</h1><p>在 crimson osd 进程启动的时候，会调用 <code>OSD::start()</code> 函数，其内部会对 <code>public_msgr</code> 和 <code>cluster_msgr</code> 两个对象执行 <code>bind</code> 和 <code>start</code> 操作。</p>
<ul>
<li><code>bind 操作</code>: 对应的函数为 <code>SocketMessenger::bind</code> ， 该函数内部最终通过调用 seastar 的 <code>invoke_on_all</code> 下发 <code>seastar::listen(s_addr, lo)</code> 操作给所有 <code>shard</code> ，使所有的 <code>shard</code> 开始监听相同的端口；</li>
<li><code>start 操作</code>: 对应的函数为 <code>SocketMessenger::start</code> ， 该函数内部通过调用 <code>ShardedServerSocket::accept</code> ，并在其内部调用 seastar 的 <code>invoke_on_all</code> 方法使每个 shard 接收新连接请求。每个 <code>shard</code> 接收到请求后，会逐步调用 <code>SocketMessenger::accept</code> &#x3D;&gt; <code>SocketConnection::start_accept</code> &#x3D;&gt; <code>ProtocolV2::start_accept</code> &#x3D;&gt; <code>ProtocolV2::execute_accepting</code> 等函数逐步处理请求，最终会调用到 <code>OSD::do_ms_dispatch</code> 函数正式处理客户端请求。</li>
</ul>
<p>在 <code>OSD::do_ms_dispatch</code> 函数内部，针对于请求消息的类型，有如下操作：</p>
<ul>
<li><code>必须在 PRIMARY_CORE shard 上执行的操作</code>: 包括 <code>CEPH_MSG_OSD_MAP</code>、<code>MSG_COMMAND</code>、<code>MSG_OSD_MARK_ME_DOWN</code> 等；</li>
<li><code>其他可以在任意 shard 上执行的操作</code>：包括 <code>CEPH_MSG_OSD_MAP</code>、<code>CEPH_MSG_OSD_OP</code>、<code>MSG_COMMAND</code> 等；</li>
</ul>
<p><img src="/assets/images/ceph-crimson-osd-pg-shard.png" alt="Shards In OSD" loading="lazy"></p>
<ul>
<li>由于 <code>OSD</code> 中的每个 <code>Shard</code> 都会监听网络信息，所以每个 <code>Shard</code> 都可以处理网络请求；</li>
<li>但是由于需要对请求按照 <code>PG</code> 映射到 <code>Shard</code> 中，所以内部引入了 <code>pg_to_shard_mapping</code> 的映射结构，每个请求都需要在 <code>Shard</code> 中检索映射表；</li>
<li>如果当前 <code>Shard</code> 中的映射表中缺少 <code>PG</code> 的映射信息，会将请求发送给 <code>Shard 0</code> 来尝试创建对应的映射记录，并将该记录广播给所有的 <code>Shard</code> ；</li>
</ul>
<p><strong>对于请求类型为 <code>CEPH_MSG_OSD_OP</code> 的关键代码链路如下:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 处理对应的 op 请求</span></span><br><span class="line">seastar::<span class="built_in">future</span>&lt;&gt; OSD::handle_osd_op(crimson::net::ConnectionRef conn, Ref&lt;MOSDOp&gt; m)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> pg_shard_manager.start_pg_operation&lt;ClientRequest&gt;(get_shard_services(), conn, <span class="built_in">std</span>::move(m)).second;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 开始 pg 操作</span></span><br><span class="line">template&lt;typename T, typename... Args&gt; <span class="keyword">auto</span> <span class="title function_">start_pg_operation</span><span class="params">(Args&amp;&amp;... args)</span></span><br><span class="line">&#123;</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> fut =</span><br><span class="line">        opref.template enter_stage&lt;&gt;(opref.get_connection_pipeline().await_active)</span><br><span class="line"></span><br><span class="line">            ......</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 从 pg_to_shard_mapping 中获取 pg 与 shard 的对应关系，</span></span><br><span class="line">            <span class="comment">// 如果对应的映射关系不存在，则根据各 shard 的负载情况创建映射关系。</span></span><br><span class="line">            .then([this, &amp;opref] &#123; <span class="keyword">return</span> get_pg_to_shard_mapping().get_or_create_pg_mapping(opref.get_pgid()); &#125;)</span><br><span class="line">            .then_wrapped([this, &amp;logger, op = <span class="built_in">std</span>::move(op)](<span class="keyword">auto</span> fut) mutable &#123;</span><br><span class="line"></span><br><span class="line">                ......</span><br><span class="line"></span><br><span class="line">                <span class="keyword">auto</span> core = fut.get();</span><br><span class="line">                logger.debug(<span class="string">&quot;&#123;&#125;: can_create=&#123;&#125;, target-core=&#123;&#125;&quot;</span>, *op, T::can_create(), core);</span><br><span class="line">                <span class="comment">// 处理已知 shard id 的 op 请求</span></span><br><span class="line">                <span class="keyword">return</span> this-&gt;template with_remote_shard_state_and_op&lt;T&gt;(</span><br><span class="line">                    core, <span class="built_in">std</span>::move(op), [this](ShardServices&amp; target_shard_services, typename T::IRef op) &#123;</span><br><span class="line">                        <span class="keyword">auto</span>&amp; opref = *op;</span><br><span class="line">                        <span class="keyword">auto</span>&amp; logger = crimson::get_logger(ceph_subsys_osd);</span><br><span class="line">                        logger.debug(<span class="string">&quot;&#123;&#125;: entering create_or_wait_pg&quot;</span>, opref);</span><br><span class="line">                        <span class="keyword">return</span> opref</span><br><span class="line">                            .template enter_stage&lt;&gt;(</span><br><span class="line">                                opref.get_pershard_pipeline(target_shard_services).create_or_wait_pg)</span><br><span class="line">                            .then([this, &amp;target_shard_services, op = <span class="built_in">std</span>::move(op)]() mutable &#123;</span><br><span class="line">                                <span class="keyword">if</span> constexpr (T::can_create()) &#123;</span><br><span class="line">                                    <span class="keyword">return</span> this-&gt;template run_with_pg_maybe_create&lt;T&gt;(<span class="built_in">std</span>::move(op),</span><br><span class="line">                                                                                        target_shard_services);</span><br><span class="line">                                &#125;</span><br><span class="line">                                <span class="keyword">else</span> &#123;</span><br><span class="line">                                    <span class="keyword">return</span> this-&gt;template run_with_pg_maybe_wait&lt;T&gt;(<span class="built_in">std</span>::move(op),</span><br><span class="line">                                                                                    target_shard_services);</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;);</span><br><span class="line">                    &#125;);</span><br><span class="line">            &#125;);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">std</span>::<span class="built_in">make_pair</span>(id, <span class="built_in">std</span>::move(fut));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取或创建 pg 和 shard 的映射关系</span></span><br><span class="line">seastar::<span class="built_in">future</span>&lt;<span class="type">core_id_t</span>&gt; <span class="title function_">PGShardMapping::get_or_create_pg_mapping</span><span class="params">(<span class="type">spg_t</span> pgid, <span class="type">core_id_t</span> core_expected)</span></span><br><span class="line">&#123;</span><br><span class="line">    LOG_PREFIX(PGShardMapping::get_or_create_pg_mapping);</span><br><span class="line">    <span class="keyword">auto</span> find_iter = pg_to_core.find(pgid);</span><br><span class="line">    <span class="keyword">if</span> (find_iter != pg_to_core.end()) &#123;</span><br><span class="line">        <span class="keyword">auto</span> core_found = find_iter-&gt;second;</span><br><span class="line">        <span class="comment">// 一些校验逻辑</span></span><br><span class="line">        assert(core_found != NULL_CORE);</span><br><span class="line">        <span class="keyword">if</span> (core_expected != NULL_CORE &amp;&amp; core_expected != core_found) &#123;</span><br><span class="line">            ERROR(<span class="string">&quot;the mapping is inconsistent for pg &#123;&#125;: core &#123;&#125;, expected &#123;&#125;&quot;</span>, pgid, core_found, core_expected);</span><br><span class="line">            ceph_abort(<span class="string">&quot;The pg mapping is inconsistent!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> seastar::make_ready_future&lt;<span class="type">core_id_t</span>&gt;(core_found);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        DEBUG(<span class="string">&quot;calling primary to add mapping for pg &#123;&#125; to the expected core &#123;&#125;&quot;</span>, pgid, core_expected);</span><br><span class="line">        <span class="comment">// 如果没有找到 pg 和 shard 的映射关系，则需要创建映射，</span></span><br><span class="line">        <span class="comment">// 创建操作必须由 shard 0 执行。</span></span><br><span class="line">        <span class="keyword">return</span> container()</span><br><span class="line">            .invoke_on(</span><br><span class="line">                <span class="number">0</span>,</span><br><span class="line">                [pgid, core_expected, FNAME](<span class="keyword">auto</span>&amp; primary_mapping) &#123;</span><br><span class="line">                    <span class="keyword">auto</span> core_to_update = core_expected;</span><br><span class="line">                    <span class="comment">// 在 shard 0 中判断对应的映射关系是否存在，</span></span><br><span class="line">                    <span class="comment">// 如果存在且校验正常则可使用该映射关系</span></span><br><span class="line">                    <span class="keyword">auto</span> find_iter = primary_mapping.pg_to_core.find(pgid);</span><br><span class="line">                    <span class="keyword">if</span> (find_iter != primary_mapping.pg_to_core.end()) &#123;</span><br><span class="line"></span><br><span class="line">                        ......</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 如果在 shard 0 中也没有找到映射关系，则创建映射</span></span><br><span class="line">                        ceph_assert_always(primary_mapping.core_to_num_pgs.size() &gt; <span class="number">0</span>);</span><br><span class="line">                        <span class="built_in">std</span>::<span class="built_in">map</span>&lt;<span class="type">core_id_t</span>, <span class="type">unsigned</span>&gt;::iterator count_iter;</span><br><span class="line">                        <span class="keyword">if</span> (core_expected == NULL_CORE) &#123;</span><br><span class="line">                            <span class="comment">// 从 shard 中选择 pg 映射数量最少的最为当前 pg 的关联 shard</span></span><br><span class="line">                            count_iter = <span class="built_in">std</span>::min_element(</span><br><span class="line">                                primary_mapping.core_to_num_pgs.begin(),</span><br><span class="line">                                primary_mapping.core_to_num_pgs.end(),</span><br><span class="line">                                [](<span class="type">const</span> <span class="keyword">auto</span>&amp; left, <span class="type">const</span> <span class="keyword">auto</span>&amp; right) &#123; <span class="keyword">return</span> left.second &lt; right.second; &#125;);</span><br><span class="line">                            core_to_update = count_iter-&gt;first;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        ......</span><br><span class="line">                    &#125;</span><br><span class="line">                    assert(core_to_update != NULL_CORE);</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 广播同步</span></span><br><span class="line">                    <span class="comment">// 通过 invoke_on_others 确保所有 Core 的映射表同步更新</span></span><br><span class="line">                    <span class="comment">// 将变更的映射关系广播给其他所有的 shard</span></span><br><span class="line">                    <span class="keyword">return</span> primary_mapping.container().invoke_on_others(</span><br><span class="line">                        [pgid, core_to_update, FNAME](<span class="keyword">auto</span>&amp; other_mapping) &#123;</span><br><span class="line"></span><br><span class="line">                            ......</span><br><span class="line">                        &#125;);</span><br><span class="line">                &#125;)</span><br><span class="line"></span><br><span class="line">                ......</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理 op 请求</span></span><br><span class="line">template&lt;typename T, typename F&gt; <span class="keyword">auto</span> with_remote_shard_state_and_op(<span class="type">core_id_t</span> core, typename T::IRef&amp;&amp; op, F&amp;&amp; f)</span><br><span class="line">&#123;</span><br><span class="line">    ceph_assert(op-&gt;use_count() == <span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 如果 op 请求的目标 shard 为当前 shard ，则在当前 shard 中处理</span></span><br><span class="line">    <span class="keyword">if</span> (seastar::this_shard_id() == core) &#123;</span><br><span class="line">        <span class="keyword">auto</span> f_conn = op-&gt;prepare_remote_submission();</span><br><span class="line">        op-&gt;finish_remote_submission(<span class="built_in">std</span>::move(f_conn));</span><br><span class="line">        <span class="keyword">auto</span>&amp; target_shard_services = shard_services.local();</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">std</span>::invoke(<span class="built_in">std</span>::move(f), target_shard_services, <span class="built_in">std</span>::move(op));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 否则，将对应的 op 请求转发给对应的 shard 处理</span></span><br><span class="line">    logger.debug(<span class="string">&quot;&#123;&#125;: send &#123;&#125; to the remote pg core &#123;&#125;&quot;</span>, opref, cc_seq, core);</span><br><span class="line">    <span class="keyword">return</span> opref.get_handle().complete().then([this, core, cc_seq, op = <span class="built_in">std</span>::move(op), f = <span class="built_in">std</span>::move(f)]() mutable &#123;</span><br><span class="line">        get_local_state().registry.remove_from_registry(*op);</span><br><span class="line">        <span class="keyword">auto</span> f_conn = op-&gt;prepare_remote_submission();</span><br><span class="line">        <span class="keyword">return</span> shard_services.invoke_on(</span><br><span class="line">            core,</span><br><span class="line">            [this, cc_seq, f = <span class="built_in">std</span>::move(f), op = <span class="built_in">std</span>::move(op), f_conn = <span class="built_in">std</span>::move(f_conn)](</span><br><span class="line">                <span class="keyword">auto</span>&amp; target_shard_services) mutable &#123;</span><br><span class="line">                op-&gt;finish_remote_submission(<span class="built_in">std</span>::move(f_conn));</span><br><span class="line">                target_shard_services.local_state.registry.add_to_registry(*op);</span><br><span class="line">                <span class="keyword">return</span> this-&gt;template process_ordered_op_remotely&lt;T&gt;(</span><br><span class="line">                    cc_seq, target_shard_services, <span class="built_in">std</span>::move(op), <span class="built_in">std</span>::move(f));</span><br><span class="line">            &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="四、线程模型"><a href="#四、线程模型" class="headerlink" title="四、线程模型"></a>四、线程模型</h1><p>在服务启动时会通过解析 <code>crimson_seastar_cpu_cores</code> 或 <code>crimson_seastar_num_threads</code> 这两个配置来设置 <code>seastar</code> 框架的并发 <code>shard</code> 数量，之后在 <code>PRIMARY_CORE</code> 初始化环境，并通过 <code>seastar</code> 的 <code>invoke_on</code>、<code>invoke_on_others</code>、<code>invoke_on_all</code>、<code>seastar::smp::submit_to</code> 等方法来给 <code>shard</code> 下发任务，从而实现 <code>osd</code> 中相互独立的 <code>shard</code> 任务模型。</p>
<h2 id="4-1、shard-相关任务"><a href="#4-1、shard-相关任务" class="headerlink" title="4.1、shard 相关任务"></a>4.1、shard 相关任务</h2><p><strong>seastar 提供的不同的下发任务的方法比较:</strong></p>
<table>
<thead>
<tr>
<th align="center">接口</th>
<th align="center">目标 Shard</th>
<th align="center">是否依赖 <code>sharded</code> 容器</th>
<th align="center">典型用途</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>invoke_on</code></td>
<td align="center">指定单个 Shard</td>
<td align="center">是</td>
<td align="center">访问特定 Shard 上的对象</td>
</tr>
<tr>
<td align="center"><code>invoke_on_others</code></td>
<td align="center">除当前 Shard 外的所有</td>
<td align="center">是</td>
<td align="center">广播操作（排除当前 Shard）</td>
</tr>
<tr>
<td align="center"><code>invoke_on_all</code></td>
<td align="center">所有 Shard（包括当前）</td>
<td align="center">是</td>
<td align="center">全局初始化&#x2F;清理</td>
</tr>
<tr>
<td align="center"><code>smp::submit_to</code></td>
<td align="center">指定单个 Shard</td>
<td align="center">否</td>
<td align="center">任意跨 Shard 任务</td>
</tr>
</tbody></table>
<p><strong><code>invoke_on</code> 的部分操作如下:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 更新配置值并通知所有观察者</span></span><br><span class="line">container().invoke_on(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 0 号 shard 上停止 shards</span></span><br><span class="line">this-&gt;container().invoke_on(<span class="number">0</span>, [](<span class="keyword">auto</span>&amp; ss) &#123; ... &#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 0 号 shard 上新增 pg 和 shard 的映射关系</span></span><br><span class="line">container().invoke_on(<span class="number">0</span>, [pgid, core_expected, FNAME](<span class="keyword">auto</span>&amp;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 0 号 shard 上移除 pg 和 shard 的映射关系</span></span><br><span class="line">container().invoke_on(<span class="number">0</span>, [pgid, FNAME](<span class="keyword">auto</span>&amp; primary_mapping) &#123; ... &#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 转发请求给特定 shard</span></span><br><span class="line">shard_services.invoke_on(core, ... )</span><br></pre></td></tr></table></figure>


<p><strong><code>invoke_on_others</code> 的部分操作如下:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 更新 proxy 配置</span></span><br><span class="line">container().invoke_on_others(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 广播 pg shard 新增映射记录</span></span><br><span class="line">primary_mapping.container().invoke_on_others(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 广播 pg shard 移除映射记录</span></span><br><span class="line">primary_mapping.container().invoke_on_others(...)</span><br></pre></td></tr></table></figure>

<p><strong><code>invoke_on_all</code> 的部分操作如下:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">seastar::listen</span><br><span class="line">ss.listener-&gt;accept()</span><br><span class="line">ss.listener-&gt;abort_accept()</span><br><span class="line">ss.listener.reset()</span><br><span class="line">local_store.mkfs()</span><br><span class="line">local_store.mount()</span><br><span class="line">local_store.umount()</span><br><span class="line">local_store.mount_managers()</span><br><span class="line">local_store.set_secondaries(...)</span><br><span class="line">local_store.mkfs_managers()</span><br><span class="line">local_device.do_shard_mount()</span><br><span class="line">local_device.shard_mount()</span><br><span class="line">local_device.shard_mkfs()</span><br><span class="line">local_service.local_state.stop_pgs()</span><br><span class="line">local_service.local_state.broadcast_map_to_pgs(local_service, epoch)</span><br><span class="line">local_service.local_state.osdmap_gate.got_map(epoch)</span><br><span class="line">local_service.local_state.set_up_epoch(e)</span><br><span class="line">local_service.local_state.update_shard_superblock(superblock)</span><br><span class="line">local.local_state.update_map(...)</span><br><span class="line">local.local_state.stop_registry()</span><br><span class="line">osd_state._set_active()</span><br><span class="line">osd_state._set_stopping()</span><br></pre></td></tr></table></figure>


<p><strong><code>seastar::smp::submit_to</code> 的部分操作如下:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在 shard 0 上处理 CEPH_MSG_OSD_MAP/MSG_COMMAND/MSG_OSD_MARK_ME_DOWN 消息</span></span><br><span class="line">seastar::smp::submit_to(PRIMARY_CORE, ... )</span><br></pre></td></tr></table></figure>


<h2 id="4-2、线程示例"><a href="#4-2、线程示例" class="headerlink" title="4.2、线程示例"></a>4.2、线程示例</h2><p><strong>当 <code>crimson_seastar_num_threads</code> 设置为 <code>2</code> 的时候，crimson osd 的线程情况:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@bugwz.host build]<span class="comment"># ps -T -p 270088</span></span><br><span class="line">    PID    SPID TTY          TIME CMD</span><br><span class="line"> 270088  270088 pts/11   00:30:31 crimson-osd</span><br><span class="line"> 270088  270130 pts/11   00:22:41 reactor-1</span><br><span class="line"> 270088  270131 pts/11   00:00:00 syscall-0</span><br><span class="line"> 270088  270132 pts/11   00:00:00 syscall-1</span><br><span class="line"> 270088  270133 pts/11   00:00:00 crimson-osd</span><br><span class="line"> 270088  270134 pts/11   00:00:00 reactor-1</span><br></pre></td></tr></table></figure>

<p><strong>当 <code>crimson_seastar_num_threads</code> 设置为 <code>8</code> 的时候，crimson osd 的线程情况:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@bugwz.host build]<span class="comment"># ps -T -p 345103</span></span><br><span class="line">    PID    SPID TTY          TIME CMD</span><br><span class="line"> 345103  345103 pts/15   00:00:04 crimson-osd</span><br><span class="line"> 345103  345145 pts/15   00:00:02 reactor-1</span><br><span class="line"> 345103  345146 pts/15   00:00:02 reactor-2</span><br><span class="line"> 345103  345147 pts/15   00:00:02 reactor-3</span><br><span class="line"> 345103  345148 pts/15   00:00:02 reactor-4</span><br><span class="line"> 345103  345149 pts/15   00:00:02 reactor-5</span><br><span class="line"> 345103  345150 pts/15   00:00:02 reactor-6</span><br><span class="line"> 345103  345151 pts/15   00:00:02 reactor-7</span><br><span class="line"> 345103  345152 pts/15   00:00:00 syscall-7</span><br><span class="line"> 345103  345153 pts/15   00:00:00 syscall-0</span><br><span class="line"> 345103  345154 pts/15   00:00:00 syscall-4</span><br><span class="line"> 345103  345155 pts/15   00:00:00 syscall-3</span><br><span class="line"> 345103  345156 pts/15   00:00:00 syscall-2</span><br><span class="line"> 345103  345157 pts/15   00:00:00 syscall-5</span><br><span class="line"> 345103  345158 pts/15   00:00:00 syscall-1</span><br><span class="line"> 345103  345159 pts/15   00:00:00 syscall-6</span><br><span class="line"> 345103  345160 pts/15   00:00:00 crimson-osd</span><br><span class="line"> 345103  345161 pts/15   00:00:00 reactor-1</span><br><span class="line"> 345103  345162 pts/15   00:00:00 reactor-4</span><br><span class="line"> 345103  345163 pts/15   00:00:00 reactor-5</span><br><span class="line"> 345103  345164 pts/15   00:00:00 reactor-6</span><br><span class="line"> 345103  345165 pts/15   00:00:00 reactor-7</span><br><span class="line"> 345103  345166 pts/15   00:00:00 reactor-2</span><br><span class="line"> 345103  345167 pts/15   00:00:00 reactor-3</span><br></pre></td></tr></table></figure>


<h1 id="五、存储模块设计"><a href="#五、存储模块设计" class="headerlink" title="五、存储模块设计"></a>五、存储模块设计</h1><h2 id="5-1、后端对象存储类型"><a href="#5-1、后端对象存储类型" class="headerlink" title="5.1、后端对象存储类型"></a>5.1、后端对象存储类型</h2><p>main 函数中会通过 <code>crimson::os::FuturizedStore::create</code> 函数来创建 <code>store</code> 对象。根据 <code>osd_objectstore</code> 和 <code>osd_data</code> 参数来配置 <code>store</code> 对象。其中 <code>osd_objectstore</code> 参数指定了后端对象存储的类型，支持的参数有 <code>alienstore/cyanstore/seastore</code> ，默认为 <code>alienstore</code> （即后端存储为 <code>bluestore</code> ）。其中 <code>osd_data</code> 参数指定了数据存储目录（比如当使用 <code>vstart.sh</code> 部署集群时，对应的配置默认为 <code>./build/dev/osd$id</code> ）。</p>
<p><strong>对象存储类型:</strong></p>
<ul>
<li><strong><code>alienstore</code></strong>: 是 seastar 线程中的一个代理，主要是与 bluestore 进行通信。由于 io 任务会与 bluestore 进行通信，因此无需针对多个 osd 分片进行特殊处理。BlueStore 中没有针对 crimson 的定制，因为 bluestore 依赖于第三方 RocksDB 项目，而该项目仍然采用线程化设计，因此无法真正将其扩展为无共享设计。然而，在 crimson 能够提供经过优化且足够稳定的原生存储后端 seastore 之前，使用合理的开销来换取完善的存储后端解决方案是可以接受的。</li>
<li><strong><code>cyanstore</code></strong>: crimson osd 中的 cyanstore 与 classic osd 中的 memstore 相对应。为了支持多分片，唯一的变化是每个分片创建独立的 cyanstore 实例。一个目标是确保虚拟 IO 操作能够在同一核心中完成，以帮助识别 osd 级别的可扩展性问题（如果有）。另一个目标是在 osd 级别与 Classic 进行直接性能比较，而不会受到 objectstore 的复杂影响。</li>
<li><strong><code>seastore</code></strong>: seastore 是 crimson osd 的原生 objectstore 解决方案，它使用 seastar 框架开发并采用相同的设计原则。</li>
</ul>
<p>在 seastore 初始化的时候，会根据 <code>seastore_main_device_type</code> 参数来初始化 <code>seastore</code> 主设备，该参数可选值为 <code>SSD/RANDOM_BLOCK_SSD</code> （代码中还实现了 <code>HDD/ZBD</code> ，但是目前并不支持） ，默认为 <code>SSD</code> 。 在调用 <code>Device::make_device(root, d_type)</code> 函数创建 <code>device</code> 的过程中，会针对不同的设备类型又做了一些区分。</p>
<p><strong>seastore 设备类型对比:</strong></p>
<table>
<thead>
<tr>
<th align="center">device_type</th>
<th align="center">backend_type</th>
<th align="center">create func</th>
</tr>
</thead>
<tbody><tr>
<td align="center">HDD</td>
<td align="center">backend_type_t::SEGMENTED</td>
<td align="center">SegmentManager::get_segment_manager</td>
</tr>
<tr>
<td align="center">SSD</td>
<td align="center">backend_type_t::SEGMENTED</td>
<td align="center">SegmentManager::get_segment_manager</td>
</tr>
<tr>
<td align="center">ZBD</td>
<td align="center">backend_type_t::SEGMENTED</td>
<td align="center">SegmentManager::get_segment_manager</td>
</tr>
<tr>
<td align="center">RANDOM_BLOCK_SSD</td>
<td align="center">backend_type_t::RANDOM_BLOCK</td>
<td align="center">get_rb_device</td>
</tr>
</tbody></table>
<h2 id="5-2、段存储格式信息"><a href="#5-2、段存储格式信息" class="headerlink" title="5.2、段存储格式信息"></a>5.2、段存储格式信息</h2><p>当使用 <code>vstart.sh</code> 脚本部署测试集群后会发现 <code>build/dev/osd*/</code> 目录下会存在一个 <code>block</code> 文件，该文件对应的就是一个 <code>osd</code> 组件后端的对象存储，由于一个 <code>osd</code> 中可能会启用多个 <code>seastar shard</code> ，并且由于 <code>shard</code> 间数据的隔离，因此需要对这大块存储空间进行切割，使每个 <code>shard</code> 各负责一块空间，从而实现操作数据的隔离。 </p>
<p><strong>后端存储的格式化规则:</strong></p>
<ul>
<li>开始部分为 superblock 空间，存储这个该存储空间的规划及使用信息；</li>
<li>剩余空间平均分配给每个 shard ，实现独立的操作空间；</li>
</ul>
<p><strong>创建 superblock 及 shard 空间规划函数如下:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">using <span class="built_in">std</span>::<span class="built_in">vector</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">block_sm_superblock_t</span> <span class="title function_">make_superblock</span><span class="params">(<span class="type">device_id_t</span> device_id, <span class="type">device_config_t</span> sm_config, <span class="type">const</span> seastar::stat_data&amp; data)</span></span><br><span class="line">&#123;</span><br><span class="line">    LOG_PREFIX(block_make_superblock);</span><br><span class="line">    using crimson::common::get_conf;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// seastore_device_size 默认为 50G</span></span><br><span class="line">    <span class="keyword">auto</span> config_size = get_conf&lt;Option::<span class="type">size_t</span>&gt;(<span class="string">&quot;seastore_device_size&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">size_t</span> size = (data.size == <span class="number">0</span>) ? config_size : data.size;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 单个 segment 的大小，默认为 64M</span></span><br><span class="line">    <span class="keyword">auto</span> config_segment_size = get_conf&lt;Option::<span class="type">size_t</span>&gt;(<span class="string">&quot;seastore_segment_size&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算 segment 数量： 总大小除以单个 segment 的大小</span></span><br><span class="line">    <span class="type">size_t</span> raw_segments = size / config_segment_size;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算每个 shard 所需要的段状态跟踪器大小</span></span><br><span class="line">    <span class="comment">// 默认为一个 data.block_size 大小，如果计算出的每个 shard 所管理的 segments 数量超过 data.block_size 大小，</span></span><br><span class="line">    <span class="comment">// 则返回超过 segments 数量的 data.block_size 的倍数值。</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// seastar::smp::count 为 crimson osd 启动时指定的 shard 数量</span></span><br><span class="line">    <span class="comment">// data.block_size 默认为 4096</span></span><br><span class="line">    <span class="type">size_t</span> shard_tracker_size = SegmentStateTracker::get_raw_size(raw_segments / seastar::smp::count, data.block_size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算全部 shard 的段状态跟踪器的总大小</span></span><br><span class="line">    <span class="type">size_t</span> total_tracker_size = shard_tracker_size * seastar::smp::count;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始的偏移应该从 superblock 之后开始</span></span><br><span class="line">    <span class="type">size_t</span> tracker_off = data.block_size;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算减去 superblock 及所有段状态跟踪器总大小之后的剩余空间可分配的 segments 数量</span></span><br><span class="line">    <span class="type">size_t</span> segments = (size - tracker_off - total_tracker_size) / config_segment_size;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算每个 shard 可分配的 segments 数量</span></span><br><span class="line">    <span class="type">size_t</span> segments_per_shard = segments / seastar::smp::count;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化每个 shard 信息</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="type">block_shard_info_t</span>&gt; <span class="title function_">shard_infos</span><span class="params">(seastar::smp::count)</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; seastar::smp::count; i++) &#123;</span><br><span class="line">        <span class="comment">// 每个 shard 管理的 segments 总大小</span></span><br><span class="line">        shard_infos[i].size = segments_per_shard * config_segment_size;</span><br><span class="line">        <span class="comment">// 每个 shard 管理的 segments 数量</span></span><br><span class="line">        shard_infos[i].segments = segments_per_shard;</span><br><span class="line">        <span class="comment">// 标记每个 shard 的段状态跟踪器的在全部空间中的偏移</span></span><br><span class="line">        shard_infos[i].tracker_offset = tracker_off + i * shard_tracker_size;</span><br><span class="line">        <span class="comment">// 标记每个 shard 的 segment 数据起始位置在全部空间中的偏移</span></span><br><span class="line">        shard_infos[i].first_segment_offset = tracker_off + total_tracker_size + i * segments_per_shard * config_segment_size;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出日志信息</span></span><br><span class="line">    INFO(<span class="string">&quot;&#123;&#125; disk_size=&#123;&#125;, segment_size=&#123;&#125;, block_size=&#123;&#125;&quot;</span>,</span><br><span class="line">         <span class="type">device_id_printer_t</span>&#123;device_id&#125;,</span><br><span class="line">         size,</span><br><span class="line">         <span class="type">uint64_t</span>(config_segment_size),</span><br><span class="line">         data.block_size);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; seastar::smp::count; i++) &#123;</span><br><span class="line">        INFO(<span class="string">&quot;shard &#123;&#125; infos:&quot;</span>, i, shard_infos[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回 superblock 全部信息</span></span><br><span class="line">    <span class="keyword">return</span> <span class="type">block_sm_superblock_t</span>&#123;seastar::smp::count, config_segment_size, data.block_size, shard_infos, <span class="built_in">std</span>::move(sm_config)&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="六、客户端使用方式"><a href="#六、客户端使用方式" class="headerlink" title="六、客户端使用方式"></a>六、客户端使用方式</h1><p>由于 crimson osd 只支持 <code>message v2</code> 协议，所以我们在挂载 cephfs&#x2F;cephrbd 等的时候需要使用 <code>message v2</code> 的方式。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 挂载 cephrbd - kernel 方式</span></span><br><span class="line">rbd device map -t krbd rbdpool/rbdimg01 -o mount_timeout=5,ms_mode=crc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载 cephrbd - nbd 方式</span></span><br><span class="line">rbd device map -t nbd rbdpool/rbdimg01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消挂载 cephrbd</span></span><br><span class="line">rbd device unmap rbdpool/rbdimg01 -t krbd</span><br><span class="line">rbd device unmap rbdpool/rbdimg01 -t nbd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载 cephfs - kernel 方式</span></span><br><span class="line">mount -t ceph 10.10.10.1:3300:/ /mnt/kernel-cephfs -o name=admin,secret=AQBVokZoak+LJRAAqgeJr6j77v729bfvBl/Z3g==,ms_mode=crc,mount_timeout=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载 cephfs - fuse 方式</span></span><br><span class="line">ceph-fuse -c /etc/ceph/ceph.conf -n client.admin -m 10.10.10.1:3300 /mnt/fuse-cephfs --client_mountpoint /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消挂载 cephfs</span></span><br><span class="line">umount /mnt/kernel-cephfs</span><br><span class="line">fusermount -u /mnt/fuse-cephfs</span><br></pre></td></tr></table></figure>


<p><strong>相关代码实现:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 筛选监听地址</span></span><br><span class="line"><span class="type">entity_addrvec_t</span> <span class="title function_">pick_addresses</span><span class="params">(<span class="type">int</span> what)</span></span><br><span class="line">&#123;</span><br><span class="line">    LOG_PREFIX(osd.cc : pick_addresses);</span><br><span class="line">    <span class="type">entity_addrvec_t</span> addrs;</span><br><span class="line">    crimson::common::CephContext cct;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 仅筛选 message v2 的地址</span></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> flags = what | CEPH_PICK_ADDRESS_MSGR2;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">int</span> r = ::pick_addresses(&amp;cct, flags, &amp;addrs, <span class="number">-1</span>); r &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        throw <span class="built_in">std</span>::runtime_error(<span class="string">&quot;failed to pick address&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> addr : addrs.v) &#123;</span><br><span class="line">        INFO(<span class="string">&quot;picked address &#123;&#125;&quot;</span>, addr);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> addrs;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 接收请求</span></span><br><span class="line">seastar::<span class="built_in">future</span>&lt;&gt; SocketMessenger::start(<span class="type">const</span> <span class="type">dispatchers_t</span>&amp; _dispatchers)</span><br><span class="line">&#123;</span><br><span class="line">    assert(seastar::this_shard_id() == sid);</span><br><span class="line">    dispatchers.assign(_dispatchers);</span><br><span class="line">    <span class="keyword">if</span> (listener) &#123;</span><br><span class="line">        <span class="comment">// 仅支持 message v2 的地址</span></span><br><span class="line">        ceph_assert(get_myaddr().is_msgr2());</span><br><span class="line">        ceph_assert(get_myaddr().get_port() &gt; <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 接收端口请求</span></span><br><span class="line">        <span class="keyword">return</span> listener-&gt;accept([this](SocketRef _socket, <span class="type">entity_addr_t</span> peer_addr) &#123;</span><br><span class="line">            assert(get_myaddr().is_msgr2());</span><br><span class="line">            SocketFRef socket = seastar::make_foreign(<span class="built_in">std</span>::move(_socket));</span><br><span class="line">            <span class="keyword">if</span> (listener-&gt;is_fixed_shard_dispatching()) &#123;</span><br><span class="line">                <span class="keyword">return</span> accept(<span class="built_in">std</span>::move(socket), peer_addr);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> seastar::smp::submit_to(sid, [this, peer_addr, socket = <span class="built_in">std</span>::move(socket)]() mutable &#123;</span><br><span class="line">                    <span class="keyword">return</span> accept(<span class="built_in">std</span>::move(socket), peer_addr);</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> seastar::now();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="七、其他特性实现"><a href="#七、其他特性实现" class="headerlink" title="七、其他特性实现"></a>七、其他特性实现</h1><h2 id="7-1、冷热存储分离"><a href="#7-1、冷热存储分离" class="headerlink" title="7.1、冷热存储分离"></a>7.1、冷热存储分离</h2><p>当使用 <code>vstart.sh</code> 脚本部署测试的时候，我们会发现 <code>--seastore-secondary-devs</code> 和 <code>--seastore-secondary-devs-type</code> 配置，如果指定了这两个参数，该脚本便会通过 <code>dd</code> 格式化对应盘，然后创建 <code>./dev/osd$id/block.$type.1</code> 目录，之后执行 <code>ln -s $device ./dev/osd$id/block.$type.1/block</code> 创建一个软链文件。详细的代码可以查看: <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph/blob/v19.2.1/src/vstart.sh#L1194">https://github.com/ceph/ceph/blob/v19.2.1/src/vstart.sh#L1194</a> 。</p>
<p>按照官方解释这两个参数是用来指定次要块设备的列表和类型，进一步分析 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/dev/crimson/crimson/">crimson 官方文档</a> 我们发现这两个配置可用于实现 ceph 的冷热存储分离特性，即随着时间的推移逐步将较快设备（主设备）中的冷数据迁移到较慢的设备（次要设备）中，通常要求次要设备的速度不应该比主设备更快。我们能发现该特性与 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/operations/cache-tiering/">Cache Tiering</a> 特性比较相似，之后也会做一下对比分析。</p>
<p><strong>关于主设备剔除数据到次要设备的相关参数:</strong></p>
<ul>
<li><code>seastore_multiple_tiers_stop_evict_ratio</code>: 当主设备的使用率低于此值时，停止将冷数据逐出到冷层。默认值为 0.5 。</li>
<li><code>seastore_multiple_tiers_default_evict_ratio</code>: 当主设备的使用率达到此值时，开始将冷数据迁移到次要设备。默认值为 0.6 。</li>
<li><code>seastore_multiple_tiers_fast_evict_ratio</code>: 当主设备的使用率达到此值时，开始执行快速逐出。默认值为 0.7 。</li>
</ul>
<h1 id="八、模块解析"><a href="#八、模块解析" class="headerlink" title="八、模块解析"></a>八、模块解析</h1><h1 id="九、代码逻辑梳理"><a href="#九、代码逻辑梳理" class="headerlink" title="九、代码逻辑梳理"></a>九、代码逻辑梳理</h1><p><code>main</code> 函数中启动的 <code>seastar::async</code> 异步任务的关键逻辑如下:</p>
<ul>
<li>设置日志级别并打开日志文件；</li>
<li>启动 <code>prometheus api server</code> ；</li>
<li>创建 <code>client/cluster/hb_front/hb_back</code> 消息管理器 <code>SocketMessenger</code> ；</li>
<li>创建 <code>store</code> 对象；</li>
<li>创建、初始化、启动 <code>crimson osd</code> 对象；</li>
</ul>
<h2 id="9-1、消息管理器创建逻辑"><a href="#9-1、消息管理器创建逻辑" class="headerlink" title="9.1、消息管理器创建逻辑"></a>9.1、消息管理器创建逻辑</h2><p>通过调用 <code>crimson::net::Messenger::create</code> 函数来依次创建 <code>client/cluster/hb_front/hb_back</code> 消息管理器，最终创建的对象类型为 <code>SocketMessenger</code> 。</p>
<p>其中创建 <code>client/cluster</code> 消息对象的时候 <code>dispatch_only_on_this_shard</code> 参数为 <code>false</code> ，意味着接收到的消息可能会交由其他的 <code>shard</code> 进行处理；创建 <code>hb_front/hb_back</code> 消息对象的时候 <code>dispatch_only_on_this_shard</code> 参数为 <code>true</code> ，意味着接收到的消息仅会由当前 <code>shard</code> 处理。</p>
<h2 id="9-2、store-对象创建逻辑"><a href="#9-2、store-对象创建逻辑" class="headerlink" title="9.2、store 对象创建逻辑"></a>9.2、store 对象创建逻辑</h2><p>通过调用 <code>crimson::os::FuturizedStore::create</code> 函数来创建 <code>store</code> 对象。根据 <code>osd_objectstore</code> 和 <code>osd_data</code> 参数来配置 <code>store</code> 对象。其中 <code>osd_objectstore</code> 参数指定了后端对象存储的类型，支持的参数有 <code>alienstore/cyanstore/seastore</code> ，默认为 <code>alienstore</code> （即后端存储为 <code>bluestore</code> ）。其中 <code>osd_data</code> 参数指定了数据存储目录（比如当使用 <code>vstart.sh</code> 部署集群时，对应的配置默认为 <code>./build/dev/osd$id</code> ）。</p>
<p><strong>crimson 支持以下三个 objectstore 后端:</strong></p>
<ul>
<li>alienstore: 提供与早期版本的对象存储（即 BlueStore）的兼容性。</li>
<li>cyanstore: 用于测试的模拟后端，由易失性内存实施。此对象存储在典型的 osd 中的 memstore 后建模。</li>
<li>seastore: 为 crimson osd 设计的新对象存储。对多个分片支持的路径因后端的特定目标而异。</li>
</ul>
<h2 id="9-3、crimson-osd-mkfs-逻辑"><a href="#9-3、crimson-osd-mkfs-逻辑" class="headerlink" title="9.3、crimson osd mkfs 逻辑"></a>9.3、crimson osd mkfs 逻辑</h2><p>由于在启动 <code>osd</code> 组件之前，我们需要初始化 <code>osd</code> 的文件系统环境，为此需要执行 <code>OSD::mkfs</code> 函数（相关操作顺序可以参考 <code>vstart.sh</code> 脚本中在启动 <code>osd</code> 组件的步骤，其中在启动 <code>osd</code> 之前需要先对其存储路径的环境执行 <code>mkfs</code> 操作。）</p>
<p><strong>OSD::mkfs 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> store.start()</span><br><span class="line">store.mkfs(osd_uuid) <span class="comment">// 重点</span></span><br><span class="line"><span class="number">2.</span> store.mount()</span><br><span class="line"><span class="number">3.</span> open_or_create_meta_coll(store)</span><br><span class="line"><span class="number">4.</span> _write_superblock(...)</span><br><span class="line"><span class="number">5.</span> store.write_meta(...)</span><br><span class="line"><span class="number">6.</span> store.umount()</span><br><span class="line"><span class="number">7.</span> store.stop()</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>1. store.start()</strong></p>
</blockquote>
<p>由于 <code>store</code> 的类型存在三种： <code>alienstore/cyanstore/seastore</code> ， 所以对应的 start 逻辑也有三种。由于 <code>alienstore</code> 只是 <code>bluestore</code> 的代理，且实现比较简单，为此不做介绍；而 <code>cyanstore</code> 是作为一个内存存储模块而存在，仅作为开发测试使用，为此这里也不做介绍；所以以下仅介绍 <code>seastore</code> 的实现逻辑，对应的函数为 <code>SeaStore::start</code> 。</p>
<p><strong>SeaStore::start 函数中关联逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> Device::make_device(root, d_type)</span><br><span class="line"><span class="number">2.</span> device-&gt;start()</span><br><span class="line"><span class="number">3.</span> shard_stores.start(root, device.get(), is_test)</span><br></pre></td></tr></table></figure>

<p><strong>1. Device::make_device(root, d_type) 逻辑解析:</strong><br>在 <code>seastore</code> 中有一个 <code>seastore_main_device_type</code> 参数，用于设置 <code>seastore</code> 主设备的类型，可选值为 <code>SSD/RANDOM_BLOCK_SSD</code> （代码中还实现了 <code>HDD/ZBD</code> ，但是目前并不支持） ，默认为 <code>SSD</code> 。 </p>
<p><code>Device::make_device(root, d_type)</code> 函数内部在创建 <code>device</code> 的过程中，会针对不同的设备类型又做了一些区分，详细的类别分类如下:</p>
<table>
<thead>
<tr>
<th align="center">device_type</th>
<th align="center">backend_type</th>
<th align="center">create func</th>
</tr>
</thead>
<tbody><tr>
<td align="center">HDD</td>
<td align="center">backend_type_t::SEGMENTED</td>
<td align="center">SegmentManager::get_segment_manager</td>
</tr>
<tr>
<td align="center">SSD</td>
<td align="center">backend_type_t::SEGMENTED</td>
<td align="center">SegmentManager::get_segment_manager</td>
</tr>
<tr>
<td align="center">ZBD</td>
<td align="center">backend_type_t::SEGMENTED</td>
<td align="center">SegmentManager::get_segment_manager</td>
</tr>
<tr>
<td align="center">RANDOM_BLOCK_SSD</td>
<td align="center">backend_type_t::RANDOM_BLOCK</td>
<td align="center">get_rb_device</td>
</tr>
</tbody></table>
<p>由于 <code>seastore_main_device_type</code> 默认为 <code>SSD</code> ，所以会通过 <code>SegmentManager::get_segment_manager</code> 函数来来创建一个 <code>segment_manager::block::BlockSegmentManager</code> 对象。</p>
<p><strong>2. device-&gt;start() 逻辑解析:</strong><br>当执行 <code>device-&gt;start()</code> 的时候，调用的就是 <code>BlockSegmentManager::start</code> 方法，继而调用的是 <code>shard_devices.start(device_path, superblock.config.spec.dtype)</code> ，由于 <code>shard_devices</code> 的类型为 <code>seastar::sharded&lt;NVMeBlockDevice&gt;</code> , 所以这里相当于调用了 <code>seastar::sharded::start</code> 函数来初始化了 <code>BlockSegmentManager</code> 对象。</p>
<p><strong>3. shard_stores.start(root, device.get(), is_test) 逻辑解析:</strong><br>之后的 <code>shard_stores.start(root, device.get(), is_test)</code> 函数执行中，由于 <code>shard_stores</code> 也是一个 <code>seastar::sharded</code> 封装的对象，所以其内部相当于调用了 <code>seastar::sharded::start</code> 函数来初始化了 <code>SeaStore::Shard</code> 对象。</p>
<blockquote>
<p><strong>2. store.mount()</strong></p>
</blockquote>
<p><code>store.mount()</code> 函数对应的是 <code>SeaStore::mount</code> 函数。</p>
<p><strong>SeaStore::mount 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">device-&gt;mount()</span><br><span class="line">device-&gt;get_sharded_device().get_secondary_devices()</span><br><span class="line">Device::make_device(path, dtype)</span><br><span class="line">sec_dev-&gt;start()</span><br><span class="line">sec_dev-&gt;mount()</span><br><span class="line">set_secondaries()</span><br></pre></td></tr></table></figure>

<p><code>device-&gt;mount()</code> 函数对应的是 BlockSegmentManager::mount 函数，这个之前解释过，其内部通过调用 <code>shard_devices.invoke_on_all</code> 来触发在每个 <code>shard</code> 中执行 <code>local_device.shard_mount()</code> 函数，因此每个 shard 中调用的函数其实是 <code>BlockSegmentManager::shard_mount()</code> ，该函数内部的执行逻辑主要包括打开 <code>device</code> ，读取 <code>superblock</code> 信息，校验 <code>superblock</code> 信息，更新 <code>tracker</code> 信息等。</p>
<blockquote>
<p><strong>3. open_or_create_meta_coll(store)</strong></p>
</blockquote>
<p><code>open_or_create_meta_coll(store)</code> 对应的函数是 <code>OSD::open_or_create_meta_coll</code> 。</p>
<p><strong>OSD::open_or_create_meta_coll 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">store.get_sharded_store().open_collection(<span class="type">coll_t</span>::meta())</span><br><span class="line">store.get_sharded_store().create_new_collection(<span class="type">coll_t</span>::meta())</span><br><span class="line">OSDMeta(ch, store.get_sharded_store())</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>4. _write_superblock(…)</strong></p>
</blockquote>
<p><code>_write_superblock(...)</code> 的完整调用为 <code>_write_superblock(store, std::move(meta_coll), std::move(superblock))</code> ，其对应的函数是 <code>OSD::_write_superblock</code> 。其内部主要的逻辑为将 <code>superblock</code> 信息写入存储中。</p>
<p><strong>OSD::_write_superblock 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">meta_coll.load_superblock()</span><br><span class="line">meta_coll.create(t)</span><br><span class="line">meta_coll.store_superblock(t, superblock)</span><br><span class="line">store.get_sharded_store().do_transaction(meta_coll.collection(), <span class="built_in">std</span>::move(t))</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>5. store.write_meta(…)</strong></p>
</blockquote>
<p>store.write_meta(…) 对应很多写元信息的操作，操作的元信息包括 <code>ceph_fsid</code> ，<code>magic</code> ，<code>whoami</code> ，<code>osd_key</code> ， <code>osdspec_affinity</code> ， <code>ready</code> 等字段。这些信息位于 <code>osd</code> 运行目录的各个配置对应的文件中。</p>
<blockquote>
<p><strong>6. store.umount()</strong></p>
</blockquote>
<p><code>store.umount()</code> 对应的函数为 <code>SeaStore::umount</code> ， 其内部会同通过调用 <code>shard_stores.invoke_on_all</code> 函数，让每个 <code>shard</code> 执行 <code>local_store.umount()</code> 函数。</p>
<blockquote>
<p><strong>7. store.stop()</strong></p>
</blockquote>
<p><code>store.stop()</code> 对应的函数为 <code>SeaStore::stop</code> 。</p>
<p><strong>SeaStore::stop 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">sec_dev-&gt;stop()</span><br><span class="line">secondaries.clear()</span><br><span class="line">device-&gt;stop()</span><br><span class="line">shard_stores.stop()</span><br></pre></td></tr></table></figure>



<h3 id="9-3-1、store-mkfs-osd-uuid"><a href="#9-3-1、store-mkfs-osd-uuid" class="headerlink" title="9.3.1、store.mkfs(osd_uuid)"></a>9.3.1、store.mkfs(osd_uuid)</h3><p><strong>SeaStore::mkfs 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> read_meta(<span class="string">&quot;mkfs_done&quot;</span>)</span><br><span class="line"><span class="number">2.</span> seastar::open_directory(root)</span><br><span class="line">        root_f-&gt;list_directory(...)</span><br><span class="line">            Device::make_device(path, dtype)</span><br><span class="line">            secondaries.emplace_back(<span class="built_in">std</span>::move(sec_dev))</span><br><span class="line">            p_sec_dev-&gt;start()</span><br><span class="line">            p_sec_dev-&gt;mkfs()</span><br><span class="line">            set_secondaries()</span><br><span class="line"><span class="number">3.</span> device-&gt;mkfs(...)</span><br><span class="line"><span class="number">4.</span> device-&gt;mount()</span><br><span class="line"><span class="number">5.</span> local_store.mkfs_managers() <span class="comment">// shard_stores.invoke_on_all(...) // 重点</span></span><br><span class="line"><span class="number">6.</span> prepare_meta(new_osd_fsid)</span><br><span class="line"><span class="number">7.</span> umount()</span><br></pre></td></tr></table></figure>



<blockquote>
<ol>
<li>read_meta(“mkfs_done”)</li>
</ol>
</blockquote>
<p><code>read_meta(&quot;mkfs_done&quot;)</code> 用于校验之前是否已经执行过 <code>mkfs</code> 操作，监测方式为读取 <code>store</code> 目录中的 <code>mkfs_done</code> 文件中的内容。</p>
<blockquote>
<ol start="2">
<li>seastar::open_directory(root)</li>
</ol>
</blockquote>
<p><code>seastar::open_directory(root)</code> 的逻辑为检索 <code>store</code> 目录中的文件，筛选前缀名为 <code>block.</code> 的文件&#x2F;目录，通过解析该文件&#x2F;目录的后缀，从而尝试调用 <code>Device::make_device(path, dtype)</code> 函数来创建对应的 <code>device</code> ， 进而操作对应的 <code>device</code> 执行 <code>start</code> 和 <code>mkfs</code> 函数操作。</p>
<blockquote>
<ol start="3">
<li>device-&gt;mkfs(…)</li>
</ol>
</blockquote>
<p><code>device-&gt;mkfs(...)</code> 对应的完整函数为 <code>device-&gt;mkfs(device_config_t::create_primary(new_osd_fsid, id, d_type, sds))</code> ， 由于 <code>seastore_main_device_type</code> 默认为 <code>SSD</code> ，所以这里的 <code>device-&gt;mkfs</code> 指的是 <code>BlockSegmentManager::mkfs</code> 函数。</p>
<p><code>BlockSegmentManager::mkfs</code> 函数中关键逻辑为:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">shard_devices.local().primary_mkfs(sm_config)</span><br><span class="line">    check_create_device(device_path, size)</span><br><span class="line">    open_device(device_path)</span><br><span class="line">    make_superblock(get_device_id(), sm_config, stat)</span><br><span class="line">    write_superblock(get_device_id(), device, sb)</span><br><span class="line">    device.close()</span><br><span class="line">local_device.shard_mkfs() <span class="comment">// shard_devices.invoke_on_all(...)</span></span><br><span class="line">    open_device(device_path)</span><br><span class="line">    read_superblock(device, sd)</span><br><span class="line">    sb.validate()</span><br><span class="line">    tracker.reset(new SegmentStateTracker(shard_info.segments, sb.block_size))</span><br><span class="line">    tracker-&gt;write_out(get_device_id(), device, shard_info.tracker_offset)</span><br><span class="line">    device.close()</span><br></pre></td></tr></table></figure>

<p>其中 <code>shard_devices.local().primary_mkfs(sm_config)</code> 对应的函数为 <code>BlockSegmentManager::primary_mkfs</code> 。其内部逻辑如下:</p>
<ul>
<li><code>check_create_device(device_path, size)</code>: 通过 <code>seastar::open_file_dma</code> 函数来打开对应的 <code>block</code> 文件，并通过 <code>f.truncate</code> 和 <code>f.allocate(0, size)</code> 函数来调整对应文件的大小，用于后续存储数据。该步骤中的 <code>seastore_block_create</code> 配置用于控制是否创建 <code>block</code> ， 该参数默认为 <code>true</code> ；<code>seastore_device_size</code> 配置用于控制 <code>block</code> 的文件大小，该参数默认为 <code>50GB</code> 。</li>
<li><code>open_device(device_path)</code>: 通过 <code>seastar::open_file_dma</code> 方法来打开对应的 <code>block</code> 文件，用于后续的数据操作。</li>
<li><code>make_superblock(get_device_id(), sm_config, stat)</code>: 初始化 <code>superblock</code> 信息。其内部根据 <code>seastar::smp::count</code> 的数量，<code>seastore_segment_size</code> 参数（用于控制单个 <code>segment</code> 的大小，默认为 <code>64M</code> ）等信息来初始化 <code>superblock</code> 信息。</li>
<li><code>write_superblock(get_device_id(), device, sb)</code>: 将序列化后的 <code>superblock</code> 信息写入 <code>block</code> 的文件头部。</li>
<li><code>device.close()</code>: 关闭打开的 <code>device</code> 。</li>
</ul>
<p>之后通过调用 <code>shard_devices.invoke_on_all(...)</code> 函数，该函数是 <code>Seastar</code> 框架中使用的方法，用于在所有的 <code>seastar shard</code> 上执行给定的函数。之后每个 <code>shard</code> 上执行 <code>local_device.shard_mkfs()</code> 函数。其内部回依次打开 <code>device</code> ，读取 <code>superblock</code> 信息，校验 <code>superblock</code> 信息，更新 <code>tracker</code> 信息等；之后便关闭 <code>device</code> 。</p>
<blockquote>
<ol start="4">
<li>device-&gt;mount()</li>
</ol>
</blockquote>
<p><code>device-&gt;mount()</code> 对应的函数为 <code>BlockSegmentManager::mount</code> 。</p>
<p><code>BlockSegmentManager::mount</code> 函数中关键逻辑为:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">local_device.shard_mount() <span class="comment">// shard_devices.invoke_on_all(...)</span></span><br></pre></td></tr></table></figure>

<p>这里也是通过调用 <code>shard_devices.invoke_on_all</code> 来触发在每个 <code>shard</code> 中执行 <code>local_device.shard_mount()</code> 函数，因此每个 shard 中调用的函数其实是 <code>BlockSegmentManager::shard_mount()</code> ，该函数内部的执行逻辑主要包括打开 <code>device</code> ，读取 <code>superblock</code> 信息，校验 <code>superblock</code> 信息，更新 <code>tracker</code> 信息等。</p>
<blockquote>
<ol start="5">
<li>local_store.mkfs_managers()</li>
</ol>
</blockquote>
<p>接着又通过调用 <code>shard_stores.invoke_on_all(...)</code> 来触发在每个 <code>shard</code> 中执行 <code>local_store.mkfs_managers()</code> 操作，对应的函数为 <code>SeaStore::Shard::mkfs_managers</code> 。</p>
<p><code>SeaStore::Shard::mkfs_managers</code> 函数中关键逻辑为:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">init_managers()</span><br><span class="line">transaction_manager-&gt;mkfs()</span><br><span class="line">init_managers()</span><br><span class="line">transaction_manager-&gt;mount()</span><br><span class="line">repeat_eagain(...)</span><br><span class="line">    transaction_manager-&gt;with_transaction_intr(...)</span><br><span class="line">        onode_manager-&gt;mkfs(t)</span><br><span class="line">        collection_manager-&gt;mkfs(t)</span><br><span class="line">        transaction_manager-&gt;write_collection_root(t, coll_root)</span><br><span class="line">        transaction_manager-&gt;submit_transaction(t)</span><br></pre></td></tr></table></figure>

<p>其中 <code>init_managers()</code> 函数指的是 <code>SeaStore::Shard::init_managers()</code> 函数，其内部会初始化 <code>transaction_manager</code> ， <code>collection_manager</code> ， <code>onode_manager</code> 对象。</p>
<ul>
<li><code>transaction_manager</code>: 初始化函数为 <code>TransactionManagerRef make_transaction_manager</code> ，该对象显然用于管理存储设备上的事务。</li>
<li><code>collection_manager</code>: 初始化函数为 <code>FlatCollectionManager::FlatCollectionManager</code> ；</li>
<li><code>onode_manager</code>: 初始化函数为 <code>FLTreeOnodeManager::FLTreeOnodeManager</code> ；</li>
</ul>
<p><strong>transaction_manager 相关执行逻辑:</strong></p>
<ul>
<li><code>transaction_manager-&gt;mkfs()</code>: 对应 <code>TransactionManager::mkfs</code> 函数；</li>
<li><code>transaction_manager-&gt;mount()</code>: 对应 <code>TransactionManager::mount</code> 函数；</li>
<li><code>transaction_manager-&gt;with_transaction_intr(...)</code>: 对应 <code>ExtentCallbackInterface::with_transaction_intr</code> 函数；</li>
</ul>
<p>其中 <code>TransactionManager::mkfs</code> 函数中关键逻辑为:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">epm-&gt;mount()</span><br><span class="line">journal-&gt;open_for_mkfs()</span><br><span class="line">epm-&gt;open_for_write()</span><br><span class="line">with_transaction_intr(...)</span><br><span class="line">close()</span><br></pre></td></tr></table></figure>



<p>其中 <code>TransactionManager::mount</code> 函数中关键逻辑为:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">cache-&gt;init()</span><br><span class="line">epm-&gt;mount()</span><br><span class="line">journal-&gt;replay(...)</span><br><span class="line">journal-&gt;open_for_mount()</span><br><span class="line">journal-&gt;get_trimmer().set_journal_head(start_seq)</span><br><span class="line">with_transaction_weak(...)</span><br><span class="line">epm-&gt;open_for_write()</span><br><span class="line">epm-&gt;start_background()</span><br></pre></td></tr></table></figure>

<p>TODO:</p>
<p><strong>onode_manager 相关执行逻辑:</strong></p>
<p>相关操作为 onode_manager-&gt;mkfs(t) ， 对应的函数为 FLTreeOnodeManager::mkfs 函数。 之后继续调用 Btree::mkfs &#x3D;&gt;  Node::mkfs </p>
<p>TODO:</p>
<p><strong>collection_manager 相关执行逻辑:</strong></p>
<p>相关操作为 collection_manager-&gt;mkfs(t) </p>
<p>TODO:</p>
<blockquote>
<ol start="6">
<li>prepare_meta(new_osd_fsid)</li>
</ol>
</blockquote>
<p><code>prepare_meta(new_osd_fsid)</code> 函数对应的是 <code>SeaStore::prepare_meta</code> 函数，其内部主要是写入一些元信息到对应的数据目录的文件中，包括向 <code>fsid</code> 文件中写入集群 id 信息；向 <code>type</code> 文件中写入后后端存储类型（比如 <code>seastore</code> ） ； 往 <code>mkfs_done</code> 文件中写入 <code>yes</code> 。</p>
<blockquote>
<ol start="7">
<li>umount()</li>
</ol>
</blockquote>
<p><code>umount()</code> 函数对应的是 <code>SeaStore::umount</code> 函数，其内部会通过 <code>shard_stores.invoke_on_all</code> 函数通知所有的 <code>shard</code> 执行 <code>local_store.umount()</code> 操作。</p>
<h2 id="9-4、crimson-osd-start-逻辑"><a href="#9-4、crimson-osd-start-逻辑" class="headerlink" title="9.4、crimson osd start 逻辑"></a>9.4、crimson osd start 逻辑</h2><p>当 <code>osd</code> 通过 <code>mkfs</code> 初始化之后才会被正式的启动，这时候就会调用 <code>OSD::start</code> 函数启动。需要注意该函数内部限制当前的 <code>shard</code> 为 <code>PRIMARY_CORE</code> 。其中 <code>store.start()</code> 和 <code>store.mount()</code> 的执行逻辑之前在 <code>osd mkfs</code> 的逻辑中已经描述过了，这里不再赘述。部分实现比较详细或逻辑接近，因此放在一块一起解释。</p>
<p><strong>OSD::start 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">store.start()</span><br><span class="line"><span class="number">1.</span> pg_to_shard_mappings.start(...)</span><br><span class="line"><span class="number">2.</span> osd_singleton_state.start_single(...)</span><br><span class="line"><span class="number">3.</span> osd_states.start()</span><br><span class="line"><span class="number">4.</span> shard_services.start(...)</span><br><span class="line"><span class="number">5.</span> heartbeat.reset(...)</span><br><span class="line">store.mount()</span><br><span class="line"><span class="number">6.</span> local_service.report_stats() <span class="comment">// shard_services.invoke_on_all(...)</span></span><br><span class="line"><span class="number">7.</span> store.report_stats()</span><br><span class="line"><span class="number">8.</span> stats_timer.arm_periodic(...)</span><br><span class="line"><span class="number">9.</span> open_meta_coll()</span><br><span class="line"><span class="number">10.</span> pg_shard_manager.get_meta_coll().load_superblock()</span><br><span class="line"><span class="number">11.</span> pg_shard_manager.set_superblock(superblock)</span><br><span class="line"><span class="number">12.</span> pg_shard_manager.get_local_map(superblock.current_epoch)</span><br><span class="line"><span class="number">13.</span> pg_shard_manager.update_map(<span class="built_in">std</span>::move(<span class="built_in">map</span>))</span><br><span class="line"><span class="number">14.</span> local_service.local_state.osdmap_gate.got_map(...) <span class="comment">// shard_services.invoke_on_all(...)</span></span><br><span class="line"><span class="number">15.</span> pg_shard_manager.load_pgs(store)</span><br><span class="line"><span class="number">16.</span> cluster_msgr-&gt;bind(pick_addresses(CEPH_PICK_ADDRESS_CLUSTER))</span><br><span class="line">    cluster_msgr-&gt;start(dispatchers)</span><br><span class="line">    public_msgr-&gt;bind(pick_addresses(CEPH_PICK_ADDRESS_PUBLIC))</span><br><span class="line">    public_msgr-&gt;start(dispatchers)</span><br><span class="line"><span class="number">17.</span> monc-&gt;start()</span><br><span class="line">    mgrc-&gt;start()</span><br><span class="line"><span class="number">18.</span> _add_me_to_crush()</span><br><span class="line"><span class="number">19.</span> monc-&gt;renew_subs()</span><br><span class="line"><span class="number">20.</span> heartbeat-&gt;start(...)</span><br><span class="line"><span class="number">21.</span> start_asok_admin()</span><br><span class="line"><span class="number">22.</span> log_client.set_fsid(monc-&gt;get_fsid())</span><br><span class="line"><span class="number">23.</span> start_boot()</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>1. pg_to_shard_mappings.start(…)</strong></p>
</blockquote>
<p><code>pg_to_shard_mappings.start(...)</code> 的原始调用信息为 <code>pg_to_shard_mappings.start(0, seastar::smp::count)</code> 。由于 <code>pg_to_shard_mappings</code> 的定义为 <code>seastar::sharded&lt;PGShardMapping&gt; pg_to_shard_mappings</code> ，因此这里的 <code>start</code> 函数其实是调用 <code>seastar::sharded::start</code> 函数来初始化了 <code>PGShardMapping</code> 对象。在 <code>PGShardMapping</code> 对象初始化的过程中会向其内部的成员变量 <code>std::map&lt;core_id_t, unsigned&gt; core_to_num_pgs</code> 中添加 <code>seastar::smp::count</code> 个元素。</p>
<blockquote>
<p><strong>2. osd_singleton_state.start_single(…)</strong></p>
</blockquote>
<p><code>osd_singleton_state.start_single(...)</code> 的原始调用信息为 <code>osd_singleton_state.start_single(whoami, std::ref(*cluster_msgr), std::ref(*public_msgr), std::ref(*monc), std::ref(*mgrc))</code> 。由于 <code>osd_singleton_state</code> 的定义为 <code>seastar::sharded&lt;OSDSingletonState&gt; osd_singleton_state</code> ，因此这里的 <code>start_single</code> 函数其实是调用了 <code>seastar::sharded::start_single</code> 函数来创建了一个 <code>OSDSingletonState</code> 对象。在 <code>OSDSingletonState</code> 对象初始化的过程中会创建一些 <code>perf</code> 和 <code>recoverystate_perf</code> 对象指针。</p>
<blockquote>
<p><strong>3. osd_states.start()</strong></p>
</blockquote>
<p>由于 <code>osd_states</code> 的定义为 <code>seastar::sharded&lt;OSDState&gt; osd_states</code> ，因此这里的 <code>start</code> 函数其实是调用 <code>seastar::sharded::start</code> 函数来初始化了 <code>OSDState</code> 对象。</p>
<blockquote>
<p><strong>4. shard_services.start(…)</strong></p>
</blockquote>
<p><code>shard_services.start(...)</code> 的原始调用信息为 <code>shard_services.start(std::ref(osd_singleton_state), std::ref(pg_to_shard_mappings), whoami, startup_time, osd_singleton_state.local().perf, osd_singleton_state.local().recoverystate_perf, std::ref(store), std::ref(osd_states))</code> 。由于 <code>shard_services</code> 的定义为 <code>seastar::sharded&lt;ShardServices&gt; shard_services</code> ，因此这里的 <code>start</code> 函数其实是调用 <code>seastar::sharded::start</code> 函数来初始化了 <code>ShardServices</code> 对象。</p>
<blockquote>
<p><strong>5. heartbeat.reset(…)</strong></p>
</blockquote>
<p>重置 <code>heartbeat</code> 对象。</p>
<blockquote>
<p><strong>6. local_service.report_stats()</strong></p>
</blockquote>
<p>该函数的调用被封装在 <code>shard_services.invoke_on_all</code> 内部，意味着这会让每个 <code>shard</code> 执行 <code>local_service.report_stats()</code> 函数。但是只有在 <code>crimson_osd_stat_interval</code> 配置了非零的情况下才会执行该逻辑。 <code>crimson_osd_stat_interval</code> 参数默认为 <code>0</code> 。</p>
<blockquote>
<p><strong>7. store.report_stats()</strong></p>
</blockquote>
<p><code>store.report_stats()</code> 对应的函数为 <code>SeaStore::report_stats</code> 。</p>
<p><strong>SeaStore::report_stats 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">local_store.get_device_stats(report_detail) <span class="comment">// shard_stores.invoke_on_all</span></span><br><span class="line">local_store.get_io_stats(report_detail) <span class="comment">// shard_stores.invoke_on_all</span></span><br><span class="line">INFO(...);</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>8. stats_timer.arm_periodic(…)</strong></p>
</blockquote>
<p><code>stats_timer.arm_periodic(...)</code> 对应的原始调用为 <code>stats_timer.arm_periodic(std::chrono::seconds(stats_seconds))</code> 。用于设置一个周期性的定时器，该定时器的运行是由 <code>Seastar</code> 框架的事件循环管理的，与函数调用的生命周期无关。</p>
<blockquote>
<p><strong>9. open_meta_coll</strong></p>
</blockquote>
<p><code>open_meta_coll</code> 对应的函数为 <code>OSD::open_meta_coll</code> 。需要注意该逻辑仅限 <code>PRIMARY_CORE</code> 对应的 <code>shard</code> 执行。</p>
<p><strong>SeaStore::report_stats 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">store.get_sharded_store().open_collection(<span class="type">coll_t</span>::meta())</span><br><span class="line">pg_shard_manager.init_meta_coll(ch, store.get_sharded_store())</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>10. pg_shard_manager.get_meta_coll().load_superblock()</strong></p>
</blockquote>
<p>对应的函数为 <code>OSDMeta::load_superblock</code> 。用于从 <code>store</code> 存储中读取 <code>superblock</code> 信息。</p>
<blockquote>
<p><strong>11. pg_shard_manager.set_superblock(superblock)</strong></p>
</blockquote>
<p>对应的函数为 <code>PGShardManager::set_superblock</code> 。 </p>
<p><strong>PGShardManager::set_superblock 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">get_osd_singleton_state().set_singleton_superblock(superblock)</span><br><span class="line">local_service.local_state.update_shard_superblock(superblock) <span class="comment">// shard_services.invoke_on_all</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>12. pg_shard_manager.get_local_map(superblock.current_epoch)</strong></p>
</blockquote>
<p>对应的函数为 <code>OSDSingletonState::get_local_map</code> 。</p>
<blockquote>
<p><strong>13. pg_shard_manager.update_map(std::move(map))</strong></p>
</blockquote>
<p>对应的函数为 <code>PGShardManager::update_map</code> 。</p>
<p><strong>PGShardManager::update_map 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">get_osd_singleton_state().update_map(...)</span><br><span class="line">local.local_state.update_map(...) <span class="comment">// shard_services.invoke_on_all</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>14. local_service.local_state.osdmap_gate.got_map(…)</strong></p>
</blockquote>
<p>原始的调用为 <code>local_service.local_state.osdmap_gate.got_map(osdmap-&gt;get_epoch())</code> ， 该函数的调用被封装在 <code>shard_services.invoke_on_all</code> 内部，意味着这会让每个 <code>shard</code> 执行 <code>local_service.local_state.osdmap_gate.got_map(osdmap-&gt;get_epoch())</code> 函数。</p>
<blockquote>
<p><strong>15. pg_shard_manager.load_pgs(store)</strong></p>
</blockquote>
<p>对应的函数为 <code>PGShardManager::load_pgs</code> 。</p>
<p><strong>PGShardManager::load_pgs 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">store.list_collections()</span><br><span class="line"><span class="comment">// seastar::parallel_for_each</span></span><br><span class="line">get_pg_to_shard_mapping().get_or_create_pg_mapping(pgid, shard_core)</span><br><span class="line">shard_services.load_pg(pgid)</span><br><span class="line">per_shard_state.pg_map.pg_loaded(pgid, <span class="built_in">std</span>::move(pg))</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>16. cluster_msgr 和 public_msgr</strong></p>
</blockquote>
<p><strong>对应的批量的原始调用为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">cluster_msgr-&gt;bind(pick_addresses(CEPH_PICK_ADDRESS_CLUSTER))</span><br><span class="line">cluster_msgr-&gt;start(dispatchers)</span><br><span class="line">public_msgr-&gt;bind(pick_addresses(CEPH_PICK_ADDRESS_PUBLIC))</span><br><span class="line">public_msgr-&gt;start(dispatchers)</span><br></pre></td></tr></table></figure>

<p><code>pick_addresses</code> 函数执行的时候，其内部仅会选择 <code>message v2</code> 的地址，因此从这里可以看出在 <code>crimson osd</code> 中不支持 <code>message v1</code> 。</p>
<p><code>bind</code> 函数对应的是 <code>SocketMessenger::bind</code> 。 <code>start</code> 函数对应的是 <code>SocketMessenger::start</code> 。</p>
<p><strong>SocketMessenger::bind 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">try_bind(addrs, local_conf()-&gt;ms_bind_port_min, local_conf()-&gt;ms_bind_port_max)</span><br><span class="line">do_listen(<span class="type">entity_addrvec_t</span>&#123;to_bind&#125;)</span><br><span class="line">ShardedServerSocket::create(dispatch_only_on_sid)</span><br><span class="line">listener-&gt;listen(listen_addr)</span><br><span class="line">    seastar::listen(s_addr, lo) <span class="comment">// this-&gt;container().invoke_on_all</span></span><br></pre></td></tr></table></figure>

<p>从上面中可以看出会让每个 <code>shard</code> 都监听相同的端口。</p>
<p><strong>SocketMessenger::start 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">listener-&gt;accept([this](SocketRef _socket, <span class="type">entity_addr_t</span> peer_addr) &#123;</span><br><span class="line">    assert(get_myaddr().is_msgr2());</span><br><span class="line">    SocketFRef socket = seastar::make_foreign(<span class="built_in">std</span>::move(_socket));</span><br><span class="line">    <span class="comment">// 对于 client 和 cluster 的消息，这里的 fix 是 false </span></span><br><span class="line">    <span class="comment">// 对于 heart beat 的消息，这里的 fix 是 true</span></span><br><span class="line">    <span class="keyword">if</span> (listener-&gt;is_fixed_shard_dispatching()) &#123;</span><br><span class="line">        <span class="keyword">return</span> accept(<span class="built_in">std</span>::move(socket), peer_addr);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 转发请求到对应的 shard 中</span></span><br><span class="line">        <span class="keyword">return</span> seastar::smp::submit_to(sid, [this, peer_addr, socket = <span class="built_in">std</span>::move(socket)]() mutable &#123;</span><br><span class="line">            <span class="keyword">return</span> accept(<span class="built_in">std</span>::move(socket), peer_addr);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>17. monc 和 mgrc 的 start</strong></p>
</blockquote>
<p><strong>对应的批量的原始调用为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">monc-&gt;start()</span><br><span class="line">mgrc-&gt;start()</span><br></pre></td></tr></table></figure>

<p>其中 <code>monc-&gt;start()</code> 对应的函数为 <code>crimson::mon::Client::start</code> 。 <code>mgrc-&gt;start()</code> 对应的函数为 <code>crimson::mgr::Client::start</code> 。</p>
<p><strong>crimson::mon::Client::start 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">auth_registry.refresh_config()</span><br><span class="line">load_keyring()</span><br><span class="line">monmap.build_initial(crimson::common::local_conf(), <span class="literal">false</span>)</span><br><span class="line">authenticate()</span><br><span class="line">timer.arm_periodic(interval)</span><br></pre></td></tr></table></figure>


<p><strong>crimson::mgr::Client::start 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">seastar::now()</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>18. _add_me_to_crush()</strong></p>
</blockquote>
<p>该函数对应的是 <code>OSD::_add_me_to_crush</code> 。在该函数中，如果 <code>osd_crush_update_on_start</code> 配置为 <code>true</code> ，则会在 <code>osd</code> 启动时尝试将自己的信息添加到 <code>crush map</code> 中。</p>
<p><strong>OSD::_add_me_to_crush 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">local_conf().get_val&lt;<span class="type">bool</span>&gt;(<span class="string">&quot;osd_crush_update_on_start&quot;</span>)</span><br><span class="line">local_conf().get_val&lt;<span class="type">double</span>&gt;(<span class="string">&quot;osd_crush_initial_weight&quot;</span>)</span><br><span class="line">store.stat()</span><br><span class="line">get_weight()</span><br><span class="line">loc.init_on_startup()</span><br><span class="line">monc-&gt;run_command(<span class="built_in">std</span>::move(cmd), &#123;&#125;)</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>19. monc-&gt;renew_subs()</strong></p>
</blockquote>
<p>对应的函数为 <code>crimson::mon::Client::renew_subs</code> 。 内部逻辑为向 <code>monitor</code> 发送 <code>CEPH_MSG_MON_SUBSCRIBE</code> 消息，用于订阅 <code>osd_pg_creates</code> ， <code>mgrmap</code> ， <code>osdmap</code> 的变更消息。</p>
<blockquote>
<p><strong>20. heartbeat-&gt;start(…)</strong></p>
</blockquote>
<p>原始的调用为 <code>heartbeat-&gt;start(pick_addresses(CEPH_PICK_ADDRESS_PUBLIC), pick_addresses(CEPH_PICK_ADDRESS_CLUSTER))</code> , 对应的函数为 <code>Heartbeat::start</code> 。</p>
<blockquote>
<p><strong>21. start_asok_admin()</strong></p>
</blockquote>
<p>对应的函数为 <code>OSD::start_asok_admin</code> 。 用于创建本地的 <code>socket</code> 文件，并注册可执行的命令。</p>
<blockquote>
<p><strong>22. log_client.set_fsid(monc-&gt;get_fsid())</strong></p>
</blockquote>
<p>设置日志记录中的 <code>fsid</code> 信息。</p>
<blockquote>
<p><strong>23. start_boot()</strong></p>
</blockquote>
<p>对应的函数为 <code>OSD::start_boot</code> 。</p>
<p><strong>OSD::start_boot 函数中关键逻辑为:</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">pg_shard_manager.set_preboot()</span><br><span class="line">monc-&gt;get_version(<span class="string">&quot;osdmap&quot;</span>)</span><br><span class="line">_preboot(oldest, newest)</span><br></pre></td></tr></table></figure>






<h1 id="十、相关资料"><a href="#十、相关资料" class="headerlink" title="十、相关资料"></a>十、相关资料</h1><ul>
<li><a target="_blank" rel="noopener" href="https://ceph.io/en/news/crimson/">https://ceph.io/en/news/crimson/</a></li>
<li><a target="_blank" rel="noopener" href="https://ceph.io/en/news/blog/2023/crimson-multi-core-scalability/">https://ceph.io/en/news/blog/2023/crimson-multi-core-scalability/</a></li>
<li><a target="_blank" rel="noopener" href="https://ceph.io/en/news/blog/2025/crimson-T-release/">https://ceph.io/en/news/blog/2025/crimson-T-release/</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/dev/crimson/crimson/">https://docs.ceph.com/en/latest/dev/crimson/crimson/</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/cephadm/install/#bootstrap-a-new-cluster">https://docs.ceph.com/en/latest/cephadm/install/#bootstrap-a-new-cluster</a></li>
<li><a target="_blank" rel="noopener" href="https://www.51cto.com/article/749735.html">https://www.51cto.com/article/749735.html</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/667949613">https://zhuanlan.zhihu.com/p/667949613</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.redhat.com/zh-cn/documentation/red_hat_ceph_storage/7/html/administration_guide/crimson">https://docs.redhat.com/zh-cn/documentation/red_hat_ceph_storage/7/html/administration_guide/crimson</a></li>
<li><a target="_blank" rel="noopener" href="https://ceph.io/en/news/blog/2023/crimson-multi-core-scalability/">https://ceph.io/en/news/blog/2023/crimson-multi-core-scalability/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.icviews.cn/semiCommunity/postDetail/6586">https://www.icviews.cn/semiCommunity/postDetail/6586</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://bugwz.com">bugwz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://bugwz.com/2025/06/01/ceph-cirmson/">https://bugwz.com/2025/06/01/ceph-cirmson/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://bugwz.com" target="_blank">咕咕</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Ceph/">Ceph</a></div><div class="post-share"><div class="social-share" data-image="/assets/images/bg/ceph.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南"><img class="cover" src="/assets/images/bg/ceph.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Ceph Crimson 集群搭建指南</div></div><div class="info-2"><div class="info-item-1">当前 ceph 集群搭建部署的方式主要有三种: ceph-ansible ，vstart.sh ， cephadm 。 其中 vstart.sh 脚本用于在开发环境中快速搭建测试集群； ceph-ansible 是一种部署 ceph 集群的老方式，支持在宿主机及容器部署的方式，目前社区已不推荐使用；cephadm 是当前最新的支持部署生产集群的方式，仅支持容器部署。接下来主要介绍通过 vstart.sh 和 cephadm 部署 crimson 集群的方式。以下测试基于 v19.2.1 版本进行。 一、vstart.sh 搭建集群vstart.sh 常用于在开发环境环境中快速搭建集群，且在部署集群前我们需要编译出对应的二进制包。由于编译环境可能会有各种依赖缺失，版本异常等问题，这里推荐使用 bugwz&#x2F;ceph-images 中提供的 CentOS Stream 9 的编译打包环境。同时后续的集群的搭建也可以在容器内部进行。 搭建集群操作步骤如下:  软件编译: 使用开发容器镜像，编译对应的 ceph 代码，产出对应的二进制运行文件； 集群部署: 在开发容器内部使用...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-12</div><div class="info-item-2">Ceph Crimson 集群搭建指南</div></div><div class="info-2"><div class="info-item-1">当前 ceph 集群搭建部署的方式主要有三种: ceph-ansible ，vstart.sh ， cephadm 。 其中 vstart.sh 脚本用于在开发环境中快速搭建测试集群； ceph-ansible 是一种部署 ceph 集群的老方式，支持在宿主机及容器部署的方式，目前社区已不推荐使用；cephadm 是当前最新的支持部署生产集群的方式，仅支持容器部署。接下来主要介绍通过 vstart.sh 和 cephadm 部署 crimson 集群的方式。以下测试基于 v19.2.1 版本进行。 一、vstart.sh 搭建集群vstart.sh 常用于在开发环境环境中快速搭建集群，且在部署集群前我们需要编译出对应的二进制包。由于编译环境可能会有各种依赖缺失，版本异常等问题，这里推荐使用 bugwz&#x2F;ceph-images 中提供的 CentOS Stream 9 的编译打包环境。同时后续的集群的搭建也可以在容器内部进行。 搭建集群操作步骤如下:  软件编译: 使用开发容器镜像，编译对应的 ceph 代码，产出对应的二进制运行文件； 集群部署: 在开发容器内部使用...</div></div></div></a><a class="pagination-related" href="/2023/04/12/ceph-ansible/" title="ceph-ansible 集群部署运维指南"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-12</div><div class="info-item-2">ceph-ansible 集群部署运维指南</div></div><div class="info-2"><div class="info-item-1">本文详细介绍了使用 ceph-ansible 部署和运维 Ceph 集群的过程，包括各版本及其依赖的 Ansible 版本的对应关系、自定义模块与任务的结构、集群部署、运维操作及相关示例。特别强调了环境配置、节点连通性验证、MDS 和 OSD 组件的管理，以及安全和性能优化注意事项。 一、项目介绍以下分析基于 ceph-ansible stable-6.0 分支代码。 1.1、版本与对应关系目前 ceph-ansible 采用不同的代码分支来支持部署不同版本的 ceph 集群，且每个代码分支需要特定的 ansible 版本支持，具体的对应关系如下（以下对应关系更新于 2025&#x2F;05&#x2F;23 ）：    ceph-ansible 分支 支持的 ceph 版本 依赖的 ansible 核心版本 依赖的 ansible 发布版本包    stable-3.0 Jewel(V10), Luminous(V12) 2.4 -   stable-3.1 Luminous(V12), Mimic(V13) 2.4 -   stable-3.2 Luminous(V12),...</div></div></div></a><a class="pagination-related" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-25</div><div class="info-item-2">Ceph QoS 机制深入分析</div></div><div class="info-2"><div class="info-item-1">一、CephFS QoS社区的相关实现：  基于 tokenbucket 算法的目录 QoS : https://github.com/ceph/ceph/pull/29266 基于 dmclock 算法的 subvolume QoS : 来自日本的 line 公司提出的想法，https://github.com/ceph/ceph/pull/38506 ， https://github.com/ceph/ceph/pull/52147  1.1、基于 TokenBucket 算法的目录 QoS该实现并未合并到主分支。  相关材料：  社区的原始PR: https://github.com/ceph/ceph/pull/29266  实现特点：  基于 TokenBucketThrottle 类在客户端侧实现的 TokenBucket 类型的 QoS，用于约束每个独立的客户端的访问请求； QoS 的限制粒度为每个独立的客户端，没有全局的QoS限制； 用于限制目录级别的操作 QoS； 支持 IOPS 和 BPS 的 QoS 限制，且支持突发流量； 仅支持 FUSE...</div></div></div></a><a class="pagination-related" href="/2023/06/30/ceph-crush/" title="Ceph CRUSH 设计实现剖析"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-30</div><div class="info-item-2">Ceph CRUSH 设计实现剖析</div></div><div class="info-2"><div class="info-item-1">CRUSH（Controlled Replication Under Scalable Hashing）是 Ceph 存储系统中用于数据分布和复制的算法。关于 CRUSH 的论文解析参考: 译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data 。CRUSH map 是 Ceph 集群中一个关键的配置组件，它定义了数据如何在集群的物理硬件上分布。 CRUSH 算法使得 Ceph 能够在无需中心化或者分布式元数据管理器的情况下，高效、可靠地进行数据复制和恢复。 一、CRUSH map 解析CRUSH map 包含了集群的层次结构和各种规则，这些规则定义了数据应该如何在集群中分布。 CRUSH map 主要包含以下几个部分：  Tunables : 一组可用于调整 CRUSH 算法行为的参数。 Devices : 定义集群中所有可用的存储设备的列表。 Types : 定义存储层次结构中的不同层级类型。 Buckets : 组织和管理存储设备（如 OSDs ）的逻辑容器。 Rules :...</div></div></div></a><a class="pagination-related" href="/2023/06/01/ceph-test/" title="Ceph 集群性能测试工具详解"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-01</div><div class="info-item-2">Ceph 集群性能测试工具详解</div></div><div class="info-2"><div class="info-item-1">本文详细介绍了包括 rados bench、rbd bench、dd 、fio 、vdbench 、mdtest 、iozone、cosbench、cbt 等测试工具对于 Ceph 集群的性能压测的使用。对于每个工具都提供了压测命令参数、示例命令等使用说明，实现了对 Ceph 块存储、文件存储、对象存储、rados 对象存储等存储类别的性能压测。文中重点阐述了各命令的使用格式、基本功能和参数选择，为用户在 Ceph 环境中进行性能评估提供了实用指南。 一、rados bench以下基于 v19.2.1 版本进行测试。 用途:  测试 ceph rados 对象存储性能；  1.1、测试配置参数命令格式: rados bench $seconds $type [args...]  $seconds : 压测运行时间； $type : 压测类型，可选值为 write&#x2F;seq&#x2F;rand （分别代表写&#x2F;连续读&#x2F;随机读）； -p : 指定压测的目标 pool ； -b : 只有当压测类型为 write 时可用，用于设置写入 block...</div></div></div></a><a class="pagination-related" href="/2023/06/20/crush/" title="译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-20</div><div class="info-item-2">译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</div></div><div class="info-2"><div class="info-item-1">  译作: 可控的、可扩展的、分布式的副本数据放置算法，论文原文 。 该论文于 2006 年 11 月发布于 SC2006 。 CRUSH 是一种用于大规模分布式存储系统的数据分布算法，它通过伪随机函数将数据对象映射到存储设备上，无需依赖中央目录。CRUSH 算法设计考虑了系统的动态性，支持在添加或移除存储设备时高效地重组数据，并最小化不必要的数据移动。此外，CRUSH 支持多种数据复制和可靠性机制，并允许根据用户定义的策略进行数据分布，这些策略能够在故障域之间有效地分离副本，增强数据安全性。 CRUSH 的核心是其层级集群图，该图描述了存储集群的物理和逻辑结构，并通过一系列规则来确定数据的放置位置。CRUSH 算法通过将数据均匀分布在加权设备上，保持存储和设备带宽资源的平衡利用。算法还考虑了设备的故障和过载情况，能够在设备发生故障或过载时重新分配数据，避免数据丢失并优化系统性能。 CRUSH 的映射性能高效，计算复杂度为 O(logn) ，适用于管理大规模（多 PB...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">bugwz</div><div class="author-info-description">持续学习，持续进步</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">126</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">134</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/bugwz" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%9E%B6%E6%9E%84%E5%AF%B9%E6%AF%94"><span class="toc-text">一、架构对比</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E9%85%8D%E7%BD%AE%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B"><span class="toc-text">二、配置解析流程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E6%B5%81%E7%A8%8B"><span class="toc-text">三、网络通信流程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">四、线程模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1%E3%80%81shard-%E7%9B%B8%E5%85%B3%E4%BB%BB%E5%8A%A1"><span class="toc-text">4.1、shard 相关任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2%E3%80%81%E7%BA%BF%E7%A8%8B%E7%A4%BA%E4%BE%8B"><span class="toc-text">4.2、线程示例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9D%97%E8%AE%BE%E8%AE%A1"><span class="toc-text">五、存储模块设计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1%E3%80%81%E5%90%8E%E7%AB%AF%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%B1%BB%E5%9E%8B"><span class="toc-text">5.1、后端对象存储类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2%E3%80%81%E6%AE%B5%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E4%BF%A1%E6%81%AF"><span class="toc-text">5.2、段存储格式信息</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F"><span class="toc-text">六、客户端使用方式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E5%85%B6%E4%BB%96%E7%89%B9%E6%80%A7%E5%AE%9E%E7%8E%B0"><span class="toc-text">七、其他特性实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1%E3%80%81%E5%86%B7%E7%83%AD%E5%AD%98%E5%82%A8%E5%88%86%E7%A6%BB"><span class="toc-text">7.1、冷热存储分离</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%A8%A1%E5%9D%97%E8%A7%A3%E6%9E%90"><span class="toc-text">八、模块解析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B9%9D%E3%80%81%E4%BB%A3%E7%A0%81%E9%80%BB%E8%BE%91%E6%A2%B3%E7%90%86"><span class="toc-text">九、代码逻辑梳理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1%E3%80%81%E6%B6%88%E6%81%AF%E7%AE%A1%E7%90%86%E5%99%A8%E5%88%9B%E5%BB%BA%E9%80%BB%E8%BE%91"><span class="toc-text">9.1、消息管理器创建逻辑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2%E3%80%81store-%E5%AF%B9%E8%B1%A1%E5%88%9B%E5%BB%BA%E9%80%BB%E8%BE%91"><span class="toc-text">9.2、store 对象创建逻辑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3%E3%80%81crimson-osd-mkfs-%E9%80%BB%E8%BE%91"><span class="toc-text">9.3、crimson osd mkfs 逻辑</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-1%E3%80%81store-mkfs-osd-uuid"><span class="toc-text">9.3.1、store.mkfs(osd_uuid)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-4%E3%80%81crimson-osd-start-%E9%80%BB%E8%BE%91"><span class="toc-text">9.4、crimson osd start 逻辑</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E3%80%81%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99"><span class="toc-text">十、相关资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 设计实现深入解析"/></a><div class="content"><a class="title" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析">Ceph Crimson 设计实现深入解析</a><time datetime="2025-05-31T16:00:00.000Z" title="发表于 2025-06-01 00:00:00">2025-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 集群搭建指南"/></a><div class="content"><a class="title" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南">Ceph Crimson 集群搭建指南</a><time datetime="2025-01-11T16:00:00.000Z" title="发表于 2025-01-12 00:00:00">2025-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph QoS 机制深入分析"/></a><div class="content"><a class="title" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析">Ceph QoS 机制深入分析</a><time datetime="2024-10-24T16:00:00.000Z" title="发表于 2024-10-25 00:00:00">2024-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/01/gpfs/" title="GPFS 集群部署与运维记录"><img src="/assets/images/bg/gpfs.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GPFS 集群部署与运维记录"/></a><div class="content"><a class="title" href="/2024/08/01/gpfs/" title="GPFS 集群部署与运维记录">GPFS 集群部署与运维记录</a><time datetime="2024-07-31T16:00:00.000Z" title="发表于 2024-08-01 00:00:00">2024-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/30/ceph-crush/" title="Ceph CRUSH 设计实现剖析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph CRUSH 设计实现剖析"/></a><div class="content"><a class="title" href="/2023/06/30/ceph-crush/" title="Ceph CRUSH 设计实现剖析">Ceph CRUSH 设计实现剖析</a><time datetime="2023-06-29T16:00:00.000Z" title="发表于 2023-06-30 00:00:00">2023-06-30</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By bugwz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: '6af3be16b94cec39bcf6',
      clientSecret: '13a5202ff773ffcea6300b6c8ff25f455566737c',
      repo: 'bugwz.github.io',
      owner: 'bugwz',
      admin: ['bugwz'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || 'e6751b14746a145610546fe4c9bc0fda'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="docsearch-wrap"><div id="docsearch" style="display:none"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css/dist/style.min.css"/><script src="https://cdn.jsdelivr.net/npm/@docsearch/js/dist/umd/index.min.js"></script><script>(() => {
  docsearch(Object.assign({
    appId: 'PFB3WGSSCO',
    apiKey: '3e9cd446e41d93f2f130b91698b699f7',
    indexName: 'bugwz',
    container: '#docsearch',
    placeholder: '请输入要搜索的内容',
  }, {"maxResultsPerGroup":10}))

  const handleClick = () => {
    document.querySelector('.DocSearch-Button').click()
  }

  const searchClickFn = () => {
    btf.addEventListenerPjax(document.querySelector('#search-button > .search'), 'click', handleClick)
  }

  searchClickFn()
  window.addEventListener('pjax:complete', searchClickFn)
})()</script></div></div></body></html>