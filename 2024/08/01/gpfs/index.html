<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>GPFS 集群部署与运维记录 | 咕咕</title><meta name="author" content="bugwz"><meta name="copyright" content="bugwz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文重点介绍了 GPFS 集群的部署，以及 GPFS 集群关于客户端的管理等的运维命令等。">
<meta property="og:type" content="article">
<meta property="og:title" content="GPFS 集群部署与运维记录">
<meta property="og:url" content="https://bugwz.com/2024/08/01/gpfs/index.html">
<meta property="og:site_name" content="咕咕">
<meta property="og:description" content="本文重点介绍了 GPFS 集群的部署，以及 GPFS 集群关于客户端的管理等的运维命令等。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bugwz.com/assets/images/bg/gpfs.png">
<meta property="article:published_time" content="2024-07-31T16:00:00.000Z">
<meta property="article:modified_time" content="2025-07-14T16:23:27.037Z">
<meta property="article:author" content="bugwz">
<meta property="article:tag" content="GPFS">
<meta property="article:tag" content="分布式存储">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bugwz.com/assets/images/bg/gpfs.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "GPFS 集群部署与运维记录",
  "url": "https://bugwz.com/2024/08/01/gpfs/",
  "image": "https://bugwz.com/assets/images/bg/gpfs.png",
  "datePublished": "2024-07-31T16:00:00.000Z",
  "dateModified": "2025-07-14T16:23:27.037Z",
  "author": [
    {
      "@type": "Person",
      "name": "bugwz",
      "url": "https://bugwz.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/assets/images/bg/favicon.png"><link rel="canonical" href="https://bugwz.com/2024/08/01/gpfs/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'GPFS 集群部署与运维记录',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">129</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">134</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/assets/images/bg/gpfs.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">咕咕</span></a><a class="nav-page-title" href="/"><span class="site-name">GPFS 集群部署与运维记录</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">GPFS 集群部署与运维记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-31T16:00:00.000Z" title="发表于 2024-08-01 00:00:00">2024-08-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-14T16:23:27.037Z" title="更新于 2025-07-15 00:23:27">2025-07-15</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">8.6k</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="一、GPFS-介绍"><a href="#一、GPFS-介绍" class="headerlink" title="一、GPFS 介绍"></a>一、GPFS 介绍</h1><p>IBM GPFS (General Parallel File System ,GPFS)是一款并行的文件系统，它保证在资源组内的所有节点可以并行访问整个文件系统，而且针对此文件系统的服务操作，可以同时安全地在此文件系统的多个节点上实现。GPFS 允许客户共享文件，而这些文件可能分布在不同节点的不同硬盘上，保证了数据的一致性和完整性。GPFS支持多种平台的部署，如Windows、Linux、AIX，每种环境部署方式相同，降低了软件部署的复杂度。</p>
<h1 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a>二、环境准备</h1><p><strong>环境拓扑介绍:</strong></p>
<table>
<thead>
<tr>
<th align="center">节点名称</th>
<th align="center">节点IP</th>
<th align="center">节点角色</th>
</tr>
</thead>
<tbody><tr>
<td align="center">node01</td>
<td align="center">10.10.0.1</td>
<td align="center">Server，GUI(Dashboard)</td>
</tr>
<tr>
<td align="center">node02</td>
<td align="center">10.10.0.2</td>
<td align="center">Server，GUI(Dashboard)，CES</td>
</tr>
<tr>
<td align="center">node03</td>
<td align="center">10.10.0.3</td>
<td align="center">Server，CES</td>
</tr>
</tbody></table>
<p><strong>相关操作步骤如下:</strong></p>
<ul>
<li><code>配置 /etc/hosts</code> : 用于节点间的 hostname 相互识别；</li>
<li><code>配置 ssh 免密登录</code> : 用于节点间的相互通信；</li>
<li><code>关闭防火墙和 selinux</code> : 避免网络问题导致节点间通信异常；</li>
<li><code>配置时间同步</code> : 避免时钟不同步，导致节点间通信等其他异常；</li>
<li><code>安装依赖包</code> : 运行 gpfs 的基础依赖软件；</li>
</ul>
<h2 id="2-1、配置-x2F-etc-x2F-hosts"><a href="#2-1、配置-x2F-etc-x2F-hosts" class="headerlink" title="2.1、配置 &#x2F;etc&#x2F;hosts"></a>2.1、配置 &#x2F;etc&#x2F;hosts</h2><p>确保每个 server&#x2F;client 节点上均配置如下的 hosts 记录。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> /etc/hosts</span><br><span class="line">10.10.0.1 node01</span><br><span class="line">10.10.0.2 node02</span><br><span class="line">10.10.0.3 node03</span><br></pre></td></tr></table></figure>

<h2 id="2-2、配置-ssh-免密登录"><a href="#2-2、配置-ssh-免密登录" class="headerlink" title="2.2、配置 ssh 免密登录"></a>2.2、配置 ssh 免密登录</h2><p>确保每个 server 间可以实现 ssh 免密访问，同时任意 server 节点均可免密访问 client 节点。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># node01</span></span><br><span class="line">ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node02</span><br><span class="line">ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node03</span><br><span class="line"></span><br><span class="line"><span class="comment"># node02</span></span><br><span class="line">ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node01</span><br><span class="line">ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node03</span><br><span class="line"></span><br><span class="line"><span class="comment"># node03</span></span><br><span class="line">ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node01</span><br><span class="line">ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node02</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试连接，分别位于不同的机器执行测试</span></span><br><span class="line">ssh root@node01</span><br><span class="line">ssh root@node02</span><br><span class="line">ssh root@node03</span><br></pre></td></tr></table></figure>


<h2 id="2-3、关闭防火墙和-selinux"><a href="#2-3、关闭防火墙和-selinux" class="headerlink" title="2.3、关闭防火墙和 selinux"></a>2.3、关闭防火墙和 selinux</h2><p><strong>相关命令:</strong> （以下操作需要在每个 server&#x2F;client 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 关闭防火墙并禁止开机启动</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭 selinux</span></span><br><span class="line"><span class="comment"># 将 SELINUX=enforcing 改为 SELINUX=disabled</span></span><br><span class="line"><span class="comment"># 重启后生效</span></span><br><span class="line">vi /etc/selinux/config</span><br><span class="line"><span class="built_in">cat</span> /etc/selinux/config</span><br></pre></td></tr></table></figure>

<h2 id="2-4、配置时间同步"><a href="#2-4、配置时间同步" class="headerlink" title="2.4、配置时间同步"></a>2.4、配置时间同步</h2><p><strong>相关命令:</strong> （以下操作需要在每个 server&#x2F;client 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动时间同步并设置开机启动</span></span><br><span class="line">systemctl start chronyd</span><br><span class="line">systemctl <span class="built_in">enable</span> chronyd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查同步状态</span></span><br><span class="line">chronyc tracking</span><br></pre></td></tr></table></figure>


<h2 id="2-5、安装依赖包"><a href="#2-5、安装依赖包" class="headerlink" title="2.5、安装依赖包"></a>2.5、安装依赖包</h2><p><strong>相关命令:</strong> （以下操作需要在每个 server&#x2F;client 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dnf install -y ksh m4 kernel-devel kernel-devel-$(<span class="built_in">uname</span> -r) kernel-headers gcc-c++ python3 net-tools perl-Thread-Queue</span><br></pre></td></tr></table></figure>


<h1 id="三、集群部署"><a href="#三、集群部署" class="headerlink" title="三、集群部署"></a>三、集群部署</h1><p>本次集群部署的软件包版本为 <code>5.1.8.1</code> ，完整的软件包名为 <code>Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install</code> 。</p>
<p><strong>集群部署的关键步骤如下:</strong></p>
<ul>
<li>解压并安装软件包 : </li>
<li>修改环境变量 : </li>
<li>构建GPFS可移植层 :</li>
<li>创建集群 : </li>
<li>创建NSD :</li>
<li>创建文件系统 :</li>
<li>挂载文件系统 : </li>
<li>配置Dashboard（可选） : </li>
<li>配置CES（可选） :</li>
</ul>
<h2 id="3-1、解压并安装软件包"><a href="#3-1、解压并安装软件包" class="headerlink" title="3.1、解压并安装软件包"></a>3.1、解压并安装软件包</h2><p>如果你是为了开发测试使用，你可以访问 <a target="_blank" rel="noopener" href="https://www.ibm.com/products/storage-scale">IBM Storage Scale</a> 官方，注册一个账户，之后签署协议，下载对应的开发者版本的软件包。在下面的示例页面中，就可以下载对应的软件包。在这次的测试中，我们并没有使用对应的开发者版本的软件包，而是使用本地生产环境使用的软件包。</p>
<p><img src="/./assets/images/gpfs-download-dev.png" alt="Storage_Scale_Developer-5.2.3.2-x86_64-Linux.zip" loading="lazy"></p>
<h3 id="3-1-1、解压并安装软件包"><a href="#3-1-1、解压并安装软件包" class="headerlink" title="3.1.1、解压并安装软件包"></a>3.1.1、解压并安装软件包</h3><p><strong>相关命令:</strong> （以下操作需要在每个 server 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解压软件包</span></span><br><span class="line"><span class="built_in">chmod</span> +x Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install</span><br><span class="line">./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压软件包，默认安装文件会被解压到 /usr/lpp/mmfs/5.1.8.1 目录下</span></span><br><span class="line"><span class="comment"># 不同操作的含义:</span></span><br><span class="line"><span class="comment">#     输入 1 : 接受协议（输入 1 后继续安装操作）</span></span><br><span class="line"><span class="comment">#     输入 2 : 拒绝协议</span></span><br><span class="line"><span class="comment">#     输入 3 : 打印协议</span></span><br><span class="line"><span class="comment">#     输入 4 : 阅读非 IBM 条款</span></span><br><span class="line"><span class="comment">#     输入 99 : 返回上一屏幕</span></span><br><span class="line">./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install --text-only</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装软件包</span></span><br><span class="line"><span class="built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/gpfs_rpms/</span><br><span class="line">rpm -ivh gpfs.base-5.1.8-1.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.compression-5.1.8-1.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.docs-5.1.8-1.noarch.rpm</span><br><span class="line">rpm -ivh gpfs.gpl-5.1.8-1.noarch.rpm</span><br><span class="line">rpm -ivh gpfs.gskit-8.0.55-19.1.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.java-5.1.8-1.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.license.da-5.1.8-1.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.msg.en_US-5.1.8-1.noarch.rpm</span><br><span class="line"><span class="built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/zimon_rpms/rhel8/</span><br><span class="line">rpm -ivh gpfs.gss.pmcollector-5.1.8-1.el8.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.gss.pmsensors-5.1.8-1.el8.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.pm-ganesha-10.0.0-2.el8.x86_64.rpm</span><br></pre></td></tr></table></figure>


<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 data]<span class="comment"># ./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install --text-only</span></span><br><span class="line"></span><br><span class="line">Extracting License Acceptance Process Tool to /usr/lpp/mmfs/5.1.8.1 ...</span><br><span class="line"><span class="built_in">tail</span> -n +660 ./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install | tar -C /usr/lpp/mmfs/5.1.8.1 -xvz --exclude=installer --exclude=*_rpms --exclude=*_debs --exclude=*rpm  --exclude=*tgz --exclude=*deb --exclude=*tools* 1&gt; /dev/null</span><br><span class="line"></span><br><span class="line">Installing JRE ...</span><br><span class="line"></span><br><span class="line">If directory /usr/lpp/mmfs/5.1.8.1 has been created or was previously created during another extraction,</span><br><span class="line">.rpm, .deb, and repository related files <span class="keyword">in</span> it (<span class="keyword">if</span> there were) will be removed to avoid conflicts with the ones being extracted.</span><br><span class="line"></span><br><span class="line"><span class="built_in">tail</span> -n +660 ./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install | tar -C /usr/lpp/mmfs/5.1.8.1 --wildcards -xvz  ibm-java*tgz 1&gt; /dev/null</span><br><span class="line">tar -C /usr/lpp/mmfs/5.1.8.1/ -xzf /usr/lpp/mmfs/5.1.8.1/ibm-java*tgz</span><br><span class="line"></span><br><span class="line">Invoking License Acceptance Process Tool ...</span><br><span class="line">/usr/lpp/mmfs/5.1.8.1/ibm-java-x86_64-80/jre/bin/java -<span class="built_in">cp</span> /usr/lpp/mmfs/5.1.8.1/LAP_HOME/LAPApp.jar com.ibm.lex.lapapp.LAP -l /usr/lpp/mmfs/5.1.8.1/LA_HOME -m /usr/lpp/mmfs/5.1.8.1 -s /usr/lpp/mmfs/5.1.8.1  -text_only</span><br><span class="line"></span><br><span class="line">LICENSE INFORMATION</span><br><span class="line"></span><br><span class="line">The Programs listed below are licensed under the following</span><br><span class="line">License Information terms and conditions <span class="keyword">in</span> addition to the</span><br><span class="line">Program license terms previously agreed to by Client and</span><br><span class="line">IBM. If Client does not have previously agreed to license</span><br><span class="line">terms <span class="keyword">in</span> effect <span class="keyword">for</span> the Program, the International Program</span><br><span class="line">License Agreement (i125-3301-15) applies.</span><br><span class="line"></span><br><span class="line">Program Name (Program Number):</span><br><span class="line">IBM Storage Scale Erasure Code Edition 5.1.8.1 (5737-J34)</span><br><span class="line">IBM Storage Scale Data Management Edition 5.1.8.1 (5737-F34)</span><br><span class="line">IBM Storage Scale Data Management Edition 5.1.8.1 (5641-DM1)</span><br><span class="line">IBM Storage Scale Data Management Edition 5.1.8.1 (5641-DM3)</span><br><span class="line"></span><br><span class="line">Press Enter to <span class="built_in">continue</span> viewing the license agreement, or</span><br><span class="line">enter <span class="string">&quot;1&quot;</span> to accept the agreement, <span class="string">&quot;2&quot;</span> to decline it, <span class="string">&quot;3&quot;</span></span><br><span class="line">to <span class="built_in">print</span> it, <span class="string">&quot;4&quot;</span> to <span class="built_in">read</span> non-IBM terms, or <span class="string">&quot;99&quot;</span> to go back</span><br><span class="line">to the previous screen.</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">License Agreement Terms accepted.</span><br><span class="line"></span><br><span class="line">Extracting Product RPMs to /usr/lpp/mmfs/5.1.8.1 ...</span><br><span class="line"><span class="built_in">tail</span> -n +660 ./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install | tar -C /usr/lpp/mmfs/5.1.8.1 --wildcards -xvz  Public_Keys ansible-toolkit cloudkit/dependencies ganesha_debs/ubuntu/ubuntu20 ganesha_debs/ubuntu/ubuntu22 gpfs_debs/ubuntu/ubuntu20 gpfs_debs/ubuntu/ubuntu22 hdfs_rpms/rhel/hdfs_3.1.1.x hdfs_rpms/rhel/hdfs_3.2.2.x hdfs_rpms/rhel/hdfs_3.3.x smb_debs/ubuntu/ubuntu20 smb_debs/ubuntu/ubuntu22 zimon_debs/ubuntu/ubuntu20 zimon_debs/ubuntu/ubuntu22 ganesha_rpms/rhel7 ganesha_rpms/rhel8 ganesha_rpms/rhel9 ganesha_rpms/sles15 gpfs_rpms/rhel7 gpfs_rpms/rhel8 gpfs_rpms/rhel9 gpfs_rpms/sles15 object_rpms/rhel8 smb_rpms/rhel7 smb_rpms/rhel8 smb_rpms/rhel9 smb_rpms/sles15 tools/repo zimon_debs/ubuntu zimon_rpms/rhel7 zimon_rpms/rhel8 zimon_rpms/rhel9 zimon_rpms/sles15 cloudkit gpfs_debs gpfs_rpms manifest 1&gt; /dev/null</span><br><span class="line">   - Public_Keys</span><br><span class="line">   - ansible-toolkit</span><br><span class="line">   - cloudkit/dependencies</span><br><span class="line">   - ganesha_debs/ubuntu/ubuntu20</span><br><span class="line">   - ganesha_debs/ubuntu/ubuntu22</span><br><span class="line">   - gpfs_debs/ubuntu/ubuntu20</span><br><span class="line">   - gpfs_debs/ubuntu/ubuntu22</span><br><span class="line">   - hdfs_rpms/rhel/hdfs_3.1.1.x</span><br><span class="line">   - hdfs_rpms/rhel/hdfs_3.2.2.x</span><br><span class="line">   - hdfs_rpms/rhel/hdfs_3.3.x</span><br><span class="line">   - smb_debs/ubuntu/ubuntu20</span><br><span class="line">   - smb_debs/ubuntu/ubuntu22</span><br><span class="line">   - zimon_debs/ubuntu/ubuntu20</span><br><span class="line">   - zimon_debs/ubuntu/ubuntu22</span><br><span class="line">   - ganesha_rpms/rhel7</span><br><span class="line">   - ganesha_rpms/rhel8</span><br><span class="line">   - ganesha_rpms/rhel9</span><br><span class="line">   - ganesha_rpms/sles15</span><br><span class="line">   - gpfs_rpms/rhel7</span><br><span class="line">   - gpfs_rpms/rhel8</span><br><span class="line">   - gpfs_rpms/rhel9</span><br><span class="line">   - gpfs_rpms/sles15</span><br><span class="line">   - object_rpms/rhel8</span><br><span class="line">   - smb_rpms/rhel7</span><br><span class="line">   - smb_rpms/rhel8</span><br><span class="line">   - smb_rpms/rhel9</span><br><span class="line">   - smb_rpms/sles15</span><br><span class="line">   - tools/repo</span><br><span class="line">   - zimon_debs/ubuntu</span><br><span class="line">   - zimon_rpms/rhel7</span><br><span class="line">   - zimon_rpms/rhel8</span><br><span class="line">   - zimon_rpms/rhel9</span><br><span class="line">   - zimon_rpms/sles15</span><br><span class="line">   - cloudkit</span><br><span class="line">   - gpfs_debs</span><br><span class="line">   - gpfs_rpms</span><br><span class="line">   - manifest</span><br><span class="line"></span><br><span class="line">Removing License Acceptance Process Tool from /usr/lpp/mmfs/5.1.8.1 ...</span><br><span class="line"><span class="built_in">rm</span> -rf  /usr/lpp/mmfs/5.1.8.1/LAP_HOME /usr/lpp/mmfs/5.1.8.1/LA_HOME</span><br><span class="line"></span><br><span class="line">Removing JRE from /usr/lpp/mmfs/5.1.8.1 ...</span><br><span class="line"><span class="built_in">rm</span> -rf /usr/lpp/mmfs/5.1.8.1/ibm-java*tgz</span><br><span class="line"></span><br><span class="line">==================================================================</span><br><span class="line">Product packages successfully extracted to /usr/lpp/mmfs/5.1.8.1</span><br><span class="line"></span><br><span class="line">   Cluster installation and protocol deployment</span><br><span class="line">      To install a cluster or deploy protocols with the IBM Storage Scale Installation Toolkit:</span><br><span class="line">      /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale -h</span><br><span class="line"></span><br><span class="line">      To install a cluster manually:  Use the GPFS packages located within /usr/lpp/mmfs/5.1.8.1/gpfs_&lt;rpms/debs&gt;</span><br><span class="line"></span><br><span class="line">      To upgrade an existing cluster using the IBM Storage Scale Installation Toolkit:</span><br><span class="line">      1) Review and update the config:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale config update</span><br><span class="line">      2) Update the cluster configuration to reflect the current cluster config:</span><br><span class="line">         /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale config populate -N &lt;node&gt;</span><br><span class="line">      3) Use online or offline upgrade depending on your requirements:</span><br><span class="line">         - Run the online rolling upgrade:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale upgrade -h</span><br><span class="line">         - Run the offline upgrade: /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale upgrade config offline -N;</span><br><span class="line">               /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale upgrade run</span><br><span class="line">      You can also run the parallel offline upgrade to upgrade all nodes parallely after shutting down GPFS</span><br><span class="line">      and stopping protocol services on all nodes.</span><br><span class="line">      You can run the parallel offline upgrade on all nodes <span class="keyword">in</span> the cluster, not on a subset of nodes.</span><br><span class="line"></span><br><span class="line">      To add nodes to an existing cluster using the IBM Storage Scale Installation Toolkit:</span><br><span class="line">      1) Add nodes to the cluster definition file:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale node add -h</span><br><span class="line">      2) Install IBM Storage Scale on the new nodes:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale install -h</span><br><span class="line">      3) Deploy protocols on the new nodes:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale deploy -h</span><br><span class="line"></span><br><span class="line">      To add NSDs or file systems to an existing cluster using the IBM Storage Scale Installation Toolkit:</span><br><span class="line">      1) Add NSDs or file systems to the cluster definition:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale nsd add -h</span><br><span class="line">      2) Install the NSDs or file systems:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale install -h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      To update the cluster definition to reflect the current cluster config examples:</span><br><span class="line">         /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale config populate -N &lt;node&gt;</span><br><span class="line">      1) Manual updates outside of the installation toolkit</span><br><span class="line">      2) Sync the current cluster state to the installation toolkit prior to upgrade</span><br><span class="line">      3) Switching from a manually managed cluster to the installation toolkit</span><br><span class="line"></span><br><span class="line">===================================================================================</span><br><span class="line">To get up and running quickly, consult the IBM Storage Scale Protocols Quick Overview:</span><br><span class="line">https://www.ibm.com/docs/en/STXKQY_5.1.8/pdf/scale_povr.pdf</span><br><span class="line">===================================================================================</span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># ll /usr/lpp/mmfs/5.1.8.1</span></span><br><span class="line">total 40</span><br><span class="line">drwxr-xr-x 11 root root   226 Jul 19  2023 ansible-toolkit</span><br><span class="line">drwxr-xr-x  3 root root    42 Jul 19  2023 cloudkit</span><br><span class="line">drwxr-xr-x  3 root root    20 Jun 19 21:10 ganesha_debs</span><br><span class="line">drwxr-xr-x  6 root root    59 Jun 19 21:10 ganesha_rpms</span><br><span class="line">drwxr-xr-x  3 root root  4096 Jul 19  2023 gpfs_debs</span><br><span class="line">drwxr-xr-x  7 root root  4096 Jul 19  2023 gpfs_rpms</span><br><span class="line">drwxr-xr-x  3 root root    18 Jun 19 21:10 hdfs_rpms</span><br><span class="line">drwxr-xr-x  3 root root  4096 Jun 19 21:10 license</span><br><span class="line">-rw-r--r--  1 root root 25195 Jul 19  2023 manifest</span><br><span class="line">drwxr-xr-x  3 root root    19 Jun 19 21:10 object_rpms</span><br><span class="line">drwxr-xr-x  2 root root    76 Jul 19  2023 Public_Keys</span><br><span class="line">drwxr-xr-x  3 root root    20 Jun 19 21:10 smb_debs</span><br><span class="line">drwxr-xr-x  6 root root    59 Jun 19 21:10 smb_rpms</span><br><span class="line">drwxr-xr-x  3 root root    18 Jun 19 21:10 tools</span><br><span class="line">drwxr-xr-x  3 root root    20 Jun 19 21:10 zimon_debs</span><br><span class="line">drwxr-xr-x  6 root root    59 Jun 19 21:10 zimon_rpms</span><br></pre></td></tr></table></figure>


<h3 id="3-1-2、仅解压软件包"><a href="#3-1-2、仅解压软件包" class="headerlink" title="3.1.2、仅解压软件包"></a>3.1.2、仅解压软件包</h3><p>如果我们只想要简单的解压对应的文件，并查看其中的文件列表，而不执行安装操作，可以使用 <a target="_blank" rel="noopener" href="https://sparanoid.com/lab/7z/download.html">7zip</a> 软件来执行解压操作。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载并安装 7z 软件</span></span><br><span class="line"><span class="built_in">mkdir</span> ./7z</span><br><span class="line">wget https://www.7-zip.org/a/7z2409-linux-x64.tar.xz</span><br><span class="line">tar -xvf 7z2409-linux-x64.tar.xz -C ./7z</span><br><span class="line"><span class="built_in">cp</span> ./7z/7zz* /usr/bin/</span><br><span class="line"><span class="built_in">rm</span> -rf ./7z 7z2409-linux-x64.tar.xz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压 GPFS 压缩包到当前目录的 gpfs 目录中， 如果 gpfs 目录不存在则自动创建</span></span><br><span class="line">7zz x Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install -o./gpfs</span><br><span class="line">7zz x ./gpfs/Storage_Scale_Data_Access-5.1.8 -o./gpfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看目录结构</span></span><br><span class="line">tree ./gpfs/ -L 1</span><br></pre></td></tr></table></figure>


<p><strong>相关输出信息:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 data]<span class="comment"># 7zz x Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install -o./gpfs</span></span><br><span class="line"></span><br><span class="line">7-Zip (z) 24.09 (x64) : Copyright (c) 1999-2024 Igor Pavlov : 2024-11-28</span><br><span class="line"> 64-bit locale=en_US.UTF-8 Threads:8 OPEN_MAX:1024, ASM</span><br><span class="line"></span><br><span class="line">Scanning the drive <span class="keyword">for</span> archives:</span><br><span class="line">1 file, 1234441464 bytes (1178 MiB)</span><br><span class="line"></span><br><span class="line">Extracting archive: Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install</span><br><span class="line">--</span><br><span class="line">Path = Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install</span><br><span class="line">Type = gzip</span><br><span class="line">Offset = 24036</span><br><span class="line">Physical Size = 1234417428</span><br><span class="line">Headers Size = 10</span><br><span class="line">Streams = 1</span><br><span class="line"></span><br><span class="line">Everything is Ok</span><br><span class="line"></span><br><span class="line">Size:       1508546560</span><br><span class="line">Compressed: 1234441464</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># 7zz x ./gpfs/Storage_Scale_Data_Access-5.1.8 -o./gpfs</span></span><br><span class="line"></span><br><span class="line">7-Zip (z) 24.09 (x64) : Copyright (c) 1999-2024 Igor Pavlov : 2024-11-28</span><br><span class="line"> 64-bit locale=en_US.UTF-8 Threads:8 OPEN_MAX:1024, ASM</span><br><span class="line"></span><br><span class="line">Scanning the drive <span class="keyword">for</span> archives:</span><br><span class="line">1 file, 1508546560 bytes (1439 MiB)</span><br><span class="line"></span><br><span class="line">Extracting archive: Storage_Scale_Data_Access-5.1.8</span><br><span class="line">--</span><br><span class="line">Path = Storage_Scale_Data_Access-5.1.8</span><br><span class="line">Type = tar</span><br><span class="line">Physical Size = 1508546560</span><br><span class="line">Headers Size = 2147328</span><br><span class="line">Code Page = UTF-8</span><br><span class="line">Characteristics = GNU LongName ASCII</span><br><span class="line"></span><br><span class="line">Everything is Ok</span><br><span class="line"></span><br><span class="line">Folders: 702</span><br><span class="line">Files: 2522</span><br><span class="line">Size:       1505726177</span><br><span class="line">Compressed: 1508546560</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># tree ./gpfs/ -L 1</span></span><br><span class="line">./gpfs/</span><br><span class="line">├── ansible-toolkit</span><br><span class="line">├── cloudkit</span><br><span class="line">├── ganesha_debs</span><br><span class="line">├── ganesha_rpms</span><br><span class="line">├── gpfs_debs</span><br><span class="line">├── gpfs_rpms</span><br><span class="line">├── hdfs_rpms</span><br><span class="line">├── ibm-java-jre-8.0-5.11-linux-x86_64.tgz</span><br><span class="line">├── LA_HOME</span><br><span class="line">├── LAP_HOME</span><br><span class="line">├── manifest</span><br><span class="line">├── object_rpms</span><br><span class="line">├── Public_Keys</span><br><span class="line">├── smb_debs</span><br><span class="line">├── smb_rpms</span><br><span class="line">├── Storage_Scale_Data_Access-5.1.8</span><br><span class="line">├── tools</span><br><span class="line">├── zimon_debs</span><br><span class="line">└── zimon_rpms</span><br><span class="line"></span><br><span class="line">16 directories, 3 files</span><br></pre></td></tr></table></figure>



<h2 id="3-2、修改环境变量"><a href="#3-2、修改环境变量" class="headerlink" title="3.2、修改环境变量"></a>3.2、修改环境变量</h2><p>为了后续方便使用 gpfs 的相关命令，我们可以把 gpfs bin 目录添加到 PATH 路径中。</p>
<p><strong>相关命令:</strong> （以下操作需要在每个 server 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置 gpfs bin PATH 路径</span></span><br><span class="line"><span class="built_in">cat</span> /root/.bash_profile</span><br><span class="line">vi /root/.bash_profile</span><br><span class="line"><span class="comment"># .bash_profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the aliases and functions</span></span><br><span class="line"><span class="keyword">if</span> [ -f ~/.bashrc ]; <span class="keyword">then</span></span><br><span class="line">        . ~/.bashrc</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># User specific environment and startup programs</span></span><br><span class="line">PATH=<span class="variable">$PATH</span>:<span class="variable">$HOME</span>/bin:/usr/lpp/mmfs/bin</span><br><span class="line"><span class="built_in">export</span> PATH</span><br></pre></td></tr></table></figure>


<h2 id="3-3、构建GPFS可移植层"><a href="#3-3、构建GPFS可移植层" class="headerlink" title="3.3、构建GPFS可移植层"></a>3.3、构建GPFS可移植层</h2><p>GPFS 可移植性层特定于当前内核和 GPFS 版本。如果内核或 GPFS 版本发生变化，则需要构建新的 GPFS 可移植层。尽管操作系统内核可能会升级到新版本，但它们在重新启动后才处于活动状态。因此，必须在重新启动操作系统后为这个新内核构建一个 GPFS 可移植层。<br>并且注意在安装新的 GPFS 可移植层之前，请确保先卸载先前版本的 GPFS 可移植层。</p>
<p>构建完成后，终端会输出生成的包的位置，然后，我们可以将生成的包复制到其他机器进行部署。默认情况下，生成的包只能部署到架构、分发级别、Linux 内核和 IBM Spectrum Scale 维护级别与构建 gpfs.gplbin 包的机器相同的机器上。不过仍然建议在每个 server 节点上执行构建操作生成本机的 GPFS 可移植层。</p>
<p><strong>相关命令:</strong> （以下操作需要在每个 server 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建 gpfs 可移植层，</span></span><br><span class="line">mmbuildgpl --build-package</span><br></pre></td></tr></table></figure>

<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 data]<span class="comment"># mmbuildgpl --build-package</span></span><br><span class="line">--------------------------------------------------------</span><br><span class="line">mmbuildgpl: Building GPL (5.1.8.1) module begins at Thu Jun 19 21:40:45 CST 2024.</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">Verifying Kernel Header...</span><br><span class="line">  kernel version = 41800348 (418000348000000, 4.18.0-348.el8.x86_64, 4.18.0-348)</span><br><span class="line">  module include <span class="built_in">dir</span> = /lib/modules/4.18.0-348.el8.x86_64/build/include</span><br><span class="line">  module build <span class="built_in">dir</span>   = /lib/modules/4.18.0-348.el8.x86_64/build</span><br><span class="line">  kernel <span class="built_in">source</span> <span class="built_in">dir</span>  = /usr/src/linux-4.18.0-348.el8.x86_64/include</span><br><span class="line">  Found valid kernel header file under /usr/src/kernels/4.18.0-348.el8.x86_64/include</span><br><span class="line">Getting Kernel Cipher mode...</span><br><span class="line">   Will use skcipher routines</span><br><span class="line">Verifying Compiler...</span><br><span class="line">  make is present at /bin/make</span><br><span class="line">  cpp is present at /bin/cpp</span><br><span class="line">  gcc is present at /bin/gcc</span><br><span class="line">  g++ is present at /bin/g++</span><br><span class="line">  ld is present at /bin/ld</span><br><span class="line">Verifying rpmbuild...</span><br><span class="line">Verifying libelf devel package...</span><br><span class="line">  Verifying  elfutils-libelf-devel is installed ...</span><br><span class="line">    Command: /bin/rpm -q  elfutils-libelf-devel</span><br><span class="line">    The required package  elfutils-libelf-devel is installed</span><br><span class="line">Verifying Additional System Headers...</span><br><span class="line">  Verifying kernel-headers is installed ...</span><br><span class="line">    Command: /bin/rpm -q kernel-headers</span><br><span class="line">    The required package kernel-headers is installed</span><br><span class="line">make World ...</span><br><span class="line">make InstallImages ...</span><br><span class="line">make rpm ...</span><br><span class="line">Wrote: /root/rpmbuild/RPMS/x86_64/gpfs.gplbin-4.18.0-348.el8.x86_64-5.1.8-1.x86_64.rpm</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">mmbuildgpl: Building GPL module completed successfully at Thu Jun 19 21:41:13 CST 2024.</span><br><span class="line">--------------------------------------------------------</span><br></pre></td></tr></table></figure>


<h2 id="3-4、创建集群"><a href="#3-4、创建集群" class="headerlink" title="3.4、创建集群"></a>3.4、创建集群</h2><p>GPFS 的仲裁机制和 ZooKeeper 的仲裁机制类似，当有一半以上的节点是 quorum 时，集群才可以启动，即： <code>quorum &gt;= 1 + sizeof(all nodes) / 2</code> 。</p>
<p><strong>相关命令:</strong> （以下操作仅在 node01 节点上执行即可）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 nodefile 文件</span></span><br><span class="line"><span class="built_in">cat</span> /etc/mmfs/nodefile</span><br><span class="line">node01:quorum-manager:</span><br><span class="line">node02:quorum-manager:</span><br><span class="line">node03:quorum-manager:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建集群</span></span><br><span class="line">mmcrcluster -N /etc/mmfs/nodefile -C gpfscluster -r /usr/bin/ssh -R /usr/bin/scp -A</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接受节点许可证</span></span><br><span class="line">mmchlicense server --accept -N all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动集群节点</span></span><br><span class="line">mmstartup -N node01</span><br><span class="line">mmstartup -N node02</span><br><span class="line">mmstartup -N node03</span><br></pre></td></tr></table></figure>

<p><strong>mmcrcluster 参数说明:</strong></p>
<ul>
<li><code>-N</code> : 表示节点的配置文件。</li>
<li><code>-C</code> : 指定集群的名称。</li>
<li><code>-r</code> : 指定 GPFS 使用的远程 shell 程序的完整路径名。默认值为 &#x2F;usr&#x2F;bin&#x2F;ssh。</li>
<li><code>-R</code> : 指定 GPFS 使用的远程文件复制程序的完整路径名。默认值为 &#x2F;usr&#x2F;bin&#x2F;scp。</li>
<li><code>-A</code> : 指定当节点启动时 GPFS 守护进程自动启动。默认情况下不自动启动守护进程。</li>
</ul>
<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 data]<span class="comment"># mmcrcluster -N /etc/mmfs/nodefile -C gpfscluster -r /usr/bin/ssh -R /usr/bin/scp -A</span></span><br><span class="line">mmcrcluster: Performing preliminary node verification ...</span><br><span class="line">mmcrcluster: Processing quorum and other critical nodes ...</span><br><span class="line">mmcrcluster: Finalizing the cluster data structures ...</span><br><span class="line">mmcrcluster: Command successfully completed</span><br><span class="line">mmcrcluster: Warning: Not all nodes have proper GPFS license designations.</span><br><span class="line">    Use the mmchlicense <span class="built_in">command</span> to designate licenses as needed.</span><br><span class="line">mmcrcluster: [I] The cluster was created with the tscCmdAllowRemoteConnections configuration parameter <span class="built_in">set</span> to <span class="string">&quot;no&quot;</span>. If a remote cluster is established with another cluster whose release level (minReleaseLevel) is less than 5.1.3.0, change the value of tscCmdAllowRemoteConnections <span class="keyword">in</span> this cluster to <span class="string">&quot;yes&quot;</span>.</span><br><span class="line">mmcrcluster: Propagating the cluster configuration data to all</span><br><span class="line">  affected nodes.  This is an asynchronous process.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># mmchlicense server --accept -N all</span></span><br><span class="line"></span><br><span class="line">The following nodes will be designated as possessing server licenses:</span><br><span class="line">        node01</span><br><span class="line">        node02</span><br><span class="line">        node03</span><br><span class="line">mmchlicense: Command successfully completed</span><br><span class="line">mmchlicense: Propagating the cluster configuration data to all</span><br><span class="line">  affected nodes.  This is an asynchronous process.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># mmstartup -N node01</span></span><br><span class="line">Fri Jun 20 10:29:53 CST 2024: mmstartup: Starting GPFS ...</span><br><span class="line">[root@node02 data]<span class="comment"># mmstartup -N node02</span></span><br><span class="line">Fri Jun 20 10:29:55 CST 2024: mmstartup: Starting GPFS ...</span><br><span class="line">[root@node03 data]<span class="comment"># mmstartup -N node03</span></span><br><span class="line">Fri Jun 20 10:29:58 CST 2024: mmstartup: Starting GPFS ...</span><br></pre></td></tr></table></figure>


<h2 id="3-5、创建NSD"><a href="#3-5、创建NSD" class="headerlink" title="3.5、创建NSD"></a>3.5、创建NSD</h2><p><strong>&#x2F;etc&#x2F;mmfs&#x2F;nsdfile 配置文件内容:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">%nsd: device=/dev/sdc</span><br><span class="line">  nsd=data01</span><br><span class="line">  servers=node01</span><br><span class="line">  usage=dataAndMetadata</span><br><span class="line">  failureGroup=-1</span><br><span class="line">  pool=system</span><br><span class="line">  thinDiskType=auto</span><br><span class="line">%nsd: device=/dev/sdd</span><br><span class="line">  nsd=data02</span><br><span class="line">  servers=node01</span><br><span class="line">  usage=dataAndMetadata</span><br><span class="line">  failureGroup=-1</span><br><span class="line">  pool=system</span><br><span class="line">  thinDiskType=auto</span><br><span class="line">%nsd: device=/dev/sdc</span><br><span class="line">  nsd=data03</span><br><span class="line">  servers=node02</span><br><span class="line">  usage=dataAndMetadata</span><br><span class="line">  failureGroup=-1</span><br><span class="line">  pool=system</span><br><span class="line">  thinDiskType=auto</span><br><span class="line">%nsd: device=/dev/sdd</span><br><span class="line">  nsd=data04</span><br><span class="line">  servers=node02</span><br><span class="line">  usage=dataAndMetadata</span><br><span class="line">  failureGroup=-1</span><br><span class="line">  pool=system</span><br><span class="line">  thinDiskType=auto</span><br><span class="line">%nsd: device=/dev/sdc</span><br><span class="line">  nsd=data05</span><br><span class="line">  servers=node03</span><br><span class="line">  usage=dataAndMetadata</span><br><span class="line">  failureGroup=-1</span><br><span class="line">  pool=system</span><br><span class="line">  thinDiskType=auto</span><br><span class="line">%nsd: device=/dev/sdd</span><br><span class="line">  nsd=data06</span><br><span class="line">  servers=node03</span><br><span class="line">  usage=dataAndMetadata</span><br><span class="line">  failureGroup=-1</span><br><span class="line">  pool=system</span><br><span class="line">  thinDiskType=auto</span><br></pre></td></tr></table></figure>


<p><strong>配置文件参数解析:</strong></p>
<ul>
<li><code>device</code> : 块设备名称，用于定义为 NSD 的磁盘。</li>
<li><code>nsd</code> : 指定要创建的 NSD 的名称。不能以保留字符串 ‘gpfs’ 开头。</li>
<li><code>servers</code> : 指定一个以逗号分隔的 NSD 服务器节点列表。</li>
<li><code>usage</code> : 指定要存储在磁盘上的数据类型。<ul>
<li>dataAndMetadata : 表示磁盘包含数据和元数据。默认配置。</li>
<li>dataOnly : 表示磁盘仅包含数据，不包含元数据。</li>
<li>metadataOnly : 表示磁盘仅包含元数据，不包含数据。</li>
<li>descOnly : 表示磁盘不包含数据和文件元数据。仅用于保存文件系统描述符的副本，并可用作某些灾难恢复配置中的第三故障组。</li>
<li>localCache : 表示磁盘将用作本地只读缓存设备。</li>
</ul>
</li>
<li><code>failureGroup</code> : 标识磁盘所属的故障组。默认值为 -1，表示该磁盘与其他任何磁盘没有共同的故障点。</li>
<li><code>pool</code> : 指定 NSD 所分配的存储池的名称。默认值为 system 。</li>
<li><code>thinDiskType</code> : 指定空间回收磁盘类型。<ul>
<li>no : 磁盘设备支持空间回收。此值为默认值。</li>
<li>nvme :  磁盘是支持 TRIM 的 NVMe 设备，支持 mmreclaimspace 命令。</li>
<li>scsi : 磁盘是薄配置的 SCSI 磁盘，支持 mmreclaimspace 命令。</li>
<li>auto : 磁盘类型为 nvme 或 scsi。IBM Storage Scale 将尝试自动检测实际磁盘类型。</li>
</ul>
</li>
</ul>
<p><strong>相关命令:</strong> （以下操作仅在 node01 上执行即可）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 nsd</span></span><br><span class="line">mmcrnsd -F /etc/mmfs/nsdfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 nsd</span></span><br><span class="line">mmlsnsd -m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动集群</span></span><br><span class="line">mmstartup -a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看集群状态</span></span><br><span class="line">mmgetstate -Las</span><br></pre></td></tr></table></figure>


<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 data]<span class="comment"># mmcrnsd -F /etc/mmfs/nsdfile</span></span><br><span class="line">mmcrnsd: Processing disk sdc</span><br><span class="line">mmcrnsd: Processing disk sdd</span><br><span class="line">mmcrnsd: Processing disk sdc</span><br><span class="line">mmcrnsd: Processing disk sdd</span><br><span class="line">mmcrnsd: Processing disk sdc</span><br><span class="line">mmcrnsd: Processing disk sdd</span><br><span class="line">mmcrnsd: Propagating the cluster configuration data to all</span><br><span class="line">  affected nodes.  This is an asynchronous process.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># mmlsnsd -m</span></span><br><span class="line"></span><br><span class="line"> Disk name       NSD volume ID      Device          Node name or Class       Remarks</span><br><span class="line">-------------------------------------------------------------------------------------------</span><br><span class="line"> data01          0A321B396854D5C1   /dev/sdc        node01                   server node</span><br><span class="line"> data02          0A321B396854D5C2   /dev/sdd        node01                   server node</span><br><span class="line"> data03          0A321B3A6854D5C3   /dev/sdc        node02                   server node</span><br><span class="line"> data04          0A321B3A6854D5C4   /dev/sdd        node02                   server node</span><br><span class="line"> data05          0A321B3B6854D5C5   /dev/sdc        node03                   server node</span><br><span class="line"> data06          0A321B3B6854D5C6   /dev/sdd        node03                   server node</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># mmstartup -a</span></span><br><span class="line">Fri Jun 20 10:34:58 CST 2024: mmstartup: Starting GPFS ...</span><br><span class="line">node01:  The GPFS subsystem is already active.</span><br><span class="line">node02:  The GPFS subsystem is already active.</span><br><span class="line">node03:  The GPFS subsystem is already active.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># mmgetstate -Las</span></span><br><span class="line"></span><br><span class="line"> Node number  Node name  Quorum  Nodes up  Total nodes  GPFS state    Remarks</span><br><span class="line">---------------------------------------------------------------------------------</span><br><span class="line">           1  node01        2         3          3      active        quorum node</span><br><span class="line">           2  node02        2         3          3      active        quorum node</span><br><span class="line">           3  node03        2         3          3      active        quorum node</span><br><span class="line"></span><br><span class="line"> Summary information</span><br><span class="line">---------------------</span><br><span class="line">Number of nodes defined <span class="keyword">in</span> the cluster:            3</span><br><span class="line">Number of <span class="built_in">local</span> nodes active <span class="keyword">in</span> the cluster:       3</span><br><span class="line">Number of remote nodes joined <span class="keyword">in</span> this cluster:     0</span><br><span class="line">Number of quorum nodes defined <span class="keyword">in</span> the cluster:     3</span><br><span class="line">Number of quorum nodes active <span class="keyword">in</span> the cluster:      3</span><br><span class="line">Quorum = 2, Quorum achieved</span><br></pre></td></tr></table></figure>


<h2 id="3-6、创建文件系统"><a href="#3-6、创建文件系统" class="headerlink" title="3.6、创建文件系统"></a>3.6、创建文件系统</h2><p><strong>相关命令:</strong> （以下操作仅在 node01 上执行即可）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建文件系统</span></span><br><span class="line">mmcrfs defaultfs \</span><br><span class="line">       -F /etc/mmfs/nsdfile \</span><br><span class="line">       -A <span class="built_in">yes</span> \</span><br><span class="line">       -B 4M \</span><br><span class="line">       -j scatter \</span><br><span class="line">       -m 2 \</span><br><span class="line">       -r 2 \</span><br><span class="line">       -M 2 \</span><br><span class="line">       -R 2 \</span><br><span class="line">       -T /gpfsdata</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件系统</span></span><br><span class="line">mmlsfs all</span><br></pre></td></tr></table></figure>

<p><strong>参数解释:</strong> （详细参数解释参见 <a target="_blank" rel="noopener" href="https://www.ibm.com/docs/en/storage-scale/5.1.8?topic=reference-mmcrfs-command">5.1.8&#x2F;mmcrfs</a> ）</p>
<ul>
<li><code>Device</code> : 指定文件系统名称。</li>
<li><code>-F</code> : 指定一个包含要添加到文件系统的磁盘的 NSD 节和池节的文件。</li>
<li><code>-A</code> : 指示文件系统何时挂载，当指定为 yes 时代表GPFS 守护进程启动时挂载。（默认为 yes ）</li>
<li><code>-B</code> : 指定文件系统中数据块的大小。</li>
<li><code>-j</code> : 指定默认的块分配映射类型。支持 cluster&#x2F;scatter 两种类型。</li>
<li><code>-m</code> : 指定文件的 inode、目录和间接块的默认副本数量。可选值为 1&#x2F;2&#x2F;3 。此值不能大于 MaxMetadataReplicas 的值。默认值为 1。</li>
<li><code>-r</code> : 指定文件的每个数据块的默认副本数量。可选值为 1&#x2F;2&#x2F;3 。此值不能大于 MaxDataReplicas 的值。默认值为 1。</li>
<li><code>-M</code> : 指定文件的 inode、目录和间接块的默认最大副本数量。可选值为 1&#x2F;2&#x2F;3 。此值不能小于 DefaultMetadataReplicas 的值。默认值为 2。</li>
<li><code>-R</code> : 指定文件的数据块的默认最大副本数量。可选值为 1&#x2F;2&#x2F;3 。此值不能小于 DefaultDataReplicas 的值。默认值为 2。</li>
<li><code>-T</code> : 指定 GPFS 文件系统的挂载点目录。如果未指定，挂载点将设置为 DefaultMountDir&#x2F;Device 。 DefaultMountDir 的默认值为 &#x2F;gpfs，但可以使用 mmchconfig 命令进行更改。</li>
</ul>
<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 data]<span class="comment"># mmcrfs defaultfs \</span></span><br><span class="line">&gt;        -F /etc/mmfs/nsdfile \</span><br><span class="line">&gt;        -A <span class="built_in">yes</span> \</span><br><span class="line">&gt;        -B 4M \</span><br><span class="line">&gt;        -j scatter \</span><br><span class="line">&gt;        -m 2 \</span><br><span class="line">&gt;        -r 2 \</span><br><span class="line">&gt;        -M 2 \</span><br><span class="line">&gt;        -R 2 \</span><br><span class="line">&gt;        -T /gpfsdata</span><br><span class="line"></span><br><span class="line">The following disks of defaultfs will be formatted on node node01:</span><br><span class="line">    data01: size 102400 MB</span><br><span class="line">    data02: size 102400 MB</span><br><span class="line">    data03: size 102400 MB</span><br><span class="line">    data04: size 102400 MB</span><br><span class="line">    data05: size 102400 MB</span><br><span class="line">    data06: size 102400 MB</span><br><span class="line">Formatting file system ...</span><br><span class="line">Disks up to size 1.56 TB can be added to storage pool system.</span><br><span class="line">Creating Inode File</span><br><span class="line">  31 % complete on Fri Jun 20 10:49:07 2024</span><br><span class="line">  46 % complete on Fri Jun 20 10:49:12 2024</span><br><span class="line">  74 % complete on Fri Jun 20 10:49:17 2024</span><br><span class="line"> 100 % complete on Fri Jun 20 10:49:21 2024</span><br><span class="line">Creating Allocation Maps</span><br><span class="line">Creating Log Files</span><br><span class="line">Clearing Inode Allocation Map</span><br><span class="line">Clearing Block Allocation Map</span><br><span class="line">Formatting Allocation Map <span class="keyword">for</span> storage pool system</span><br><span class="line">Completed creation of file system /dev/defaultfs.</span><br><span class="line">mmcrfs: Propagating the cluster configuration data to all</span><br><span class="line">  affected nodes.  This is an asynchronous process.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># mmlsfs all</span></span><br><span class="line"></span><br><span class="line">File system attributes <span class="keyword">for</span> /dev/defaultfs:</span><br><span class="line">==========================================</span><br><span class="line">flag                value                    description</span><br><span class="line">------------------- ------------------------ -----------------------------------</span><br><span class="line"> -f                 8192                     Minimum fragment (subblock) size <span class="keyword">in</span> bytes</span><br><span class="line"> -i                 4096                     Inode size <span class="keyword">in</span> bytes</span><br><span class="line"> -I                 32768                    Indirect block size <span class="keyword">in</span> bytes</span><br><span class="line"> -m                 2                        Default number of metadata replicas</span><br><span class="line"> -M                 2                        Maximum number of metadata replicas</span><br><span class="line"> -r                 2                        Default number of data replicas</span><br><span class="line"> -R                 2                        Maximum number of data replicas</span><br><span class="line"> -j                 scatter                  Block allocation <span class="built_in">type</span></span><br><span class="line"> -D                 nfs4                     File locking semantics <span class="keyword">in</span> effect</span><br><span class="line"> -k                 all                      ACL semantics <span class="keyword">in</span> effect</span><br><span class="line"> -n                 32                       Estimated number of nodes that will mount file system</span><br><span class="line"> -B                 4194304                  Block size</span><br><span class="line"> -Q                 none                     Quotas accounting enabled</span><br><span class="line">                    none                     Quotas enforced</span><br><span class="line">                    none                     Default quotas enabled</span><br><span class="line"> --perfileset-quota no                       Per-fileset quota enforcement</span><br><span class="line"> --filesetdf        no                       Fileset <span class="built_in">df</span> enabled?</span><br><span class="line"> -V                 31.00 (5.1.7.0)          File system version</span><br><span class="line"> --create-time      Fri Jun 20 11:32:26 2024 File system creation time</span><br><span class="line"> -z                 no                       Is DMAPI enabled?</span><br><span class="line"> -L                 33554432                 Logfile size</span><br><span class="line"> -E                 <span class="built_in">yes</span>                      Exact mtime mount option</span><br><span class="line"> -S                 relatime                 Suppress atime mount option</span><br><span class="line"> -K                 whenpossible             Strict replica allocation option</span><br><span class="line"> --fastea           <span class="built_in">yes</span>                      Fast external attributes enabled?</span><br><span class="line"> --encryption       no                       Encryption enabled?</span><br><span class="line"> --inode-limit      615424                   Maximum number of inodes</span><br><span class="line"> --uid              3B1B320A:6854D64A        File system UID</span><br><span class="line"> --log-replicas     0                        Number of <span class="built_in">log</span> replicas</span><br><span class="line"> --is4KAligned      <span class="built_in">yes</span>                      is4KAligned?</span><br><span class="line"> --rapid-repair     <span class="built_in">yes</span>                      rapidRepair enabled?</span><br><span class="line"> --write-cache-threshold 0                   HAWC Threshold (max 65536)</span><br><span class="line"> --subblocks-per-full-block 512              Number of subblocks per full block</span><br><span class="line"> -P                 system                   Disk storage pools <span class="keyword">in</span> file system</span><br><span class="line"> --file-audit-log   no                       File Audit Logging enabled?</span><br><span class="line"> --maintenance-mode no                       Maintenance Mode enabled?</span><br><span class="line"> --flush-on-close   no                       flush cache on file close enabled?</span><br><span class="line"> --auto-inode-limit no                       Increase maximum number of inodes per inode space automatically?</span><br><span class="line"> --nfs4-owner-write-acl <span class="built_in">yes</span>                  NFSv4 implicit owner WRITE_ACL permission enabled?</span><br><span class="line"> -d                 data01;data02;data03;data04;data05;data06  Disks <span class="keyword">in</span> file system</span><br><span class="line"> -A                 <span class="built_in">yes</span>                      Automatic mount option</span><br><span class="line"> -o                 none                     Additional mount options</span><br><span class="line"> -T                 /gpfsdata                Default mount point</span><br><span class="line"> --mount-priority   0                        Mount priority</span><br></pre></td></tr></table></figure>


<h2 id="3-7、挂载文件系统"><a href="#3-7、挂载文件系统" class="headerlink" title="3.7、挂载文件系统"></a>3.7、挂载文件系统</h2><p>该方式用于在 server 节点上挂载测试文件系统。如果需要在其他客户端上挂载测试文件系统，建议查看第四栏目中的集群运维操作。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 挂载文件系统</span></span><br><span class="line"><span class="comment"># 所有 server 上均挂载，执行时间可能会长一些</span></span><br><span class="line">mmmount defaultfs /gpfsdata -N all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看挂载点信息</span></span><br><span class="line"><span class="built_in">df</span> -hT</span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问文件系统目录</span></span><br><span class="line"><span class="built_in">ls</span> -al /gpfsdata</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消挂载</span></span><br><span class="line">mmumount /gpfsdata</span><br></pre></td></tr></table></figure>

<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 data]<span class="comment"># mmmount defaultfs /gpfsdata -N all</span></span><br><span class="line">Fri Jun 20 11:34:12 CST 2024: mmmount: Mounting file systems ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># df -hT /gpfsdata</span></span><br><span class="line">Filesystem     Type  Size  Used Avail Use% Mounted on</span><br><span class="line">defaultfs      gpfs  600G  6.5G  594G   2% /gpfsdata</span><br></pre></td></tr></table></figure>


<h2 id="3-8、配置Dashboard"><a href="#3-8、配置Dashboard" class="headerlink" title="3.8、配置Dashboard"></a>3.8、配置Dashboard</h2><p>该步骤用于配置 GPFS GUI ， 即 GPFS 的 Dashboard Web UI ，可用于从界面管控集群。 </p>
<p><strong>相关命令:</strong> （在期望运行 Dashboard 的节点上执行，这里选择 node01 和 node02 两个节点）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装外部依赖软件包（在选择的每个节点执行）</span></span><br><span class="line">dnf install -y postgresql-contrib postgresql-server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 gpfs 软件包（在选择的每个节点执行）</span></span><br><span class="line"><span class="built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/zimon_rpms/rhel8/</span><br><span class="line">rpm -ivh gpfs.gss.pmcollector-5.1.8-1.el8.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.gss.pmsensors-5.1.8-1.el8.x86_64.rpm</span><br><span class="line"><span class="built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/gpfs_rpms/</span><br><span class="line">rpm -ivh gpfs.java-5.1.8-1.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.gui-5.1.8-1.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化收集器节点（在其中一个节点执行即可）</span></span><br><span class="line">mmperfmon config generate --collectors node01,node02</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置传感器节点，即监控数据采集的来源节点（在其中一个节点执行即可）</span></span><br><span class="line">mmchnode --perfmon -N node01,node02,node03</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 gui dashboard 组件（在选择的每个节点执行）</span></span><br><span class="line">systemctl start gpfsgui</span><br><span class="line">systemctl <span class="built_in">enable</span> gpfsgui</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 gui 用户，根据提示输出密码（在其中一个节点执行即可）</span></span><br><span class="line">/usr/lpp/mmfs/gui/cli/mkuser admin -g SecurityAdmin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问 web ui ，对应为 node01 或 node02 的地址</span></span><br><span class="line">https://10.10.0.1</span><br><span class="line">https://10.10.0.2</span><br></pre></td></tr></table></figure>


<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 data]<span class="comment"># mmperfmon config generate --collectors node01,node02</span></span><br><span class="line">mmperfmon: Node node02 is not a perfmon node.</span><br><span class="line">mmperfmon: Node node01 is not a perfmon node.</span><br><span class="line">mmperfmon: Propagating the cluster configuration data to all</span><br><span class="line">  affected nodes.  This is an asynchronous process.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node01 data]<span class="comment"># mmchnode --perfmon -N node01,node02,node03</span></span><br><span class="line">Fri Jun 20 13:59:26 CST 2024: mmchnode: Processing node node03</span><br><span class="line">Fri Jun 20 13:59:26 CST 2024: mmchnode: Processing node node02</span><br><span class="line">Fri Jun 20 13:59:26 CST 2024: mmchnode: Processing node node01</span><br><span class="line">mmchnode: Propagating the cluster configuration data to all</span><br><span class="line">  affected nodes.  This is an asynchronous process.</span><br></pre></td></tr></table></figure>

<h2 id="3-9、配置CES"><a href="#3-9、配置CES" class="headerlink" title="3.9、配置CES"></a>3.9、配置CES</h2><p>GPFS 的 CES 节点用于支持 NFS 访问，提供通用的 NFS 访问方式。</p>
<p>GPFS 提供两种高可用 NFS 服务的方式，分别是 <code>Cluster NFS (CNFS)</code> 和 <code>Cluster Export Services (CES)</code> ，二者互斥只能选其一。 </p>
<ul>
<li><code>Cluster NFS (CNFS)</code>: 只支持 NFS 。基于 Linux kernel 的 NFS server ， NFS 的配置不由 GPFS 管理，元数据性能较好。</li>
<li><code>Cluster Export Services (CES)</code>: 支持 NFS&#x2F;SMB&#x2F;Object 。基于用户空间的 Ganesha NFS server ， GPFS 管理 NFS 配置，数据流式访问性能好。</li>
</ul>
<p>我们在 node02 和 node03 上部署 CES 服务。</p>
<table>
<thead>
<tr>
<th align="center">节点名称</th>
<th align="center">节点IP</th>
<th align="center">VIP</th>
</tr>
</thead>
<tbody><tr>
<td align="center">node02</td>
<td align="center">10.10.0.2</td>
<td align="center">10.10.0.102</td>
</tr>
<tr>
<td align="center">node03</td>
<td align="center">10.10.0.3</td>
<td align="center">10.10.0.103</td>
</tr>
</tbody></table>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改所有 server 节点中 /etc/hosts ，添加虚拟 ip 映射</span></span><br><span class="line"><span class="built_in">cat</span> /etc/hosts</span><br><span class="line">10.10.0.1 node01</span><br><span class="line">10.10.0.2 node02</span><br><span class="line">10.10.0.3 node03</span><br><span class="line">10.10.0.101 node01</span><br><span class="line">10.10.0.102 node02</span><br><span class="line">10.10.0.103 node03</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 ces 共享目录（在选择的任一节点上执行）</span></span><br><span class="line">mmchconfig cesSharedRoot=/gpfsdata</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启需要部署 ces 服务的节点上的服务（在选择的每个节点上执行）</span></span><br><span class="line">mmshutdown -N node02</span><br><span class="line">mmstartup -N node02</span><br><span class="line">mmshutdown -N node03</span><br><span class="line">mmstartup -N node03</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 ces 节点（在选择的任一节点上执行）</span></span><br><span class="line">mmchnode --ces-enable -N node02,node03</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查虚拟 ip 的解析是否存在问题（在选择的任一节点上执行）</span></span><br><span class="line">mmcmi host 10.10.0.102</span><br><span class="line">mmcmi host 10.10.0.103</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 ces 虚拟 ip ， ces 组件会在对应的网卡建立该虚拟 ip（在选择的任一节点上执行）</span></span><br><span class="line">mmces address add --ces-ip 10.10.0.102,10.10.0.103</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 ces ip（在选择的任一节点上执行）</span></span><br><span class="line">mmces address list --full-list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 ces 节点上的虚拟 ip 绑定情况（在每个节点上执行）</span></span><br><span class="line">ip a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看集群 ces 节点（在选择的任一节点上执行）</span></span><br><span class="line">mmlscluster --ces</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 nfs-ganesha/smb 软件（在选择的每个节点上执行）</span></span><br><span class="line"><span class="built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/ganesha_rpms/rhel8/</span><br><span class="line">dnf remove -y nfs-ganesha</span><br><span class="line">rpm -ivh gpfs.nfs-ganesha-debuginfo-3.5-ibm071.22.el8.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.nfs-ganesha-3.5-ibm071.22.el8.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.nfs-ganesha-gpfs-3.5-ibm071.22.el8.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.nfs-ganesha-utils-3.5-ibm071.22.el8.x86_64.rpm</span><br><span class="line"><span class="built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/smb_rpms/rhel8/</span><br><span class="line">rpm -ivh gpfs.smb-4.17.5_gpfs_1-3.el8.x86_64.rpm</span><br><span class="line">rpm -ivh gpfs.smb-debuginfo-4.17.5_gpfs_1-3.el8.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 nfs 服务（在选择的任一节点上执行）</span></span><br><span class="line">mmces service <span class="built_in">enable</span> NFS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 nfs 状态</span></span><br><span class="line">mmces service list -a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置用户认证方式</span></span><br><span class="line">mmuserauth service create --data-access-method file --<span class="built_in">type</span> userdefined</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增 nfs export</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /gpfsdata/nfsexport01</span><br><span class="line">mmnfs <span class="built_in">export</span> add /gpfsdata/nfsexport01 --client <span class="string">&quot;10.10.0.1(Access_Type=RW,Squash=no_root_squash)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 客户端挂载 nfs export</span></span><br><span class="line"><span class="comment"># 可以使用 ces vip 的两个任意一个 vip 连接</span></span><br><span class="line">mount -t nfs -o vers=4,ro 10.10.0.102:/gpfsdata/nfsexport01 /mnt/share</span><br></pre></td></tr></table></figure>


<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node02 data]<span class="comment"># mmchconfig cesSharedRoot=/gpfsdata</span></span><br><span class="line">mmchconfig: Command successfully completed</span><br><span class="line">mmchconfig: Propagating the cluster configuration data to all</span><br><span class="line">  affected nodes.  This is an asynchronous process.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node02 data]<span class="comment"># mmchnode --ces-enable -N node02,node03</span></span><br><span class="line">Fri Jun 20 15:49:12 CST 2024: mmchnode: Processing node node03</span><br><span class="line">Fri Jun 20 15:49:19 CST 2024: mmchnode: Processing node node02</span><br><span class="line">mmchnode: Propagating the cluster configuration data to all</span><br><span class="line">  affected nodes.  This is an asynchronous process.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node02 data]<span class="comment"># mmces address list --full-list</span></span><br><span class="line">cesAddress     cesNode     attributes   cesGroup   cesPrefix   preferredNode   unhostableNodes</span><br><span class="line">-------------- ----------- ------------ ---------- ----------- --------------- -----------------</span><br><span class="line">10.10.0.102    node02      none         none       none        none            none</span><br><span class="line">10.10.0.103    node03      none         none       none        none            none</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node02 data]<span class="comment"># ip a</span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/ether 00:50:56:85:34:ba brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.10.0.2/24 brd 10.10.0.255 scope global noprefixroute ens0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.10.0.102/24 scope global secondary ens0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::250:56ff:fe85:34ba/64 scope <span class="built_in">link</span> noprefixroute</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node03 data]<span class="comment"># ip a</span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/ether 00:50:56:85:fb:43 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.10.0.3/24 brd 10.10.0.255 scope global noprefixroute ens0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.10.0.103/24 scope global secondary ens0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::250:56ff:fe85:fb43/64 scope <span class="built_in">link</span> noprefixroute</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node02 data]<span class="comment"># mmlscluster --ces</span></span><br><span class="line">GPFS cluster information</span><br><span class="line">========================</span><br><span class="line">  GPFS cluster name:         gpfscluster.node01</span><br><span class="line">  GPFS cluster <span class="built_in">id</span>:           12883004940134699797</span><br><span class="line"></span><br><span class="line">Cluster Export Services global parameters</span><br><span class="line">-----------------------------------------</span><br><span class="line">  Shared root directory:                /gpfsdata</span><br><span class="line">  Enabled Services:                     None</span><br><span class="line">  Log level:                            0</span><br><span class="line">  Address distribution policy:          even-coverage</span><br><span class="line"></span><br><span class="line">Node   Daemon node name            IP address       CES IP address list</span><br><span class="line">-----------------------------------------------------------------------</span><br><span class="line">   2   node02                      10.10.0.2        10.10.0.102</span><br><span class="line">   3   node03                      10.10.0.3        10.10.0.103</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node02 data]<span class="comment"># mmces service enable nfs</span></span><br><span class="line">mmchconfig: Command successfully completed</span><br><span class="line">mmchconfig: Propagating the cluster configuration data to all</span><br><span class="line">  affected nodes.  This is an asynchronous process.</span><br><span class="line">node02:  NFS: service already running.</span><br><span class="line">node03:  NFS: service already running.</span><br></pre></td></tr></table></figure>



<h1 id="四、集群运维"><a href="#四、集群运维" class="headerlink" title="四、集群运维"></a>四、集群运维</h1><h2 id="4-1、新增客户端"><a href="#4-1、新增客户端" class="headerlink" title="4.1、新增客户端"></a>4.1、新增客户端</h2><p>在执行以下命令操作前，请确保如下条件满足:</p>
<ul>
<li><code>所有 server 节点</code>:<ul>
<li>&#x2F;etc&#x2F;hosts 文件中已配置新增 client 节点的映射记录；</li>
<li>可通过 ssh 免密访问 client 节点；</li>
</ul>
</li>
<li><code>新增的 client 节点</code>:<ul>
<li>&#x2F;etc&#x2F;hosts 文件中已记录所有 server 节点的映射记录；</li>
<li>本地已存在待安装的 gpfs 软件包；</li>
</ul>
</li>
</ul>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ========== 1. 安装 gpfs 依赖包/软件包 ==========</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for centos 7/8</span></span><br><span class="line">dnf install -y ksh m4 kernel-devel kernel-devel-$(<span class="built_in">uname</span> -r) kernel-headers gcc-c++ \</span><br><span class="line">               python3 net-tools perl-Thread-Queue</span><br><span class="line">rpm -ivh --replacepkgs gpfs.base-5.1.8-1.x86_64.rpm</span><br><span class="line">rpm -ivh --replacepkgs gpfs.docs-5.1.8-1.noarch.rpm</span><br><span class="line">rpm -ivh --replacepkgs gpfs.gpl-5.1.8-1.noarch.rpm</span><br><span class="line">rpm -ivh --replacepkgs gpfs.gskit-8.0.55-19.1.x86_64.rpm</span><br><span class="line">rpm -ivh --replacepkgs gpfs.gss.pmsensors-5.1.8-1.el8.x86_64.rpm</span><br><span class="line">rpm -ivh --replacepkgs gpfs.license.da-5.1.8-1.x86_64.rpm</span><br><span class="line">rpm -ivh --replacepkgs gpfs.msg.en_US-5.1.8-1.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># for ubuntu 20/22</span></span><br><span class="line">apt-get install -y make cpp gcc g++ binutils ksh m4 linux-kernel-headers libaio1 \</span><br><span class="line">                   selinux-utils binfmt-support libssl-dev gawk libsasl2-dev</span><br><span class="line">dpkg -i gpfs.afm.cos_1.0.0-10.1_amd64.deb</span><br><span class="line">dpkg -i gpfs.base_5.1.8-1_amd64.deb</span><br><span class="line">dpkg -i gpfs.compression_5.1.8-1_amd64.deb</span><br><span class="line">dpkg -i gpfs.docs_5.1.8-1_all.deb</span><br><span class="line">dpkg -i gpfs.gpl_5.1.8-1_all.deb</span><br><span class="line">dpkg -i gpfs.gskit_8.0.55-19.1_amd64.deb</span><br><span class="line">dpkg -i gpfs.java_5.1.8-1_amd64.deb</span><br><span class="line">dpkg -i gpfs.license.da_5.1.8-1_amd64.deb</span><br><span class="line">dpkg -i gpfs.msg.en-us_5.1.8-1_all.deb</span><br><span class="line"></span><br><span class="line"><span class="comment"># for ubuntu 20</span></span><br><span class="line">dpkg -i gpfs.gss.pmsensors_5.1.8-1.U20.04_amd64.deb</span><br><span class="line">dpkg -i gpfs.librdkafka_5.1.8-1.U20.04_amd64.deb</span><br><span class="line"></span><br><span class="line"><span class="comment"># for ubuntu 22</span></span><br><span class="line">dpkg -i gpfs.gss.pmsensors_5.1.8-1.U22.04_amd64.deb</span><br><span class="line">dpkg -i gpfs.librdkafka_5.1.8-1.U22.04_amd64.deb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 bin 路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for centos 7/8</span></span><br><span class="line">/etc/bashrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;export PATH=\$PATH:/usr/lpp/mmfs/bin&quot;</span> | sudo <span class="built_in">tee</span> -a &gt; /etc/bashrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># for ubuntu 20/22</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;export PATH=\$PATH:/usr/lpp/mmfs/bin&quot;</span> | sudo <span class="built_in">tee</span> -a &gt; /etc/bash.bashrc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========== 2. 构建并添加客户端节点 ==========</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建可移植层软件（在 client 节点上执行）</span></span><br><span class="line">mmbuildgpl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加新 client 节点（在 server 节点上执行）</span></span><br><span class="line">mmaddnode -N client01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整节点许可证（在 server 节点上执行）</span></span><br><span class="line">mmchlicense client -N client01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 client 节点配置（在 server 节点上执行）</span></span><br><span class="line">mmchconfig <span class="built_in">autoload</span>=<span class="built_in">yes</span>,verbsRdma=<span class="built_in">disable</span> -N client01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 client 节点（在 client 节点上执行）</span></span><br><span class="line">mmstartup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看节点挂载</span></span><br><span class="line"><span class="built_in">df</span> -h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置传感器节点（在 server 节点上执行）</span></span><br><span class="line">mmchnode --perfmon -N client01</span><br></pre></td></tr></table></figure>


<h2 id="4-2、GUI运维操作"><a href="#4-2、GUI运维操作" class="headerlink" title="4.2、GUI运维操作"></a>4.2、GUI运维操作</h2><h3 id="4-2-1、命令操作"><a href="#4-2-1、命令操作" class="headerlink" title="4.2.1、命令操作"></a>4.2.1、命令操作</h3><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ========== gui 配置变更 ==========</span></span><br><span class="line"><span class="comment"># 仅需在任意 server 节点上执行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置容量监控节点和间隔</span></span><br><span class="line">mmperfmon config update GPFSDiskCap.restrict=[node] GPFSDiskCap.period=86400</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 fileset 容量监控节点和间隔</span></span><br><span class="line">mmperfmon config update GPFSFilesetQuota.restrict=[node] GPFSFilesetQuota.period=3600</span><br><span class="line"></span><br><span class="line"><span class="comment"># ========== 移除 gui 组件 ==========</span></span><br><span class="line"><span class="comment"># 仅需在任意 server 节点上执行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止 gui 节点服务</span></span><br><span class="line">systemctl stop gpfsgui</span><br><span class="line">systemctl <span class="built_in">disable</span> gpfsgui</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取传感器节点列表</span></span><br><span class="line">mmlscluster | grep perfmon</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除传感器节点</span></span><br><span class="line">mmchnode --noperfmon -N node01,node02,node03</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2、API操作"><a href="#4-2-2、API操作" class="headerlink" title="4.2.2、API操作"></a>4.2.2、API操作</h3><p>GPFS GUI 提供了 API 服务，用于获取集群变更，变更集群配置等。</p>
<p><strong>相关API:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取 gpfs gui api 列表</span></span><br><span class="line">curl -k -u <span class="string">&quot;admin:password&quot;</span> -X GET \</span><br><span class="line">    --header <span class="string">&quot;accept:application/json&quot;</span> \</span><br><span class="line">    <span class="string">&quot;https://10.10.0.1:443/scalemgmt/v2/info&quot;</span></span><br></pre></td></tr></table></figure>


<h2 id="4-3、移除集群"><a href="#4-3、移除集群" class="headerlink" title="4.3、移除集群"></a>4.3、移除集群</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ========== 移除集群 ==========</span></span><br><span class="line"><span class="comment"># 需要在所有 server 节点上执行，执行前需确保已移除所有客户端节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止所有 server 节点</span></span><br><span class="line">mmshutdown</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载软件</span></span><br><span class="line">dnf remove -y <span class="string">&quot;gpfs*&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 gpfs 相关的 fstab 开机挂载</span></span><br><span class="line"><span class="built_in">cat</span> /etc/fstab</span><br><span class="line">vi /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 gpfs 相关的文件</span></span><br><span class="line">ll /etc/systemd/system</span><br><span class="line">ll /etc/systemd/system/multi-user.target.wants</span><br><span class="line"><span class="built_in">rm</span> -rf /etc/systemd/system/gpfscsi-wr.service</span><br><span class="line"><span class="built_in">rm</span> -rf /etc/systemd/system/multi-user.target.wants/gpfscsi-wr.service</span><br><span class="line"><span class="built_in">rm</span> -rf /usr/lpp/mmfs</span><br><span class="line"><span class="built_in">rm</span> -rf /var/mmfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除 gpfs nsd 相关硬盘数据（每块盘都需要执行，避免下次再次部署时新增 nsd 组件出错）</span></span><br><span class="line">fdisk /dev/sdc</span><br><span class="line">wipefs -a /dev/sdc</span><br><span class="line"><span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=/dev/sdc bs=1M count=100</span><br></pre></td></tr></table></figure>

<h2 id="4-4、CES运维"><a href="#4-4、CES运维" class="headerlink" title="4.4、CES运维"></a>4.4、CES运维</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 新增 nfs export</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /gpfsdata/nfsexport01</span><br><span class="line">mmnfs <span class="built_in">export</span> add /gpfsdata/nfsexport01 --client <span class="string">&quot;10.10.0.1(Access_Type=RW,Squash=no_root_squash)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 nfs export</span></span><br><span class="line">mmnfs <span class="built_in">export</span> remove /gpfsdata/nfsexport01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增 nfs export client</span></span><br><span class="line">mmnfs <span class="built_in">export</span> change /gpfsdata/nfsexport01 --nfsadd <span class="string">&quot;10.10.0.5(Access_Type=RO,Squash=no_root_squash)&quot;</span> --nfsposition 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 nfs export client</span></span><br><span class="line">mmnfs <span class="built_in">export</span> change /gpfsdata/nfsexport01 --nfschange <span class="string">&quot;10.10.0.5(Access_Type=RW,Squash=no_root_squash)&quot;</span> --nfsposition 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 nfs export client</span></span><br><span class="line">mmnfs <span class="built_in">export</span> change /gpfsdata/nfsexport01 --nfsremove 10.10.0.5</span><br></pre></td></tr></table></figure>

<h2 id="4-5、配置变更"><a href="#4-5、配置变更" class="headerlink" title="4.5、配置变更"></a>4.5、配置变更</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定配额命令是否忽略数据复制因子。有效值为 yes/no 。默认值为 no。</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># 该值为 no : 两副本集群中设置 quota 为 100GB ，实际可用为 50GB ；</span></span><br><span class="line"><span class="comment"># 该值为 yes : 两副本集群中设置 quota 为 100GB ，实际可用为 100GB ；</span></span><br><span class="line">mmchconfig ignoreReplicationForQuota=<span class="built_in">yes</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定GPFS 文件系统上的df命令输出是否忽略数据复制因子。有效值为 yes/no 。默认值为no。</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># 该值为 no : 两副本集群中，实际存储文件大小为 100GB ，通过 df 显示为 200GB ；</span></span><br><span class="line"><span class="comment"># 该值为 yes : 两副本集群中，实际存储文件大小为 100GB ， 通过 df 显示为 100GB ；</span></span><br><span class="line">mmchconfig ignoreReplicationOnStatfs=<span class="built_in">yes</span></span><br></pre></td></tr></table></figure>

<h2 id="4-6、Fileset运维"><a href="#4-6、Fileset运维" class="headerlink" title="4.6、Fileset运维"></a>4.6、Fileset运维</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 新增 fileset</span></span><br><span class="line">mmcrfileset defaultfs fileset01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 fileset</span></span><br><span class="line">mmdelfileset defaultfs fileset01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 fileset link path</span></span><br><span class="line"><span class="comment"># 输出信息每列自动对齐</span></span><br><span class="line">mmlsfileset defaultfs | column -t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 fileset link path</span></span><br><span class="line">mmlinkfileset defaultfs fileset01 -J /gpfsdata/fileset01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 fileset link path</span></span><br><span class="line">mmunlinkfileset defaultfs fileset01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 fileset quota</span></span><br><span class="line"><span class="comment"># 输出信息每列自动对齐</span></span><br><span class="line">mmrepquota -j defaultfs | column -t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 fileset quota 块大小</span></span><br><span class="line"><span class="comment"># soft limit 为 90G ， hard limit 为 100G</span></span><br><span class="line">mmsetquota defaultfs:fileset01 --block 90G:100G</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 fileset quota 文件数量</span></span><br><span class="line"><span class="comment"># soft limit 为 9000 ， hard limit 为 10000</span></span><br><span class="line">mmsetquota defaultfs:fileset01 --files 9000:10000</span><br></pre></td></tr></table></figure>


<h2 id="4-7、文件-x2F-目录属性"><a href="#4-7、文件-x2F-目录属性" class="headerlink" title="4.7、文件&#x2F;目录属性"></a>4.7、文件&#x2F;目录属性</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看文件属性</span></span><br><span class="line"><span class="built_in">touch</span> /gpfsdata/testfile</span><br><span class="line">mmlsattr -L /gpfsdata/testfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件属性启用 immutable</span></span><br><span class="line">mmchattr -i <span class="built_in">yes</span> /gpfsdata/testfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件属性关闭 immutable</span></span><br><span class="line">mmchattr -i no /gpfsdata/testfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件属性启用 appendonly</span></span><br><span class="line">mmchattr -a <span class="built_in">yes</span> /gpfsdata/testfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件属性关闭 appendonly</span></span><br><span class="line">mmchattr -a no /gpfsdata/testfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看目录属性</span></span><br><span class="line">mmlsattr -L /gpfsdata</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置目录及目录下所有文件目录的属性，开启 immutable</span></span><br><span class="line"><span class="comment"># 其他属性操作同理</span></span><br><span class="line">find /gpfsdata | xargs -n 1 mmchattr -i <span class="built_in">yes</span></span><br></pre></td></tr></table></figure>


<h1 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h1><ul>
<li>GPFS安装搭建: <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a0ecc0838b3b?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation">https://www.jianshu.com/p/a0ecc0838b3b?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation</a></li>
<li>IBM Storage Scale Doc: <a target="_blank" rel="noopener" href="https://www.ibm.com/docs/en/storage-scale/5.1.8">https://www.ibm.com/docs/en/storage-scale/5.1.8</a></li>
<li>GPFS 分布式文件系统在云计算环境中的实践: <a target="_blank" rel="noopener" href="https://www.sohu.com/a/213249408_151779">https://www.sohu.com/a/213249408_151779</a></li>
<li>GPFS 文件系统部署步骤: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/despotic/p/17304002.html">https://www.cnblogs.com/despotic/p/17304002.html</a></li>
<li>安装 GPFS 管理GUI: <a target="_blank" rel="noopener" href="https://www.yaoge123.com/blog/archives/1424">https://www.yaoge123.com/blog/archives/1424</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://bugwz.com">bugwz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://bugwz.com/2024/08/01/gpfs/">https://bugwz.com/2024/08/01/gpfs/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://bugwz.com" target="_blank">咕咕</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/GPFS/">GPFS</a><a class="post-meta__tags" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/">分布式存储</a></div><div class="post-share"><div class="social-share" data-image="/assets/images/bg/gpfs.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/03/05/ceph-csi/" title="Ceph CSI 对接 K8S 指南"><img class="cover" src="/assets/images/bg/ceph.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Ceph CSI 对接 K8S 指南</div></div><div class="info-2"><div class="info-item-1">一、介绍1.1、Ceph CSI 介绍Ceph CSI 插件实现了支持 CSI 的容器编排器 (CO) 与 Ceph 集群之间的接口。它们支持动态配置 Ceph 卷并将其附加到工作负载。项目地址: https://github.com/ceph/ceph-csi 。该仓库包含用于 RBD、CephFS 和 Kubernetes sidecar 部署 YAML 的 Ceph 容器存储接口 (CSI) 驱动程序，以支持 CSI 功能：provisioner、attacher、resizer、driver-registrar 和 snapper。 本文基于 Ceph CSI v3.14.1 版本进行测试。 Ceph CSI 驱动与测试过的 Kubernetes 版本信息表: (参考 known-to-work-co-platforms)    Ceph CSI 版本 Kubernetes...</div></div></div></a><a class="pagination-related" href="/2024/09/01/gpfs-csi/" title="GPFS CSI 对接 K8S 指南"><img class="cover" src="/assets/images/bg/gpfs.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">GPFS CSI 对接 K8S 指南</div></div><div class="info-2"><div class="info-item-1">一、介绍GPFS CSI 指的是 GPFS（现在被称为IBM Spectrum Scale）的容器存储接口。IBM Spectrum Scale 是一种高性能的共享磁盘文件管理系统，旨在支持大规模的数据集和高吞吐量的环境，如高性能计算（HPC），大数据分析和AI工作负载。通过GPFS CSI，用户可以有效地将 Spectrum Scale 集成到 Kubernetes 这样的容器管理系统中，以实现数据的动态扩展和管理。 GPFS CSI 仓库代码: https://github.com/IBM/ibm-spectrum-scale-csi 本文中的机器部署拓扑:    机器节点 机器IP地址 角色    node01 10.10.0.1 Server&#x2F;Client&#x2F;GUI(Dashboard)   node02 10.10.0.2 Server&#x2F;Client   node03 10.10.0.3 Server&#x2F;Client&#x2F;minikube   字段解释:  Server: 部署 GPFS 集群的节点； Client: 挂载...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/09/01/gpfs-csi/" title="GPFS CSI 对接 K8S 指南"><img class="cover" src="/assets/images/bg/gpfs.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-01</div><div class="info-item-2">GPFS CSI 对接 K8S 指南</div></div><div class="info-2"><div class="info-item-1">一、介绍GPFS CSI 指的是 GPFS（现在被称为IBM Spectrum Scale）的容器存储接口。IBM Spectrum Scale 是一种高性能的共享磁盘文件管理系统，旨在支持大规模的数据集和高吞吐量的环境，如高性能计算（HPC），大数据分析和AI工作负载。通过GPFS CSI，用户可以有效地将 Spectrum Scale 集成到 Kubernetes 这样的容器管理系统中，以实现数据的动态扩展和管理。 GPFS CSI 仓库代码: https://github.com/IBM/ibm-spectrum-scale-csi 本文中的机器部署拓扑:    机器节点 机器IP地址 角色    node01 10.10.0.1 Server&#x2F;Client&#x2F;GUI(Dashboard)   node02 10.10.0.2 Server&#x2F;Client   node03 10.10.0.3 Server&#x2F;Client&#x2F;minikube   字段解释:  Server: 部署 GPFS 集群的节点； Client: 挂载...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">bugwz</div><div class="author-info-description">持续学习，持续进步</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">129</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">134</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/bugwz" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81GPFS-%E4%BB%8B%E7%BB%8D"><span class="toc-text">一、GPFS 介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-text">二、环境准备</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1%E3%80%81%E9%85%8D%E7%BD%AE-x2F-etc-x2F-hosts"><span class="toc-text">2.1、配置 &#x2F;etc&#x2F;hosts</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E3%80%81%E9%85%8D%E7%BD%AE-ssh-%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95"><span class="toc-text">2.2、配置 ssh 免密登录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3%E3%80%81%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99%E5%92%8C-selinux"><span class="toc-text">2.3、关闭防火墙和 selinux</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4%E3%80%81%E9%85%8D%E7%BD%AE%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5"><span class="toc-text">2.4、配置时间同步</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5%E3%80%81%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%8C%85"><span class="toc-text">2.5、安装依赖包</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2"><span class="toc-text">三、集群部署</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E3%80%81%E8%A7%A3%E5%8E%8B%E5%B9%B6%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E5%8C%85"><span class="toc-text">3.1、解压并安装软件包</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1%E3%80%81%E8%A7%A3%E5%8E%8B%E5%B9%B6%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E5%8C%85"><span class="toc-text">3.1.1、解压并安装软件包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2%E3%80%81%E4%BB%85%E8%A7%A3%E5%8E%8B%E8%BD%AF%E4%BB%B6%E5%8C%85"><span class="toc-text">3.1.2、仅解压软件包</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2%E3%80%81%E4%BF%AE%E6%94%B9%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-text">3.2、修改环境变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3%E3%80%81%E6%9E%84%E5%BB%BAGPFS%E5%8F%AF%E7%A7%BB%E6%A4%8D%E5%B1%82"><span class="toc-text">3.3、构建GPFS可移植层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4%E3%80%81%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4"><span class="toc-text">3.4、创建集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5%E3%80%81%E5%88%9B%E5%BB%BANSD"><span class="toc-text">3.5、创建NSD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6%E3%80%81%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-text">3.6、创建文件系统</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7%E3%80%81%E6%8C%82%E8%BD%BD%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-text">3.7、挂载文件系统</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-8%E3%80%81%E9%85%8D%E7%BD%AEDashboard"><span class="toc-text">3.8、配置Dashboard</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-9%E3%80%81%E9%85%8D%E7%BD%AECES"><span class="toc-text">3.9、配置CES</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4"><span class="toc-text">四、集群运维</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1%E3%80%81%E6%96%B0%E5%A2%9E%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">4.1、新增客户端</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2%E3%80%81GUI%E8%BF%90%E7%BB%B4%E6%93%8D%E4%BD%9C"><span class="toc-text">4.2、GUI运维操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1%E3%80%81%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C"><span class="toc-text">4.2.1、命令操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2%E3%80%81API%E6%93%8D%E4%BD%9C"><span class="toc-text">4.2.2、API操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3%E3%80%81%E7%A7%BB%E9%99%A4%E9%9B%86%E7%BE%A4"><span class="toc-text">4.3、移除集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4%E3%80%81CES%E8%BF%90%E7%BB%B4"><span class="toc-text">4.4、CES运维</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%98%E6%9B%B4"><span class="toc-text">4.5、配置变更</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6%E3%80%81Fileset%E8%BF%90%E7%BB%B4"><span class="toc-text">4.6、Fileset运维</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-7%E3%80%81%E6%96%87%E4%BB%B6-x2F-%E7%9B%AE%E5%BD%95%E5%B1%9E%E6%80%A7"><span class="toc-text">4.7、文件&#x2F;目录属性</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-text">五、参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 设计实现深入解析"/></a><div class="content"><a class="title" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析">Ceph Crimson 设计实现深入解析</a><time datetime="2025-05-31T16:00:00.000Z" title="发表于 2025-06-01 00:00:00">2025-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 集群搭建指南"/></a><div class="content"><a class="title" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南">Ceph Crimson 集群搭建指南</a><time datetime="2025-01-11T16:00:00.000Z" title="发表于 2025-01-12 00:00:00">2025-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph QoS 机制深入分析"/></a><div class="content"><a class="title" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析">Ceph QoS 机制深入分析</a><time datetime="2024-10-24T16:00:00.000Z" title="发表于 2024-10-25 00:00:00">2024-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/01/gpfs-csi/" title="GPFS CSI 对接 K8S 指南"><img src="/assets/images/bg/gpfs.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GPFS CSI 对接 K8S 指南"/></a><div class="content"><a class="title" href="/2024/09/01/gpfs-csi/" title="GPFS CSI 对接 K8S 指南">GPFS CSI 对接 K8S 指南</a><time datetime="2024-08-31T16:00:00.000Z" title="发表于 2024-09-01 00:00:00">2024-09-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/01/gpfs/" title="GPFS 集群部署与运维记录"><img src="/assets/images/bg/gpfs.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GPFS 集群部署与运维记录"/></a><div class="content"><a class="title" href="/2024/08/01/gpfs/" title="GPFS 集群部署与运维记录">GPFS 集群部署与运维记录</a><time datetime="2024-07-31T16:00:00.000Z" title="发表于 2024-08-01 00:00:00">2024-08-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By bugwz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: '6af3be16b94cec39bcf6',
      clientSecret: '13a5202ff773ffcea6300b6c8ff25f455566737c',
      repo: 'bugwz.github.io',
      owner: 'bugwz',
      admin: ['bugwz'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || '546f0352dc2f857ad2f50f9a673d0147'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><div class="docsearch-wrap"><div id="docsearch" style="display:none"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css/dist/style.min.css"/><script src="https://cdn.jsdelivr.net/npm/@docsearch/js/dist/umd/index.min.js"></script><script>(() => {
  docsearch(Object.assign({
    appId: 'PFB3WGSSCO',
    apiKey: '3e9cd446e41d93f2f130b91698b699f7',
    indexName: 'bugwz',
    container: '#docsearch',
    placeholder: '请输入要搜索的内容',
  }, {"maxResultsPerGroup":10}))

  const handleClick = () => {
    document.querySelector('.DocSearch-Button').click()
  }

  const searchClickFn = () => {
    btf.addEventListenerPjax(document.querySelector('#search-button > .search'), 'click', handleClick)
  }

  searchClickFn()
  window.addEventListener('pjax:complete', searchClickFn)
})()</script></div></div></body></html>