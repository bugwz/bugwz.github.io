<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>GPFS 集群部署与运维记录 | 咕咕</title><meta name="author" content="bugwz"><meta name="copyright" content="bugwz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文重点介绍了 GPFS 集群的部署，以及 GPFS 集群关于客户端的管理等的运维命令等。">
<meta property="og:type" content="article">
<meta property="og:title" content="GPFS 集群部署与运维记录">
<meta property="og:url" content="https://bugwz.com/2024/08/01/gpfs/index.html">
<meta property="og:site_name" content="咕咕">
<meta property="og:description" content="本文重点介绍了 GPFS 集群的部署，以及 GPFS 集群关于客户端的管理等的运维命令等。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bugwz.com/assets/images/bg/gpfs.png">
<meta property="article:published_time" content="2024-07-31T18:30:00.000Z">
<meta property="article:modified_time" content="2025-08-07T14:32:14.375Z">
<meta property="article:author" content="bugwz">
<meta property="article:tag" content="GPFS">
<meta property="article:tag" content="分布式存储">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bugwz.com/assets/images/bg/gpfs.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "GPFS 集群部署与运维记录",
  "url": "https://bugwz.com/2024/08/01/gpfs/",
  "image": "https://bugwz.com/assets/images/bg/gpfs.png",
  "datePublished": "2024-07-31T18:30:00.000Z",
  "dateModified": "2025-08-07T14:32:14.375Z",
  "author": [
    {
      "@type": "Person",
      "name": "bugwz",
      "url": "https://bugwz.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/assets/images/bg/favicon.png"><link rel="canonical" href="https://bugwz.com/2024/08/01/gpfs/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'GPFS 集群部署与运维记录',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/self/github-dark.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">132</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">134</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/assets/images/bg/gpfs.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">咕咕</span></a><a class="nav-page-title" href="/"><span class="site-name">GPFS 集群部署与运维记录</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">GPFS 集群部署与运维记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-31T18:30:00.000Z" title="发表于 2024-08-01 00:00:00">2024-08-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-07T14:32:14.375Z" title="更新于 2025-08-07 20:02:14">2025-08-07</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">9k</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="一、GPFS-介绍"><a href="#一、GPFS-介绍" class="headerlink" title="一、GPFS 介绍"></a>一、GPFS 介绍</h1><p>IBM GPFS (General Parallel File System ,GPFS)是一款并行的文件系统，它保证在资源组内的所有节点可以并行访问整个文件系统，而且针对此文件系统的服务操作，可以同时安全地在此文件系统的多个节点上实现。GPFS 允许客户共享文件，而这些文件可能分布在不同节点的不同硬盘上，保证了数据的一致性和完整性。GPFS支持多种平台的部署，如Windows、Linux、AIX，每种环境部署方式相同，降低了软件部署的复杂度。</p>
<h1 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a>二、环境准备</h1><p><strong>环境拓扑介绍:</strong></p>
<table>
<thead>
<tr>
<th align="center">节点名称</th>
<th align="center">节点IP</th>
<th align="center">节点角色</th>
</tr>
</thead>
<tbody><tr>
<td align="center">node01</td>
<td align="center">10.10.0.1</td>
<td align="center">Server，GUI(Dashboard)</td>
</tr>
<tr>
<td align="center">node02</td>
<td align="center">10.10.0.2</td>
<td align="center">Server，GUI(Dashboard)，CES</td>
</tr>
<tr>
<td align="center">node03</td>
<td align="center">10.10.0.3</td>
<td align="center">Server，CES</td>
</tr>
</tbody></table>
<p><strong>相关操作步骤如下:</strong></p>
<ul>
<li><code>配置 /etc/hosts</code> : 用于节点间的 hostname 相互识别；</li>
<li><code>配置 ssh 免密登录</code> : 用于节点间的相互通信；</li>
<li><code>关闭防火墙和 selinux</code> : 避免网络问题导致节点间通信异常；</li>
<li><code>配置时间同步</code> : 避免时钟不同步，导致节点间通信等其他异常；</li>
<li><code>安装依赖包</code> : 运行 gpfs 的基础依赖软件；</li>
</ul>
<h2 id="2-1、配置-x2F-etc-x2F-hosts"><a href="#2-1、配置-x2F-etc-x2F-hosts" class="headerlink" title="2.1、配置 &#x2F;etc&#x2F;hosts"></a>2.1、配置 &#x2F;etc&#x2F;hosts</h2><p>确保每个 server&#x2F;client 节点上均配置如下的 hosts 记录。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> /etc/hosts<br>10.10.0.1 node01<br>10.10.0.2 node02<br>10.10.0.3 node03<br></code></pre></td></tr></table></figure>

<h2 id="2-2、配置-ssh-免密登录"><a href="#2-2、配置-ssh-免密登录" class="headerlink" title="2.2、配置 ssh 免密登录"></a>2.2、配置 ssh 免密登录</h2><p>确保每个 server 间可以实现 ssh 免密访问，同时任意 server 节点均可免密访问 client 节点。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># node01</span><br>ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node02<br>ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node03<br><br><span class="hljs-comment"># node02</span><br>ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node01<br>ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node03<br><br><span class="hljs-comment"># node03</span><br>ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node01<br>ssh-copy-id -f -i /root/.ssh/id_rsa.pub root@node02<br><br><span class="hljs-comment"># 测试连接，分别位于不同的机器执行测试</span><br>ssh root@node01<br>ssh root@node02<br>ssh root@node03<br></code></pre></td></tr></table></figure>


<h2 id="2-3、关闭防火墙和-selinux"><a href="#2-3、关闭防火墙和-selinux" class="headerlink" title="2.3、关闭防火墙和 selinux"></a>2.3、关闭防火墙和 selinux</h2><p><strong>相关命令:</strong> （以下操作需要在每个 server&#x2F;client 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 关闭防火墙并禁止开机启动</span><br>systemctl stop firewalld<br>systemctl <span class="hljs-built_in">disable</span> firewalld<br><br><span class="hljs-comment"># 关闭 selinux</span><br><span class="hljs-comment"># 将 SELINUX=enforcing 改为 SELINUX=disabled</span><br><span class="hljs-comment"># 重启后生效</span><br>vi /etc/selinux/config<br><span class="hljs-built_in">cat</span> /etc/selinux/config<br></code></pre></td></tr></table></figure>

<h2 id="2-4、配置时间同步"><a href="#2-4、配置时间同步" class="headerlink" title="2.4、配置时间同步"></a>2.4、配置时间同步</h2><p><strong>相关命令:</strong> （以下操作需要在每个 server&#x2F;client 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 启动时间同步并设置开机启动</span><br>systemctl start chronyd<br>systemctl <span class="hljs-built_in">enable</span> chronyd<br><br><span class="hljs-comment"># 检查同步状态</span><br>chronyc tracking<br></code></pre></td></tr></table></figure>


<h2 id="2-5、安装依赖包"><a href="#2-5、安装依赖包" class="headerlink" title="2.5、安装依赖包"></a>2.5、安装依赖包</h2><p><strong>相关命令:</strong> （以下操作需要在每个 server&#x2F;client 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">dnf install -y ksh m4 kernel-devel kernel-devel-$(<span class="hljs-built_in">uname</span> -r) kernel-headers gcc-c++ python3 net-tools perl-Thread-Queue<br></code></pre></td></tr></table></figure>


<h1 id="三、集群部署"><a href="#三、集群部署" class="headerlink" title="三、集群部署"></a>三、集群部署</h1><p>本次集群部署的软件包版本为 <code>5.1.8.1</code> ，完整的软件包名为 <code>Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install</code> 。</p>
<p><strong>集群部署的关键步骤如下:</strong></p>
<ul>
<li>解压并安装软件包 : </li>
<li>修改环境变量 : </li>
<li>构建GPFS可移植层 :</li>
<li>创建集群 : </li>
<li>创建NSD :</li>
<li>创建文件系统 :</li>
<li>挂载文件系统 : </li>
<li>配置Dashboard（可选） : </li>
<li>配置CES（可选） :</li>
</ul>
<h2 id="3-1、解压并安装软件包"><a href="#3-1、解压并安装软件包" class="headerlink" title="3.1、解压并安装软件包"></a>3.1、解压并安装软件包</h2><p>如果你是为了开发测试使用，你可以访问 <a target="_blank" rel="noopener" href="https://www.ibm.com/products/storage-scale">IBM Storage Scale</a> 官方，注册一个账户，之后签署协议，下载对应的开发者版本的软件包。在下面的示例页面中，就可以下载对应的软件包。在这次的测试中，我们并没有使用对应的开发者版本的软件包，而是使用本地生产环境使用的软件包。</p>
<p><img src="/./assets/images/gpfs-download-dev.png" alt="Storage_Scale_Developer-5.2.3.2-x86_64-Linux.zip" loading="lazy"></p>
<h3 id="3-1-1、解压并安装软件包"><a href="#3-1-1、解压并安装软件包" class="headerlink" title="3.1.1、解压并安装软件包"></a>3.1.1、解压并安装软件包</h3><p><strong>相关命令:</strong> （以下操作需要在每个 server 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 解压软件包</span><br><span class="hljs-built_in">chmod</span> +x Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install<br>./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install --<span class="hljs-built_in">help</span><br><br><br><span class="hljs-comment"># 解压软件包，默认安装文件会被解压到 /usr/lpp/mmfs/5.1.8.1 目录下</span><br><span class="hljs-comment"># 不同操作的含义:</span><br><span class="hljs-comment">#     输入 1 : 接受协议（输入 1 后继续安装操作）</span><br><span class="hljs-comment">#     输入 2 : 拒绝协议</span><br><span class="hljs-comment">#     输入 3 : 打印协议</span><br><span class="hljs-comment">#     输入 4 : 阅读非 IBM 条款</span><br><span class="hljs-comment">#     输入 99 : 返回上一屏幕</span><br>./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install --text-only<br><br><span class="hljs-comment"># 安装软件包</span><br><span class="hljs-built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/gpfs_rpms/<br>rpm -ivh gpfs.base-5.1.8-1.x86_64.rpm<br>rpm -ivh gpfs.compression-5.1.8-1.x86_64.rpm<br>rpm -ivh gpfs.docs-5.1.8-1.noarch.rpm<br>rpm -ivh gpfs.gpl-5.1.8-1.noarch.rpm<br>rpm -ivh gpfs.gskit-8.0.55-19.1.x86_64.rpm<br>rpm -ivh gpfs.java-5.1.8-1.x86_64.rpm<br>rpm -ivh gpfs.license.da-5.1.8-1.x86_64.rpm<br>rpm -ivh gpfs.msg.en_US-5.1.8-1.noarch.rpm<br><span class="hljs-built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/zimon_rpms/rhel8/<br>rpm -ivh gpfs.gss.pmcollector-5.1.8-1.el8.x86_64.rpm<br>rpm -ivh gpfs.gss.pmsensors-5.1.8-1.el8.x86_64.rpm<br>rpm -ivh gpfs.pm-ganesha-10.0.0-2.el8.x86_64.rpm<br></code></pre></td></tr></table></figure>


<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[root@node01 data]<span class="hljs-comment"># ./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install --text-only</span><br><br>Extracting License Acceptance Process Tool to /usr/lpp/mmfs/5.1.8.1 ...<br><span class="hljs-built_in">tail</span> -n +660 ./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install | tar -C /usr/lpp/mmfs/5.1.8.1 -xvz --exclude=installer --exclude=*_rpms --exclude=*_debs --exclude=*rpm  --exclude=*tgz --exclude=*deb --exclude=*tools* 1&gt; /dev/null<br><br>Installing JRE ...<br><br>If directory /usr/lpp/mmfs/5.1.8.1 has been created or was previously created during another extraction,<br>.rpm, .deb, and repository related files <span class="hljs-keyword">in</span> it (<span class="hljs-keyword">if</span> there were) will be removed to avoid conflicts with the ones being extracted.<br><br><span class="hljs-built_in">tail</span> -n +660 ./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install | tar -C /usr/lpp/mmfs/5.1.8.1 --wildcards -xvz  ibm-java*tgz 1&gt; /dev/null<br>tar -C /usr/lpp/mmfs/5.1.8.1/ -xzf /usr/lpp/mmfs/5.1.8.1/ibm-java*tgz<br><br>Invoking License Acceptance Process Tool ...<br>/usr/lpp/mmfs/5.1.8.1/ibm-java-x86_64-80/jre/bin/java -<span class="hljs-built_in">cp</span> /usr/lpp/mmfs/5.1.8.1/LAP_HOME/LAPApp.jar com.ibm.lex.lapapp.LAP -l /usr/lpp/mmfs/5.1.8.1/LA_HOME -m /usr/lpp/mmfs/5.1.8.1 -s /usr/lpp/mmfs/5.1.8.1  -text_only<br><br>LICENSE INFORMATION<br><br>The Programs listed below are licensed under the following<br>License Information terms and conditions <span class="hljs-keyword">in</span> addition to the<br>Program license terms previously agreed to by Client and<br>IBM. If Client does not have previously agreed to license<br>terms <span class="hljs-keyword">in</span> effect <span class="hljs-keyword">for</span> the Program, the International Program<br>License Agreement (i125-3301-15) applies.<br><br>Program Name (Program Number):<br>IBM Storage Scale Erasure Code Edition 5.1.8.1 (5737-J34)<br>IBM Storage Scale Data Management Edition 5.1.8.1 (5737-F34)<br>IBM Storage Scale Data Management Edition 5.1.8.1 (5641-DM1)<br>IBM Storage Scale Data Management Edition 5.1.8.1 (5641-DM3)<br><br>Press Enter to <span class="hljs-built_in">continue</span> viewing the license agreement, or<br>enter <span class="hljs-string">&quot;1&quot;</span> to accept the agreement, <span class="hljs-string">&quot;2&quot;</span> to decline it, <span class="hljs-string">&quot;3&quot;</span><br>to <span class="hljs-built_in">print</span> it, <span class="hljs-string">&quot;4&quot;</span> to <span class="hljs-built_in">read</span> non-IBM terms, or <span class="hljs-string">&quot;99&quot;</span> to go back<br>to the previous screen.<br>1<br><br>License Agreement Terms accepted.<br><br>Extracting Product RPMs to /usr/lpp/mmfs/5.1.8.1 ...<br><span class="hljs-built_in">tail</span> -n +660 ./Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install | tar -C /usr/lpp/mmfs/5.1.8.1 --wildcards -xvz  Public_Keys ansible-toolkit cloudkit/dependencies ganesha_debs/ubuntu/ubuntu20 ganesha_debs/ubuntu/ubuntu22 gpfs_debs/ubuntu/ubuntu20 gpfs_debs/ubuntu/ubuntu22 hdfs_rpms/rhel/hdfs_3.1.1.x hdfs_rpms/rhel/hdfs_3.2.2.x hdfs_rpms/rhel/hdfs_3.3.x smb_debs/ubuntu/ubuntu20 smb_debs/ubuntu/ubuntu22 zimon_debs/ubuntu/ubuntu20 zimon_debs/ubuntu/ubuntu22 ganesha_rpms/rhel7 ganesha_rpms/rhel8 ganesha_rpms/rhel9 ganesha_rpms/sles15 gpfs_rpms/rhel7 gpfs_rpms/rhel8 gpfs_rpms/rhel9 gpfs_rpms/sles15 object_rpms/rhel8 smb_rpms/rhel7 smb_rpms/rhel8 smb_rpms/rhel9 smb_rpms/sles15 tools/repo zimon_debs/ubuntu zimon_rpms/rhel7 zimon_rpms/rhel8 zimon_rpms/rhel9 zimon_rpms/sles15 cloudkit gpfs_debs gpfs_rpms manifest 1&gt; /dev/null<br>   - Public_Keys<br>   - ansible-toolkit<br>   - cloudkit/dependencies<br>   - ganesha_debs/ubuntu/ubuntu20<br>   - ganesha_debs/ubuntu/ubuntu22<br>   - gpfs_debs/ubuntu/ubuntu20<br>   - gpfs_debs/ubuntu/ubuntu22<br>   - hdfs_rpms/rhel/hdfs_3.1.1.x<br>   - hdfs_rpms/rhel/hdfs_3.2.2.x<br>   - hdfs_rpms/rhel/hdfs_3.3.x<br>   - smb_debs/ubuntu/ubuntu20<br>   - smb_debs/ubuntu/ubuntu22<br>   - zimon_debs/ubuntu/ubuntu20<br>   - zimon_debs/ubuntu/ubuntu22<br>   - ganesha_rpms/rhel7<br>   - ganesha_rpms/rhel8<br>   - ganesha_rpms/rhel9<br>   - ganesha_rpms/sles15<br>   - gpfs_rpms/rhel7<br>   - gpfs_rpms/rhel8<br>   - gpfs_rpms/rhel9<br>   - gpfs_rpms/sles15<br>   - object_rpms/rhel8<br>   - smb_rpms/rhel7<br>   - smb_rpms/rhel8<br>   - smb_rpms/rhel9<br>   - smb_rpms/sles15<br>   - tools/repo<br>   - zimon_debs/ubuntu<br>   - zimon_rpms/rhel7<br>   - zimon_rpms/rhel8<br>   - zimon_rpms/rhel9<br>   - zimon_rpms/sles15<br>   - cloudkit<br>   - gpfs_debs<br>   - gpfs_rpms<br>   - manifest<br><br>Removing License Acceptance Process Tool from /usr/lpp/mmfs/5.1.8.1 ...<br><span class="hljs-built_in">rm</span> -rf  /usr/lpp/mmfs/5.1.8.1/LAP_HOME /usr/lpp/mmfs/5.1.8.1/LA_HOME<br><br>Removing JRE from /usr/lpp/mmfs/5.1.8.1 ...<br><span class="hljs-built_in">rm</span> -rf /usr/lpp/mmfs/5.1.8.1/ibm-java*tgz<br><br>==================================================================<br>Product packages successfully extracted to /usr/lpp/mmfs/5.1.8.1<br><br>   Cluster installation and protocol deployment<br>      To install a cluster or deploy protocols with the IBM Storage Scale Installation Toolkit:<br>      /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale -h<br><br>      To install a cluster manually:  Use the GPFS packages located within /usr/lpp/mmfs/5.1.8.1/gpfs_&lt;rpms/debs&gt;<br><br>      To upgrade an existing cluster using the IBM Storage Scale Installation Toolkit:<br>      1) Review and update the config:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale config update<br>      2) Update the cluster configuration to reflect the current cluster config:<br>         /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale config populate -N &lt;node&gt;<br>      3) Use online or offline upgrade depending on your requirements:<br>         - Run the online rolling upgrade:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale upgrade -h<br>         - Run the offline upgrade: /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale upgrade config offline -N;<br>               /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale upgrade run<br>      You can also run the parallel offline upgrade to upgrade all nodes parallely after shutting down GPFS<br>      and stopping protocol services on all nodes.<br>      You can run the parallel offline upgrade on all nodes <span class="hljs-keyword">in</span> the cluster, not on a subset of nodes.<br><br>      To add nodes to an existing cluster using the IBM Storage Scale Installation Toolkit:<br>      1) Add nodes to the cluster definition file:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale node add -h<br>      2) Install IBM Storage Scale on the new nodes:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale install -h<br>      3) Deploy protocols on the new nodes:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale deploy -h<br><br>      To add NSDs or file systems to an existing cluster using the IBM Storage Scale Installation Toolkit:<br>      1) Add NSDs or file systems to the cluster definition:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale nsd add -h<br>      2) Install the NSDs or file systems:  /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale install -h<br><br><br>      To update the cluster definition to reflect the current cluster config examples:<br>         /usr/lpp/mmfs/5.1.8.1/ansible-toolkit/spectrumscale config populate -N &lt;node&gt;<br>      1) Manual updates outside of the installation toolkit<br>      2) Sync the current cluster state to the installation toolkit prior to upgrade<br>      3) Switching from a manually managed cluster to the installation toolkit<br><br>===================================================================================<br>To get up and running quickly, consult the IBM Storage Scale Protocols Quick Overview:<br>https://www.ibm.com/docs/en/STXKQY_5.1.8/pdf/scale_povr.pdf<br>===================================================================================<br><br>[root@node01 data]<span class="hljs-comment"># ll /usr/lpp/mmfs/5.1.8.1</span><br>total 40<br>drwxr-xr-x 11 root root   226 Jul 19  2023 ansible-toolkit<br>drwxr-xr-x  3 root root    42 Jul 19  2023 cloudkit<br>drwxr-xr-x  3 root root    20 Jun 19 21:10 ganesha_debs<br>drwxr-xr-x  6 root root    59 Jun 19 21:10 ganesha_rpms<br>drwxr-xr-x  3 root root  4096 Jul 19  2023 gpfs_debs<br>drwxr-xr-x  7 root root  4096 Jul 19  2023 gpfs_rpms<br>drwxr-xr-x  3 root root    18 Jun 19 21:10 hdfs_rpms<br>drwxr-xr-x  3 root root  4096 Jun 19 21:10 license<br>-rw-r--r--  1 root root 25195 Jul 19  2023 manifest<br>drwxr-xr-x  3 root root    19 Jun 19 21:10 object_rpms<br>drwxr-xr-x  2 root root    76 Jul 19  2023 Public_Keys<br>drwxr-xr-x  3 root root    20 Jun 19 21:10 smb_debs<br>drwxr-xr-x  6 root root    59 Jun 19 21:10 smb_rpms<br>drwxr-xr-x  3 root root    18 Jun 19 21:10 tools<br>drwxr-xr-x  3 root root    20 Jun 19 21:10 zimon_debs<br>drwxr-xr-x  6 root root    59 Jun 19 21:10 zimon_rpms<br></code></pre></td></tr></table></figure>


<h3 id="3-1-2、仅解压软件包"><a href="#3-1-2、仅解压软件包" class="headerlink" title="3.1.2、仅解压软件包"></a>3.1.2、仅解压软件包</h3><p>如果我们只想要简单的解压对应的文件，并查看其中的文件列表，而不执行安装操作，可以使用 <a target="_blank" rel="noopener" href="https://sparanoid.com/lab/7z/download.html">7zip</a> 软件来执行解压操作。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 下载并安装 7z 软件</span><br><span class="hljs-built_in">mkdir</span> ./7z<br>wget https://www.7-zip.org/a/7z2409-linux-x64.tar.xz<br>tar -xvf 7z2409-linux-x64.tar.xz -C ./7z<br><span class="hljs-built_in">cp</span> ./7z/7zz* /usr/bin/<br><span class="hljs-built_in">rm</span> -rf ./7z 7z2409-linux-x64.tar.xz<br><br><span class="hljs-comment"># 解压 GPFS 压缩包到当前目录的 gpfs 目录中， 如果 gpfs 目录不存在则自动创建</span><br>7zz x Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install -o./gpfs<br>7zz x ./gpfs/Storage_Scale_Data_Access-5.1.8 -o./gpfs<br><br><span class="hljs-comment"># 查看目录结构</span><br>tree ./gpfs/ -L 1<br></code></pre></td></tr></table></figure>


<p><strong>相关输出信息:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[root@node01 data]<span class="hljs-comment"># 7zz x Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install -o./gpfs</span><br><br>7-Zip (z) 24.09 (x64) : Copyright (c) 1999-2024 Igor Pavlov : 2024-11-28<br> 64-bit locale=en_US.UTF-8 Threads:8 OPEN_MAX:1024, ASM<br><br>Scanning the drive <span class="hljs-keyword">for</span> archives:<br>1 file, 1234441464 bytes (1178 MiB)<br><br>Extracting archive: Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install<br>--<br>Path = Storage_Scale_Data_Access-5.1.8.1-x86_64-Linux-install<br>Type = gzip<br>Offset = 24036<br>Physical Size = 1234417428<br>Headers Size = 10<br>Streams = 1<br><br>Everything is Ok<br><br>Size:       1508546560<br>Compressed: 1234441464<br><br><br>[root@node01 data]<span class="hljs-comment"># 7zz x ./gpfs/Storage_Scale_Data_Access-5.1.8 -o./gpfs</span><br><br>7-Zip (z) 24.09 (x64) : Copyright (c) 1999-2024 Igor Pavlov : 2024-11-28<br> 64-bit locale=en_US.UTF-8 Threads:8 OPEN_MAX:1024, ASM<br><br>Scanning the drive <span class="hljs-keyword">for</span> archives:<br>1 file, 1508546560 bytes (1439 MiB)<br><br>Extracting archive: Storage_Scale_Data_Access-5.1.8<br>--<br>Path = Storage_Scale_Data_Access-5.1.8<br>Type = tar<br>Physical Size = 1508546560<br>Headers Size = 2147328<br>Code Page = UTF-8<br>Characteristics = GNU LongName ASCII<br><br>Everything is Ok<br><br>Folders: 702<br>Files: 2522<br>Size:       1505726177<br>Compressed: 1508546560<br><br><br>[root@node01 data]<span class="hljs-comment"># tree ./gpfs/ -L 1</span><br>./gpfs/<br>├── ansible-toolkit<br>├── cloudkit<br>├── ganesha_debs<br>├── ganesha_rpms<br>├── gpfs_debs<br>├── gpfs_rpms<br>├── hdfs_rpms<br>├── ibm-java-jre-8.0-5.11-linux-x86_64.tgz<br>├── LA_HOME<br>├── LAP_HOME<br>├── manifest<br>├── object_rpms<br>├── Public_Keys<br>├── smb_debs<br>├── smb_rpms<br>├── Storage_Scale_Data_Access-5.1.8<br>├── tools<br>├── zimon_debs<br>└── zimon_rpms<br><br>16 directories, 3 files<br></code></pre></td></tr></table></figure>



<h2 id="3-2、修改环境变量"><a href="#3-2、修改环境变量" class="headerlink" title="3.2、修改环境变量"></a>3.2、修改环境变量</h2><p>为了后续方便使用 gpfs 的相关命令，我们可以把 gpfs bin 目录添加到 PATH 路径中。</p>
<p><strong>相关命令:</strong> （以下操作需要在每个 server 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 设置 gpfs bin PATH 路径</span><br><span class="hljs-built_in">cat</span> /root/.bash_profile<br>vi /root/.bash_profile<br><span class="hljs-comment"># .bash_profile</span><br><br><span class="hljs-comment"># Get the aliases and functions</span><br><span class="hljs-keyword">if</span> [ -f ~/.bashrc ]; <span class="hljs-keyword">then</span><br>        . ~/.bashrc<br><span class="hljs-keyword">fi</span><br><br><span class="hljs-comment"># User specific environment and startup programs</span><br>PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$HOME</span>/bin:/usr/lpp/mmfs/bin<br><span class="hljs-built_in">export</span> PATH<br></code></pre></td></tr></table></figure>


<h2 id="3-3、构建GPFS可移植层"><a href="#3-3、构建GPFS可移植层" class="headerlink" title="3.3、构建GPFS可移植层"></a>3.3、构建GPFS可移植层</h2><p>GPFS 可移植性层特定于当前内核和 GPFS 版本。如果内核或 GPFS 版本发生变化，则需要构建新的 GPFS 可移植层。尽管操作系统内核可能会升级到新版本，但它们在重新启动后才处于活动状态。因此，必须在重新启动操作系统后为这个新内核构建一个 GPFS 可移植层。<br>并且注意在安装新的 GPFS 可移植层之前，请确保先卸载先前版本的 GPFS 可移植层。</p>
<p>构建完成后，终端会输出生成的包的位置，然后，我们可以将生成的包复制到其他机器进行部署。默认情况下，生成的包只能部署到架构、分发级别、Linux 内核和 IBM Spectrum Scale 维护级别与构建 gpfs.gplbin 包的机器相同的机器上。不过仍然建议在每个 server 节点上执行构建操作生成本机的 GPFS 可移植层。</p>
<p><strong>相关命令:</strong> （以下操作需要在每个 server 节点上执行）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 构建 gpfs 可移植层，</span><br>mmbuildgpl --build-package<br></code></pre></td></tr></table></figure>

<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[root@node01 data]<span class="hljs-comment"># mmbuildgpl --build-package</span><br>--------------------------------------------------------<br>mmbuildgpl: Building GPL (5.1.8.1) module begins at Thu Jun 19 21:40:45 CST 2024.<br>--------------------------------------------------------<br>Verifying Kernel Header...<br>  kernel version = 41800348 (418000348000000, 4.18.0-348.el8.x86_64, 4.18.0-348)<br>  module include <span class="hljs-built_in">dir</span> = /lib/modules/4.18.0-348.el8.x86_64/build/include<br>  module build <span class="hljs-built_in">dir</span>   = /lib/modules/4.18.0-348.el8.x86_64/build<br>  kernel <span class="hljs-built_in">source</span> <span class="hljs-built_in">dir</span>  = /usr/src/linux-4.18.0-348.el8.x86_64/include<br>  Found valid kernel header file under /usr/src/kernels/4.18.0-348.el8.x86_64/include<br>Getting Kernel Cipher mode...<br>   Will use skcipher routines<br>Verifying Compiler...<br>  make is present at /bin/make<br>  cpp is present at /bin/cpp<br>  gcc is present at /bin/gcc<br>  g++ is present at /bin/g++<br>  ld is present at /bin/ld<br>Verifying rpmbuild...<br>Verifying libelf devel package...<br>  Verifying  elfutils-libelf-devel is installed ...<br>    Command: /bin/rpm -q  elfutils-libelf-devel<br>    The required package  elfutils-libelf-devel is installed<br>Verifying Additional System Headers...<br>  Verifying kernel-headers is installed ...<br>    Command: /bin/rpm -q kernel-headers<br>    The required package kernel-headers is installed<br>make World ...<br>make InstallImages ...<br>make rpm ...<br>Wrote: /root/rpmbuild/RPMS/x86_64/gpfs.gplbin-4.18.0-348.el8.x86_64-5.1.8-1.x86_64.rpm<br>--------------------------------------------------------<br>mmbuildgpl: Building GPL module completed successfully at Thu Jun 19 21:41:13 CST 2024.<br>--------------------------------------------------------<br></code></pre></td></tr></table></figure>


<h2 id="3-4、创建集群"><a href="#3-4、创建集群" class="headerlink" title="3.4、创建集群"></a>3.4、创建集群</h2><p>GPFS 的仲裁机制和 ZooKeeper 的仲裁机制类似，当有一半以上的节点是 quorum 时，集群才可以启动，即： <code>quorum &gt;= 1 + sizeof(all nodes) / 2</code> 。</p>
<p><strong>相关命令:</strong> （以下操作仅在 node01 节点上执行即可）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建 nodefile 文件</span><br><span class="hljs-built_in">cat</span> /etc/mmfs/nodefile<br>node01:quorum-manager:<br>node02:quorum-manager:<br>node03:quorum-manager:<br><br><br><span class="hljs-comment"># 创建集群</span><br>mmcrcluster -N /etc/mmfs/nodefile -C gpfscluster -r /usr/bin/ssh -R /usr/bin/scp -A<br><br><span class="hljs-comment"># 接受节点许可证</span><br>mmchlicense server --accept -N all<br><br><span class="hljs-comment"># 启动集群节点</span><br>mmstartup -N node01<br>mmstartup -N node02<br>mmstartup -N node03<br></code></pre></td></tr></table></figure>

<p><strong>mmcrcluster 参数说明:</strong></p>
<ul>
<li><code>-N</code> : 表示节点的配置文件。</li>
<li><code>-C</code> : 指定集群的名称。</li>
<li><code>-r</code> : 指定 GPFS 使用的远程 shell 程序的完整路径名。默认值为 &#x2F;usr&#x2F;bin&#x2F;ssh。</li>
<li><code>-R</code> : 指定 GPFS 使用的远程文件复制程序的完整路径名。默认值为 &#x2F;usr&#x2F;bin&#x2F;scp。</li>
<li><code>-A</code> : 指定当节点启动时 GPFS 守护进程自动启动。默认情况下不自动启动守护进程。</li>
</ul>
<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[root@node01 data]<span class="hljs-comment"># mmcrcluster -N /etc/mmfs/nodefile -C gpfscluster -r /usr/bin/ssh -R /usr/bin/scp -A</span><br>mmcrcluster: Performing preliminary node verification ...<br>mmcrcluster: Processing quorum and other critical nodes ...<br>mmcrcluster: Finalizing the cluster data structures ...<br>mmcrcluster: Command successfully completed<br>mmcrcluster: Warning: Not all nodes have proper GPFS license designations.<br>    Use the mmchlicense <span class="hljs-built_in">command</span> to designate licenses as needed.<br>mmcrcluster: [I] The cluster was created with the tscCmdAllowRemoteConnections configuration parameter <span class="hljs-built_in">set</span> to <span class="hljs-string">&quot;no&quot;</span>. If a remote cluster is established with another cluster whose release level (minReleaseLevel) is less than 5.1.3.0, change the value of tscCmdAllowRemoteConnections <span class="hljs-keyword">in</span> this cluster to <span class="hljs-string">&quot;yes&quot;</span>.<br>mmcrcluster: Propagating the cluster configuration data to all<br>  affected nodes.  This is an asynchronous process.<br><br><br>[root@node01 data]<span class="hljs-comment"># mmchlicense server --accept -N all</span><br><br>The following nodes will be designated as possessing server licenses:<br>        node01<br>        node02<br>        node03<br>mmchlicense: Command successfully completed<br>mmchlicense: Propagating the cluster configuration data to all<br>  affected nodes.  This is an asynchronous process.<br><br><br>[root@node01 data]<span class="hljs-comment"># mmstartup -N node01</span><br>Fri Jun 20 10:29:53 CST 2024: mmstartup: Starting GPFS ...<br>[root@node02 data]<span class="hljs-comment"># mmstartup -N node02</span><br>Fri Jun 20 10:29:55 CST 2024: mmstartup: Starting GPFS ...<br>[root@node03 data]<span class="hljs-comment"># mmstartup -N node03</span><br>Fri Jun 20 10:29:58 CST 2024: mmstartup: Starting GPFS ...<br></code></pre></td></tr></table></figure>


<h2 id="3-5、创建NSD"><a href="#3-5、创建NSD" class="headerlink" title="3.5、创建NSD"></a>3.5、创建NSD</h2><p><strong>&#x2F;etc&#x2F;mmfs&#x2F;nsdfile 配置文件内容:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">%nsd: device=/dev/sdc<br>  nsd=data01<br>  servers=node01<br>  usage=dataAndMetadata<br>  failureGroup=-1<br>  pool=system<br>  thinDiskType=auto<br>%nsd: device=/dev/sdd<br>  nsd=data02<br>  servers=node01<br>  usage=dataAndMetadata<br>  failureGroup=-1<br>  pool=system<br>  thinDiskType=auto<br>%nsd: device=/dev/sdc<br>  nsd=data03<br>  servers=node02<br>  usage=dataAndMetadata<br>  failureGroup=-1<br>  pool=system<br>  thinDiskType=auto<br>%nsd: device=/dev/sdd<br>  nsd=data04<br>  servers=node02<br>  usage=dataAndMetadata<br>  failureGroup=-1<br>  pool=system<br>  thinDiskType=auto<br>%nsd: device=/dev/sdc<br>  nsd=data05<br>  servers=node03<br>  usage=dataAndMetadata<br>  failureGroup=-1<br>  pool=system<br>  thinDiskType=auto<br>%nsd: device=/dev/sdd<br>  nsd=data06<br>  servers=node03<br>  usage=dataAndMetadata<br>  failureGroup=-1<br>  pool=system<br>  thinDiskType=auto<br></code></pre></td></tr></table></figure>


<p><strong>配置文件参数解析:</strong></p>
<ul>
<li><code>device</code> : 块设备名称，用于定义为 NSD 的磁盘。</li>
<li><code>nsd</code> : 指定要创建的 NSD 的名称。不能以保留字符串 ‘gpfs’ 开头。</li>
<li><code>servers</code> : 指定一个以逗号分隔的 NSD 服务器节点列表。</li>
<li><code>usage</code> : 指定要存储在磁盘上的数据类型。<ul>
<li>dataAndMetadata : 表示磁盘包含数据和元数据。默认配置。</li>
<li>dataOnly : 表示磁盘仅包含数据，不包含元数据。</li>
<li>metadataOnly : 表示磁盘仅包含元数据，不包含数据。</li>
<li>descOnly : 表示磁盘不包含数据和文件元数据。仅用于保存文件系统描述符的副本，并可用作某些灾难恢复配置中的第三故障组。</li>
<li>localCache : 表示磁盘将用作本地只读缓存设备。</li>
</ul>
</li>
<li><code>failureGroup</code> : 标识磁盘所属的故障组。默认值为 -1，表示该磁盘与其他任何磁盘没有共同的故障点。</li>
<li><code>pool</code> : 指定 NSD 所分配的存储池的名称。默认值为 system 。</li>
<li><code>thinDiskType</code> : 指定空间回收磁盘类型。<ul>
<li>no : 磁盘设备支持空间回收。此值为默认值。</li>
<li>nvme :  磁盘是支持 TRIM 的 NVMe 设备，支持 mmreclaimspace 命令。</li>
<li>scsi : 磁盘是薄配置的 SCSI 磁盘，支持 mmreclaimspace 命令。</li>
<li>auto : 磁盘类型为 nvme 或 scsi。IBM Storage Scale 将尝试自动检测实际磁盘类型。</li>
</ul>
</li>
</ul>
<p><strong>相关命令:</strong> （以下操作仅在 node01 上执行即可）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建 nsd</span><br>mmcrnsd -F /etc/mmfs/nsdfile<br><br><span class="hljs-comment"># 查看 nsd</span><br>mmlsnsd -m<br><br><span class="hljs-comment"># 启动集群</span><br>mmstartup -a<br><br><span class="hljs-comment"># 查看集群状态</span><br>mmgetstate -Las<br></code></pre></td></tr></table></figure>


<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[root@node01 data]<span class="hljs-comment"># mmcrnsd -F /etc/mmfs/nsdfile</span><br>mmcrnsd: Processing disk sdc<br>mmcrnsd: Processing disk sdd<br>mmcrnsd: Processing disk sdc<br>mmcrnsd: Processing disk sdd<br>mmcrnsd: Processing disk sdc<br>mmcrnsd: Processing disk sdd<br>mmcrnsd: Propagating the cluster configuration data to all<br>  affected nodes.  This is an asynchronous process.<br><br><br>[root@node01 data]<span class="hljs-comment"># mmlsnsd -m</span><br><br> Disk name       NSD volume ID      Device          Node name or Class       Remarks<br>-------------------------------------------------------------------------------------------<br> data01          0A321B396854D5C1   /dev/sdc        node01                   server node<br> data02          0A321B396854D5C2   /dev/sdd        node01                   server node<br> data03          0A321B3A6854D5C3   /dev/sdc        node02                   server node<br> data04          0A321B3A6854D5C4   /dev/sdd        node02                   server node<br> data05          0A321B3B6854D5C5   /dev/sdc        node03                   server node<br> data06          0A321B3B6854D5C6   /dev/sdd        node03                   server node<br><br><br>[root@node01 data]<span class="hljs-comment"># mmstartup -a</span><br>Fri Jun 20 10:34:58 CST 2024: mmstartup: Starting GPFS ...<br>node01:  The GPFS subsystem is already active.<br>node02:  The GPFS subsystem is already active.<br>node03:  The GPFS subsystem is already active.<br><br><br>[root@node01 data]<span class="hljs-comment"># mmgetstate -Las</span><br><br> Node number  Node name  Quorum  Nodes up  Total nodes  GPFS state    Remarks<br>---------------------------------------------------------------------------------<br>           1  node01        2         3          3      active        quorum node<br>           2  node02        2         3          3      active        quorum node<br>           3  node03        2         3          3      active        quorum node<br><br> Summary information<br>---------------------<br>Number of nodes defined <span class="hljs-keyword">in</span> the cluster:            3<br>Number of <span class="hljs-built_in">local</span> nodes active <span class="hljs-keyword">in</span> the cluster:       3<br>Number of remote nodes joined <span class="hljs-keyword">in</span> this cluster:     0<br>Number of quorum nodes defined <span class="hljs-keyword">in</span> the cluster:     3<br>Number of quorum nodes active <span class="hljs-keyword">in</span> the cluster:      3<br>Quorum = 2, Quorum achieved<br></code></pre></td></tr></table></figure>


<h2 id="3-6、创建文件系统"><a href="#3-6、创建文件系统" class="headerlink" title="3.6、创建文件系统"></a>3.6、创建文件系统</h2><p><strong>相关命令:</strong> （以下操作仅在 node01 上执行即可）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建文件系统</span><br>mmcrfs defaultfs \<br>       -F /etc/mmfs/nsdfile \<br>       -A <span class="hljs-built_in">yes</span> \<br>       -B 4M \<br>       -j scatter \<br>       -m 2 \<br>       -r 2 \<br>       -M 2 \<br>       -R 2 \<br>       -T /gpfsdata<br><br><span class="hljs-comment"># 查看文件系统</span><br>mmlsfs all<br></code></pre></td></tr></table></figure>

<p><strong>参数解释:</strong> （详细参数解释参见 <a target="_blank" rel="noopener" href="https://www.ibm.com/docs/en/storage-scale/5.1.8?topic=reference-mmcrfs-command">5.1.8&#x2F;mmcrfs</a> ）</p>
<ul>
<li><code>Device</code> : 指定文件系统名称。</li>
<li><code>-F</code> : 指定一个包含要添加到文件系统的磁盘的 NSD 节和池节的文件。</li>
<li><code>-A</code> : 指示文件系统何时挂载，当指定为 yes 时代表GPFS 守护进程启动时挂载。（默认为 yes ）</li>
<li><code>-B</code> : 指定文件系统中数据块的大小。</li>
<li><code>-j</code> : 指定默认的块分配映射类型。支持 cluster&#x2F;scatter 两种类型。</li>
<li><code>-m</code> : 指定文件的 inode、目录和间接块的默认副本数量。可选值为 1&#x2F;2&#x2F;3 。此值不能大于 MaxMetadataReplicas 的值。默认值为 1。</li>
<li><code>-r</code> : 指定文件的每个数据块的默认副本数量。可选值为 1&#x2F;2&#x2F;3 。此值不能大于 MaxDataReplicas 的值。默认值为 1。</li>
<li><code>-M</code> : 指定文件的 inode、目录和间接块的默认最大副本数量。可选值为 1&#x2F;2&#x2F;3 。此值不能小于 DefaultMetadataReplicas 的值。默认值为 2。</li>
<li><code>-R</code> : 指定文件的数据块的默认最大副本数量。可选值为 1&#x2F;2&#x2F;3 。此值不能小于 DefaultDataReplicas 的值。默认值为 2。</li>
<li><code>-T</code> : 指定 GPFS 文件系统的挂载点目录。如果未指定，挂载点将设置为 DefaultMountDir&#x2F;Device 。 DefaultMountDir 的默认值为 &#x2F;gpfs，但可以使用 mmchconfig 命令进行更改。</li>
</ul>
<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[root@node01 data]<span class="hljs-comment"># mmcrfs defaultfs \</span><br>&gt;        -F /etc/mmfs/nsdfile \<br>&gt;        -A <span class="hljs-built_in">yes</span> \<br>&gt;        -B 4M \<br>&gt;        -j scatter \<br>&gt;        -m 2 \<br>&gt;        -r 2 \<br>&gt;        -M 2 \<br>&gt;        -R 2 \<br>&gt;        -T /gpfsdata<br><br>The following disks of defaultfs will be formatted on node node01:<br>    data01: size 102400 MB<br>    data02: size 102400 MB<br>    data03: size 102400 MB<br>    data04: size 102400 MB<br>    data05: size 102400 MB<br>    data06: size 102400 MB<br>Formatting file system ...<br>Disks up to size 1.56 TB can be added to storage pool system.<br>Creating Inode File<br>  31 % complete on Fri Jun 20 10:49:07 2024<br>  46 % complete on Fri Jun 20 10:49:12 2024<br>  74 % complete on Fri Jun 20 10:49:17 2024<br> 100 % complete on Fri Jun 20 10:49:21 2024<br>Creating Allocation Maps<br>Creating Log Files<br>Clearing Inode Allocation Map<br>Clearing Block Allocation Map<br>Formatting Allocation Map <span class="hljs-keyword">for</span> storage pool system<br>Completed creation of file system /dev/defaultfs.<br>mmcrfs: Propagating the cluster configuration data to all<br>  affected nodes.  This is an asynchronous process.<br><br><br>[root@node01 data]<span class="hljs-comment"># mmlsfs all</span><br><br>File system attributes <span class="hljs-keyword">for</span> /dev/defaultfs:<br>==========================================<br>flag                value                    description<br>------------------- ------------------------ -----------------------------------<br> -f                 8192                     Minimum fragment (subblock) size <span class="hljs-keyword">in</span> bytes<br> -i                 4096                     Inode size <span class="hljs-keyword">in</span> bytes<br> -I                 32768                    Indirect block size <span class="hljs-keyword">in</span> bytes<br> -m                 2                        Default number of metadata replicas<br> -M                 2                        Maximum number of metadata replicas<br> -r                 2                        Default number of data replicas<br> -R                 2                        Maximum number of data replicas<br> -j                 scatter                  Block allocation <span class="hljs-built_in">type</span><br> -D                 nfs4                     File locking semantics <span class="hljs-keyword">in</span> effect<br> -k                 all                      ACL semantics <span class="hljs-keyword">in</span> effect<br> -n                 32                       Estimated number of nodes that will mount file system<br> -B                 4194304                  Block size<br> -Q                 none                     Quotas accounting enabled<br>                    none                     Quotas enforced<br>                    none                     Default quotas enabled<br> --perfileset-quota no                       Per-fileset quota enforcement<br> --filesetdf        no                       Fileset <span class="hljs-built_in">df</span> enabled?<br> -V                 31.00 (5.1.7.0)          File system version<br> --create-time      Fri Jun 20 11:32:26 2024 File system creation time<br> -z                 no                       Is DMAPI enabled?<br> -L                 33554432                 Logfile size<br> -E                 <span class="hljs-built_in">yes</span>                      Exact mtime mount option<br> -S                 relatime                 Suppress atime mount option<br> -K                 whenpossible             Strict replica allocation option<br> --fastea           <span class="hljs-built_in">yes</span>                      Fast external attributes enabled?<br> --encryption       no                       Encryption enabled?<br> --inode-limit      615424                   Maximum number of inodes<br> --uid              3B1B320A:6854D64A        File system UID<br> --log-replicas     0                        Number of <span class="hljs-built_in">log</span> replicas<br> --is4KAligned      <span class="hljs-built_in">yes</span>                      is4KAligned?<br> --rapid-repair     <span class="hljs-built_in">yes</span>                      rapidRepair enabled?<br> --write-cache-threshold 0                   HAWC Threshold (max 65536)<br> --subblocks-per-full-block 512              Number of subblocks per full block<br> -P                 system                   Disk storage pools <span class="hljs-keyword">in</span> file system<br> --file-audit-log   no                       File Audit Logging enabled?<br> --maintenance-mode no                       Maintenance Mode enabled?<br> --flush-on-close   no                       flush cache on file close enabled?<br> --auto-inode-limit no                       Increase maximum number of inodes per inode space automatically?<br> --nfs4-owner-write-acl <span class="hljs-built_in">yes</span>                  NFSv4 implicit owner WRITE_ACL permission enabled?<br> -d                 data01;data02;data03;data04;data05;data06  Disks <span class="hljs-keyword">in</span> file system<br> -A                 <span class="hljs-built_in">yes</span>                      Automatic mount option<br> -o                 none                     Additional mount options<br> -T                 /gpfsdata                Default mount point<br> --mount-priority   0                        Mount priority<br></code></pre></td></tr></table></figure>


<h2 id="3-7、挂载文件系统"><a href="#3-7、挂载文件系统" class="headerlink" title="3.7、挂载文件系统"></a>3.7、挂载文件系统</h2><p>该方式用于在 server 节点上挂载测试文件系统。如果需要在其他客户端上挂载测试文件系统，建议查看第四栏目中的集群运维操作。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 挂载文件系统</span><br><span class="hljs-comment"># 所有 server 上均挂载，执行时间可能会长一些</span><br>mmmount defaultfs /gpfsdata -N all<br><br><span class="hljs-comment"># 查看挂载点信息</span><br><span class="hljs-built_in">df</span> -hT<br><br><span class="hljs-comment"># 访问文件系统目录</span><br><span class="hljs-built_in">ls</span> -al /gpfsdata<br><br><span class="hljs-comment"># 取消挂载</span><br>mmumount /gpfsdata<br></code></pre></td></tr></table></figure>

<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[root@node01 data]<span class="hljs-comment"># mmmount defaultfs /gpfsdata -N all</span><br>Fri Jun 20 11:34:12 CST 2024: mmmount: Mounting file systems ...<br><br><br>[root@node01 data]<span class="hljs-comment"># df -hT /gpfsdata</span><br>Filesystem     Type  Size  Used Avail Use% Mounted on<br>defaultfs      gpfs  600G  6.5G  594G   2% /gpfsdata<br></code></pre></td></tr></table></figure>


<h2 id="3-8、配置Dashboard"><a href="#3-8、配置Dashboard" class="headerlink" title="3.8、配置Dashboard"></a>3.8、配置Dashboard</h2><p>该步骤用于配置 GPFS GUI ， 即 GPFS 的 Dashboard Web UI ，可用于从界面管控集群。 </p>
<p><strong>相关命令:</strong> （在期望运行 Dashboard 的节点上执行，这里选择 node01 和 node02 两个节点）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 安装外部依赖软件包（在选择的每个节点执行）</span><br>dnf install -y postgresql-contrib postgresql-server<br><br><span class="hljs-comment"># 安装 gpfs 软件包（在选择的每个节点执行）</span><br><span class="hljs-built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/zimon_rpms/rhel8/<br>rpm -ivh gpfs.gss.pmcollector-5.1.8-1.el8.x86_64.rpm<br>rpm -ivh gpfs.gss.pmsensors-5.1.8-1.el8.x86_64.rpm<br><span class="hljs-built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/gpfs_rpms/<br>rpm -ivh gpfs.java-5.1.8-1.x86_64.rpm<br>rpm -ivh gpfs.gui-5.1.8-1.noarch.rpm<br><br><span class="hljs-comment"># 初始化收集器节点（在其中一个节点执行即可）</span><br>mmperfmon config generate --collectors node01,node02<br><br><span class="hljs-comment"># 设置传感器节点，即监控数据采集的来源节点（在其中一个节点执行即可）</span><br>mmchnode --perfmon -N node01,node02,node03<br><br><span class="hljs-comment"># 启动 gui dashboard 组件（在选择的每个节点执行）</span><br>systemctl start gpfsgui<br>systemctl <span class="hljs-built_in">enable</span> gpfsgui<br><br><span class="hljs-comment"># 创建 gui 用户，根据提示输出密码（在其中一个节点执行即可）</span><br>/usr/lpp/mmfs/gui/cli/mkuser admin -g SecurityAdmin<br><br><span class="hljs-comment"># 访问 web ui ，对应为 node01 或 node02 的地址</span><br>https://10.10.0.1<br>https://10.10.0.2<br></code></pre></td></tr></table></figure>


<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[root@node01 data]<span class="hljs-comment"># mmperfmon config generate --collectors node01,node02</span><br>mmperfmon: Node node02 is not a perfmon node.<br>mmperfmon: Node node01 is not a perfmon node.<br>mmperfmon: Propagating the cluster configuration data to all<br>  affected nodes.  This is an asynchronous process.<br><br><br>[root@node01 data]<span class="hljs-comment"># mmchnode --perfmon -N node01,node02,node03</span><br>Fri Jun 20 13:59:26 CST 2024: mmchnode: Processing node node03<br>Fri Jun 20 13:59:26 CST 2024: mmchnode: Processing node node02<br>Fri Jun 20 13:59:26 CST 2024: mmchnode: Processing node node01<br>mmchnode: Propagating the cluster configuration data to all<br>  affected nodes.  This is an asynchronous process.<br></code></pre></td></tr></table></figure>

<h2 id="3-9、配置CES"><a href="#3-9、配置CES" class="headerlink" title="3.9、配置CES"></a>3.9、配置CES</h2><p>GPFS 的 CES 节点用于支持 NFS 访问，提供通用的 NFS 访问方式。</p>
<p>GPFS 提供两种高可用 NFS 服务的方式，分别是 <code>Cluster NFS (CNFS)</code> 和 <code>Cluster Export Services (CES)</code> ，二者互斥只能选其一。 </p>
<ul>
<li><code>Cluster NFS (CNFS)</code>: 只支持 NFS 。基于 Linux kernel 的 NFS server ， NFS 的配置不由 GPFS 管理，元数据性能较好。</li>
<li><code>Cluster Export Services (CES)</code>: 支持 NFS&#x2F;SMB&#x2F;Object 。基于用户空间的 Ganesha NFS server ， GPFS 管理 NFS 配置，数据流式访问性能好。</li>
</ul>
<p>我们在 node02 和 node03 上部署 CES 服务。</p>
<table>
<thead>
<tr>
<th align="center">节点名称</th>
<th align="center">节点IP</th>
<th align="center">VIP</th>
</tr>
</thead>
<tbody><tr>
<td align="center">node02</td>
<td align="center">10.10.0.2</td>
<td align="center">10.10.0.102</td>
</tr>
<tr>
<td align="center">node03</td>
<td align="center">10.10.0.3</td>
<td align="center">10.10.0.103</td>
</tr>
</tbody></table>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 修改所有 server 节点中 /etc/hosts ，添加虚拟 ip 映射</span><br><span class="hljs-built_in">cat</span> /etc/hosts<br>10.10.0.1 node01<br>10.10.0.2 node02<br>10.10.0.3 node03<br>10.10.0.101 node01<br>10.10.0.102 node02<br>10.10.0.103 node03<br><br><span class="hljs-comment"># 设置 ces 共享目录（在选择的任一节点上执行）</span><br>mmchconfig cesSharedRoot=/gpfsdata<br><br><span class="hljs-comment"># 重启需要部署 ces 服务的节点上的服务（在选择的每个节点上执行）</span><br>mmshutdown -N node02<br>mmstartup -N node02<br>mmshutdown -N node03<br>mmstartup -N node03<br><br><span class="hljs-comment"># 添加 ces 节点（在选择的任一节点上执行）</span><br>mmchnode --ces-enable -N node02,node03<br><br><span class="hljs-comment"># 检查虚拟 ip 的解析是否存在问题（在选择的任一节点上执行）</span><br>mmcmi host 10.10.0.102<br>mmcmi host 10.10.0.103<br><br><span class="hljs-comment"># 添加 ces 虚拟 ip ， ces 组件会在对应的网卡建立该虚拟 ip（在选择的任一节点上执行）</span><br>mmces address add --ces-ip 10.10.0.102,10.10.0.103<br><br><span class="hljs-comment"># 查看 ces ip（在选择的任一节点上执行）</span><br>mmces address list --full-list<br><br><span class="hljs-comment"># 查看 ces 节点上的虚拟 ip 绑定情况（在每个节点上执行）</span><br>ip a<br><br><span class="hljs-comment"># 查看集群 ces 节点（在选择的任一节点上执行）</span><br>mmlscluster --ces<br><br><span class="hljs-comment"># 安装 nfs-ganesha/smb 软件（在选择的每个节点上执行）</span><br><span class="hljs-built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/ganesha_rpms/rhel8/<br>dnf remove -y nfs-ganesha<br>rpm -ivh gpfs.nfs-ganesha-debuginfo-3.5-ibm071.22.el8.x86_64.rpm<br>rpm -ivh gpfs.nfs-ganesha-3.5-ibm071.22.el8.x86_64.rpm<br>rpm -ivh gpfs.nfs-ganesha-gpfs-3.5-ibm071.22.el8.x86_64.rpm<br>rpm -ivh gpfs.nfs-ganesha-utils-3.5-ibm071.22.el8.x86_64.rpm<br><span class="hljs-built_in">cd</span> /usr/lpp/mmfs/5.1.8.1/smb_rpms/rhel8/<br>rpm -ivh gpfs.smb-4.17.5_gpfs_1-3.el8.x86_64.rpm<br>rpm -ivh gpfs.smb-debuginfo-4.17.5_gpfs_1-3.el8.x86_64.rpm<br><br><span class="hljs-comment"># 启动 nfs 服务（在选择的任一节点上执行）</span><br>mmces service <span class="hljs-built_in">enable</span> NFS<br><br><span class="hljs-comment"># 检查 nfs 状态</span><br>mmces service list -a<br><br><span class="hljs-comment"># 设置用户认证方式</span><br>mmuserauth service create --data-access-method file --<span class="hljs-built_in">type</span> userdefined<br><br><span class="hljs-comment"># 新增 nfs export</span><br><span class="hljs-built_in">mkdir</span> -p /gpfsdata/nfsexport01<br>mmnfs <span class="hljs-built_in">export</span> add /gpfsdata/nfsexport01 --client <span class="hljs-string">&quot;10.10.0.1(Access_Type=RW,Squash=no_root_squash)&quot;</span><br><br><span class="hljs-comment"># 客户端挂载 nfs export</span><br><span class="hljs-comment"># 可以使用 ces vip 的两个任意一个 vip 连接</span><br>mount -t nfs -o vers=4,ro 10.10.0.102:/gpfsdata/nfsexport01 /mnt/share<br></code></pre></td></tr></table></figure>


<p><strong>相关操作日志:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[root@node02 data]<span class="hljs-comment"># mmchconfig cesSharedRoot=/gpfsdata</span><br>mmchconfig: Command successfully completed<br>mmchconfig: Propagating the cluster configuration data to all<br>  affected nodes.  This is an asynchronous process.<br><br><br>[root@node02 data]<span class="hljs-comment"># mmchnode --ces-enable -N node02,node03</span><br>Fri Jun 20 15:49:12 CST 2024: mmchnode: Processing node node03<br>Fri Jun 20 15:49:19 CST 2024: mmchnode: Processing node node02<br>mmchnode: Propagating the cluster configuration data to all<br>  affected nodes.  This is an asynchronous process.<br><br><br>[root@node02 data]<span class="hljs-comment"># mmces address list --full-list</span><br>cesAddress     cesNode     attributes   cesGroup   cesPrefix   preferredNode   unhostableNodes<br>-------------- ----------- ------------ ---------- ----------- --------------- -----------------<br>10.10.0.102    node02      none         none       none        none            none<br>10.10.0.103    node03      none         none       none        none            none<br><br><br>[root@node02 data]<span class="hljs-comment"># ip a</span><br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000<br>    <span class="hljs-built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>       valid_lft forever preferred_lft forever<br>    inet6 ::1/128 scope host<br>       valid_lft forever preferred_lft forever<br>2: ens0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000<br>    <span class="hljs-built_in">link</span>/ether 00:50:56:85:34:ba brd ff:ff:ff:ff:ff:ff<br>    inet 10.10.0.2/24 brd 10.10.0.255 scope global noprefixroute ens0<br>       valid_lft forever preferred_lft forever<br>    inet 10.10.0.102/24 scope global secondary ens0<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::250:56ff:fe85:34ba/64 scope <span class="hljs-built_in">link</span> noprefixroute<br>       valid_lft forever preferred_lft forever<br><br><br>[root@node03 data]<span class="hljs-comment"># ip a</span><br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000<br>    <span class="hljs-built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>       valid_lft forever preferred_lft forever<br>    inet6 ::1/128 scope host<br>       valid_lft forever preferred_lft forever<br>2: ens0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000<br>    <span class="hljs-built_in">link</span>/ether 00:50:56:85:fb:43 brd ff:ff:ff:ff:ff:ff<br>    inet 10.10.0.3/24 brd 10.10.0.255 scope global noprefixroute ens0<br>       valid_lft forever preferred_lft forever<br>    inet 10.10.0.103/24 scope global secondary ens0<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::250:56ff:fe85:fb43/64 scope <span class="hljs-built_in">link</span> noprefixroute<br>       valid_lft forever preferred_lft forever<br><br><br>[root@node02 data]<span class="hljs-comment"># mmlscluster --ces</span><br>GPFS cluster information<br>========================<br>  GPFS cluster name:         gpfscluster.node01<br>  GPFS cluster <span class="hljs-built_in">id</span>:           12883004940134699797<br><br>Cluster Export Services global parameters<br>-----------------------------------------<br>  Shared root directory:                /gpfsdata<br>  Enabled Services:                     None<br>  Log level:                            0<br>  Address distribution policy:          even-coverage<br><br>Node   Daemon node name            IP address       CES IP address list<br>-----------------------------------------------------------------------<br>   2   node02                      10.10.0.2        10.10.0.102<br>   3   node03                      10.10.0.3        10.10.0.103<br><br><br>[root@node02 data]<span class="hljs-comment"># mmces service enable nfs</span><br>mmchconfig: Command successfully completed<br>mmchconfig: Propagating the cluster configuration data to all<br>  affected nodes.  This is an asynchronous process.<br>node02:  NFS: service already running.<br>node03:  NFS: service already running.<br></code></pre></td></tr></table></figure>



<h1 id="四、集群运维"><a href="#四、集群运维" class="headerlink" title="四、集群运维"></a>四、集群运维</h1><h2 id="4-1、新增客户端"><a href="#4-1、新增客户端" class="headerlink" title="4.1、新增客户端"></a>4.1、新增客户端</h2><p>在执行以下命令操作前，请确保如下条件满足:</p>
<ul>
<li><code>所有 server 节点</code>:<ul>
<li>&#x2F;etc&#x2F;hosts 文件中已配置新增 client 节点的映射记录；</li>
<li>可通过 ssh 免密访问 client 节点；</li>
</ul>
</li>
<li><code>新增的 client 节点</code>:<ul>
<li>&#x2F;etc&#x2F;hosts 文件中已记录所有 server 节点的映射记录；</li>
<li>本地已存在待安装的 gpfs 软件包；</li>
</ul>
</li>
</ul>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ========== 1. 安装 gpfs 依赖包/软件包 ==========</span><br><br><span class="hljs-comment"># for centos 7/8</span><br>dnf install -y ksh m4 kernel-devel kernel-devel-$(<span class="hljs-built_in">uname</span> -r) kernel-headers gcc-c++ \<br>               python3 net-tools perl-Thread-Queue<br>rpm -ivh --replacepkgs gpfs.base-5.1.8-1.x86_64.rpm<br>rpm -ivh --replacepkgs gpfs.docs-5.1.8-1.noarch.rpm<br>rpm -ivh --replacepkgs gpfs.gpl-5.1.8-1.noarch.rpm<br>rpm -ivh --replacepkgs gpfs.gskit-8.0.55-19.1.x86_64.rpm<br>rpm -ivh --replacepkgs gpfs.gss.pmsensors-5.1.8-1.el8.x86_64.rpm<br>rpm -ivh --replacepkgs gpfs.license.da-5.1.8-1.x86_64.rpm<br>rpm -ivh --replacepkgs gpfs.msg.en_US-5.1.8-1.noarch.rpm<br><br><span class="hljs-comment"># for ubuntu 20/22</span><br>apt-get install -y make cpp gcc g++ binutils ksh m4 linux-kernel-headers libaio1 \<br>                   selinux-utils binfmt-support libssl-dev gawk libsasl2-dev<br>dpkg -i gpfs.afm.cos_1.0.0-10.1_amd64.deb<br>dpkg -i gpfs.base_5.1.8-1_amd64.deb<br>dpkg -i gpfs.compression_5.1.8-1_amd64.deb<br>dpkg -i gpfs.docs_5.1.8-1_all.deb<br>dpkg -i gpfs.gpl_5.1.8-1_all.deb<br>dpkg -i gpfs.gskit_8.0.55-19.1_amd64.deb<br>dpkg -i gpfs.java_5.1.8-1_amd64.deb<br>dpkg -i gpfs.license.da_5.1.8-1_amd64.deb<br>dpkg -i gpfs.msg.en-us_5.1.8-1_all.deb<br><br><span class="hljs-comment"># for ubuntu 20</span><br>dpkg -i gpfs.gss.pmsensors_5.1.8-1.U20.04_amd64.deb<br>dpkg -i gpfs.librdkafka_5.1.8-1.U20.04_amd64.deb<br><br><span class="hljs-comment"># for ubuntu 22</span><br>dpkg -i gpfs.gss.pmsensors_5.1.8-1.U22.04_amd64.deb<br>dpkg -i gpfs.librdkafka_5.1.8-1.U22.04_amd64.deb<br><br><span class="hljs-comment"># 配置 bin 路径</span><br><br><span class="hljs-comment"># for centos 7/8</span><br>/etc/bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;export PATH=\$PATH:/usr/lpp/mmfs/bin&quot;</span> | sudo <span class="hljs-built_in">tee</span> -a &gt; /etc/bashrc<br><br><span class="hljs-comment"># for ubuntu 20/22</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;export PATH=\$PATH:/usr/lpp/mmfs/bin&quot;</span> | sudo <span class="hljs-built_in">tee</span> -a &gt; /etc/bash.bashrc<br><br><br><br><span class="hljs-comment"># ========== 2. 构建并添加客户端节点 ==========</span><br><br><span class="hljs-comment"># 构建可移植层软件（在 client 节点上执行）</span><br>mmbuildgpl<br><br><span class="hljs-comment"># 添加新 client 节点（在 server 节点上执行）</span><br>mmaddnode -N client01<br><br><span class="hljs-comment"># 调整节点许可证（在 server 节点上执行）</span><br>mmchlicense client -N client01<br><br><span class="hljs-comment"># 修改 client 节点配置（在 server 节点上执行）</span><br>mmchconfig <span class="hljs-built_in">autoload</span>=<span class="hljs-built_in">yes</span>,verbsRdma=<span class="hljs-built_in">disable</span> -N client01<br><br><span class="hljs-comment"># 启动 client 节点（在 client 节点上执行）</span><br>mmstartup<br><br><span class="hljs-comment"># 查看节点挂载</span><br><span class="hljs-built_in">df</span> -h<br><br><span class="hljs-comment"># 设置传感器节点（在 server 节点上执行）</span><br>mmchnode --perfmon -N client01<br></code></pre></td></tr></table></figure>


<h2 id="4-2、GUI运维操作"><a href="#4-2、GUI运维操作" class="headerlink" title="4.2、GUI运维操作"></a>4.2、GUI运维操作</h2><h3 id="4-2-1、命令操作"><a href="#4-2-1、命令操作" class="headerlink" title="4.2.1、命令操作"></a>4.2.1、命令操作</h3><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ========== gui 配置变更 ==========</span><br><span class="hljs-comment"># 仅需在任意 server 节点上执行</span><br><br><span class="hljs-comment"># 设置容量监控节点和间隔</span><br>mmperfmon config update GPFSDiskCap.restrict=[node] GPFSDiskCap.period=86400<br><br><span class="hljs-comment"># 设置 fileset 容量监控节点和间隔</span><br>mmperfmon config update GPFSFilesetQuota.restrict=[node] GPFSFilesetQuota.period=3600<br><br><span class="hljs-comment"># ========== 移除 gui 组件 ==========</span><br><span class="hljs-comment"># 仅需在任意 server 节点上执行</span><br><br><span class="hljs-comment"># 停止 gui 节点服务</span><br>systemctl stop gpfsgui<br>systemctl <span class="hljs-built_in">disable</span> gpfsgui<br><br><span class="hljs-comment"># 获取传感器节点列表</span><br>mmlscluster | grep perfmon<br><br><span class="hljs-comment"># 移除传感器节点</span><br>mmchnode --noperfmon -N node01,node02,node03<br></code></pre></td></tr></table></figure>

<h3 id="4-2-2、API操作"><a href="#4-2-2、API操作" class="headerlink" title="4.2.2、API操作"></a>4.2.2、API操作</h3><p>GPFS GUI 提供了 API 服务，用于获取集群变更，变更集群配置等。</p>
<p><strong>相关API:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 获取 gpfs gui api 列表</span><br>curl -k -u <span class="hljs-string">&quot;admin:password&quot;</span> -X GET \<br>    --header <span class="hljs-string">&quot;accept:application/json&quot;</span> \<br>    <span class="hljs-string">&quot;https://10.10.0.1:443/scalemgmt/v2/info&quot;</span><br></code></pre></td></tr></table></figure>


<h2 id="4-3、移除集群"><a href="#4-3、移除集群" class="headerlink" title="4.3、移除集群"></a>4.3、移除集群</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ========== 移除集群 ==========</span><br><span class="hljs-comment"># 需要在所有 server 节点上执行，执行前需确保已移除所有客户端节点</span><br><br><span class="hljs-comment"># 停止所有 server 节点</span><br>mmshutdown<br><br><span class="hljs-comment"># 卸载软件</span><br>dnf remove -y <span class="hljs-string">&quot;gpfs*&quot;</span><br><br><span class="hljs-comment"># 移除 gpfs 相关的 fstab 开机挂载</span><br><span class="hljs-built_in">cat</span> /etc/fstab<br>vi /etc/fstab<br><br><span class="hljs-comment"># 移除 gpfs 相关的文件</span><br>ll /etc/systemd/system<br>ll /etc/systemd/system/multi-user.target.wants<br><span class="hljs-built_in">rm</span> -rf /etc/systemd/system/gpfscsi-wr.service<br><span class="hljs-built_in">rm</span> -rf /etc/systemd/system/multi-user.target.wants/gpfscsi-wr.service<br><span class="hljs-built_in">rm</span> -rf /usr/lpp/mmfs<br><span class="hljs-built_in">rm</span> -rf /var/mmfs<br><br><span class="hljs-comment"># 清除 gpfs nsd 相关硬盘数据（每块盘都需要执行，避免下次再次部署时新增 nsd 组件出错）</span><br>fdisk /dev/sdc<br>wipefs -a /dev/sdc<br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=/dev/sdc bs=1M count=100<br></code></pre></td></tr></table></figure>

<h2 id="4-4、CES运维"><a href="#4-4、CES运维" class="headerlink" title="4.4、CES运维"></a>4.4、CES运维</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 新增 nfs export</span><br><span class="hljs-built_in">mkdir</span> -p /gpfsdata/nfsexport01<br>mmnfs <span class="hljs-built_in">export</span> add /gpfsdata/nfsexport01 --client <span class="hljs-string">&quot;10.10.0.1(Access_Type=RW,Squash=no_root_squash)&quot;</span><br><br><span class="hljs-comment"># 移除 nfs export</span><br>mmnfs <span class="hljs-built_in">export</span> remove /gpfsdata/nfsexport01<br><br><span class="hljs-comment"># 新增 nfs export client</span><br>mmnfs <span class="hljs-built_in">export</span> change /gpfsdata/nfsexport01 --nfsadd <span class="hljs-string">&quot;10.10.0.5(Access_Type=RO,Squash=no_root_squash)&quot;</span> --nfsposition 0<br><br><span class="hljs-comment"># 修改 nfs export client</span><br>mmnfs <span class="hljs-built_in">export</span> change /gpfsdata/nfsexport01 --nfschange <span class="hljs-string">&quot;10.10.0.5(Access_Type=RW,Squash=no_root_squash)&quot;</span> --nfsposition 0<br><br><span class="hljs-comment"># 移除 nfs export client</span><br>mmnfs <span class="hljs-built_in">export</span> change /gpfsdata/nfsexport01 --nfsremove 10.10.0.5<br></code></pre></td></tr></table></figure>

<h2 id="4-5、配置变更"><a href="#4-5、配置变更" class="headerlink" title="4.5、配置变更"></a>4.5、配置变更</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 指定配额命令是否忽略数据复制因子。有效值为 yes/no 。默认值为 no。</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># 该值为 no : 两副本集群中设置 quota 为 100GB ，实际可用为 50GB ；</span><br><span class="hljs-comment"># 该值为 yes : 两副本集群中设置 quota 为 100GB ，实际可用为 100GB ；</span><br>mmchconfig ignoreReplicationForQuota=<span class="hljs-built_in">yes</span><br><br><br><span class="hljs-comment"># 指定GPFS 文件系统上的df命令输出是否忽略数据复制因子。有效值为 yes/no 。默认值为no。</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># 该值为 no : 两副本集群中，实际存储文件大小为 100GB ，通过 df 显示为 200GB ；</span><br><span class="hljs-comment"># 该值为 yes : 两副本集群中，实际存储文件大小为 100GB ， 通过 df 显示为 100GB ；</span><br>mmchconfig ignoreReplicationOnStatfs=<span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure>

<h2 id="4-6、Fileset运维"><a href="#4-6、Fileset运维" class="headerlink" title="4.6、Fileset运维"></a>4.6、Fileset运维</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 新增 fileset</span><br>mmcrfileset defaultfs fileset01<br><br><span class="hljs-comment"># 删除 fileset</span><br>mmdelfileset defaultfs fileset01<br><br><span class="hljs-comment"># 查看 fileset link path</span><br><span class="hljs-comment"># 输出信息每列自动对齐</span><br>mmlsfileset defaultfs | column -t<br><br><span class="hljs-comment"># 创建 fileset link path</span><br>mmlinkfileset defaultfs fileset01 -J /gpfsdata/fileset01<br><br><span class="hljs-comment"># 删除 fileset link path</span><br>mmunlinkfileset defaultfs fileset01<br><br><span class="hljs-comment"># 查看 fileset quota</span><br><span class="hljs-comment"># 输出信息每列自动对齐</span><br>mmrepquota -j defaultfs | column -t<br><br><span class="hljs-comment"># 设置 fileset quota 块大小</span><br><span class="hljs-comment"># soft limit 为 90G ， hard limit 为 100G</span><br>mmsetquota defaultfs:fileset01 --block 90G:100G<br><br><span class="hljs-comment"># 设置 fileset quota 文件数量</span><br><span class="hljs-comment"># soft limit 为 9000 ， hard limit 为 10000</span><br>mmsetquota defaultfs:fileset01 --files 9000:10000<br></code></pre></td></tr></table></figure>


<h2 id="4-7、文件-x2F-目录属性"><a href="#4-7、文件-x2F-目录属性" class="headerlink" title="4.7、文件&#x2F;目录属性"></a>4.7、文件&#x2F;目录属性</h2><blockquote>
<p><strong>注意:</strong> 如果客户端机器上启动了 <code>selinux</code> ，那么当客户端挂载存储时可能会导致 <code>mount | grep gpfs</code> 中存在 <code>seclabel</code> 参数，这可能会导致在启用了 <code>appendonly</code> 的目录中执行 <code>mkdir、cp</code> 等命令时报错 <code>Read-only file system</code> ，从而导致操作结果不明确。比如执行 <code>mkdir</code> 已经成功了，但是仍会报错 <code>Read-only file system</code> ； 比如执行 <code>cp -R</code> 操作失败，但是也已经往目标路径中传输了部分数据等。</p>
</blockquote>
<p><strong>属性说明:</strong></p>
<ul>
<li><code>appendonly</code> :<ul>
<li>对目录设置后，该目录中可新建文件&#x2F;目录，该目录不可删除，不可重命名。且该目录下的子目录和子文件将继承父目录的属性。</li>
<li>对文件设置后，该文件只可追加写，不可删除，不可重命名。</li>
</ul>
</li>
<li><code>immutable</code> : <ul>
<li>对目录设置后，该目录中无法创建任何文件&#x2F;目录，该目录不可删除，不可重命名。所有会导致数据变更的操作均会报错: <code>Read-only file system</code> 。<ul>
<li>当该目录中之前存在文件&#x2F;目录，且当仅对该目录启用 immutable 属性后，该目录下的子文件和子目录不可以被删除，重命名等，但是其子目录中仍然可以创建删除文件目录等操作。</li>
</ul>
</li>
<li>对文件设置后，该文件内容不可修改，不可删除，不可重命名。所有会导致数据变更的操作均会报错: <code>Read-only file system</code> 。</li>
</ul>
</li>
</ul>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看文件属性</span><br><span class="hljs-built_in">touch</span> /gpfsdata/testfile<br>mmlsattr -L /gpfsdata/testfile<br><br><span class="hljs-comment"># 文件属性启用 immutable</span><br>mmchattr -i <span class="hljs-built_in">yes</span> /gpfsdata/testfile<br><br><span class="hljs-comment"># 文件属性关闭 immutable</span><br>mmchattr -i no /gpfsdata/testfile<br><br><span class="hljs-comment"># 文件属性启用 appendonly</span><br>mmchattr -a <span class="hljs-built_in">yes</span> /gpfsdata/testfile<br><br><span class="hljs-comment"># 文件属性关闭 appendonly</span><br>mmchattr -a no /gpfsdata/testfile<br><br><span class="hljs-comment"># 查看目录属性</span><br>mmlsattr -L /gpfsdata<br><br><span class="hljs-comment"># 设置目录及目录下所有文件目录的属性，开启 immutable</span><br><span class="hljs-comment"># 其他属性操作同理</span><br>find /gpfsdata | xargs -n 1 mmchattr -i <span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure>


<h1 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h1><ul>
<li>GPFS安装搭建: <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a0ecc0838b3b?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation">https://www.jianshu.com/p/a0ecc0838b3b?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation</a></li>
<li>IBM Storage Scale Doc: <a target="_blank" rel="noopener" href="https://www.ibm.com/docs/en/storage-scale/5.1.8">https://www.ibm.com/docs/en/storage-scale/5.1.8</a></li>
<li>GPFS 分布式文件系统在云计算环境中的实践: <a target="_blank" rel="noopener" href="https://www.sohu.com/a/213249408_151779">https://www.sohu.com/a/213249408_151779</a></li>
<li>GPFS 文件系统部署步骤: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/despotic/p/17304002.html">https://www.cnblogs.com/despotic/p/17304002.html</a></li>
<li>安装 GPFS 管理GUI: <a target="_blank" rel="noopener" href="https://www.yaoge123.com/blog/archives/1424">https://www.yaoge123.com/blog/archives/1424</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://bugwz.com">bugwz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://bugwz.com/2024/08/01/gpfs/">https://bugwz.com/2024/08/01/gpfs/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://bugwz.com" target="_blank">咕咕</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/GPFS/">GPFS</a><a class="post-meta__tags" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/">分布式存储</a></div><div class="post-share"><div class="social-share" data-image="/assets/images/bg/gpfs.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/05/11/ceph-rdma/" title="Ceph RDMA 集群部署教程"><img class="cover" src="/assets/images/bg/ceph.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Ceph RDMA 集群部署教程</div></div><div class="info-2"><div class="info-item-1">一、Ceph RDMA 介绍RDMA（Remote Direct Memory Access）是一种远程直接内存访问技术，它允许客户端系统将数据从存储服务器的内存直接复制到该客户端自己的内存中。这种内存直通技术可以提升存储带宽，降低访问时延，同时还可以减少客户端和存储的 CPU 负载。 按照 Ceph 文档给出的介绍，目前虽然 Ceph 已经支持 RDMA 功能，但是除了其功能可能处于实验阶段，并且支持的能力可能受限，参考文档 。所以我的意见是并不建议在生产环境中使用。 1.2、RDMA 环境初始化以下测试工具均基于 CentOS 8.5.2111 进行测试，不同系统类型版本对应的软件包及命令可能存在差异。 查看 RDMA 硬件及驱动信息: # RDMA 相关软件dnf install -y infiniband-diags rdma-core rdma-core-devel perftest \               librdmacm librdmacm-utils libibverbs libibverbs-utils iproute# 查看 ib 网卡信息#...</div></div></div></a><a class="pagination-related" href="/2024/08/10/cephfs-nfs-ganesha/" title="CephFS 对接 NFS-Ganesha 使用教程"><img class="cover" src="/assets/images/bg/ceph.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">CephFS 对接 NFS-Ganesha 使用教程</div></div><div class="info-2"><div class="info-item-1">考虑到目前 Ceph 的集群部署主要有两种方式: ceph-ansible 和 cephadm ，因此这里主要会针对这两种方式来详细解释如何使用 CephFS NFS 功能。 一、NFS-Ganesha 介绍1.1、NFS 协议介绍关于不同 NFS 版本的关联文档参见: src&#x2F;doc&#x2F;USEFUL-RFCs.txt    协议版本 发布时间 相关文档    NFSv2 1989 年 RFC 1092   NFSv3 1995 年 RFC 1813   NFSv4(NFSv4.0) 2002 年 RFC 3530, RFC 7530, RFC 7531, RFC 7931   NFSv4(NFSv4.1) 2010 年 RFC 5661, RFC 5662, RFC 5663, RFC 5664, RFC 8435   NFSv4(NFSv4.2) 2016 年 RFC 7862, RFC 7863   不同版本的协议特点:  NFSv2 :  无状态协议； 第一个以RFC形式发布的版本，实现了基本的功能； 每次读写操作中传输数据的最大长度上限值为 8192...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/09/01/gpfs-csi/" title="GPFS CSI 对接 K8S 指南"><img class="cover" src="/assets/images/bg/gpfs.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-01</div><div class="info-item-2">GPFS CSI 对接 K8S 指南</div></div><div class="info-2"><div class="info-item-1">一、介绍GPFS CSI 指的是 GPFS（现在被称为IBM Spectrum Scale）的容器存储接口。IBM Spectrum Scale 是一种高性能的共享磁盘文件管理系统，旨在支持大规模的数据集和高吞吐量的环境，如高性能计算（HPC），大数据分析和AI工作负载。通过GPFS CSI，用户可以有效地将 Spectrum Scale 集成到 Kubernetes 这样的容器管理系统中，以实现数据的动态扩展和管理。 GPFS CSI 仓库代码: https://github.com/IBM/ibm-spectrum-scale-csi 本文中的机器部署拓扑:    机器节点 机器IP地址 角色    node01 10.10.0.1 Server&#x2F;Client&#x2F;GUI(Dashboard)   node02 10.10.0.2 Server&#x2F;Client   node03 10.10.0.3 Server&#x2F;Client&#x2F;minikube   字段解释:  Server: 部署 GPFS 集群的节点； Client: 挂载...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">bugwz</div><div class="author-info-description">持续学习，持续进步</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">132</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">134</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/bugwz" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81GPFS-%E4%BB%8B%E7%BB%8D"><span class="toc-text">一、GPFS 介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-text">二、环境准备</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1%E3%80%81%E9%85%8D%E7%BD%AE-x2F-etc-x2F-hosts"><span class="toc-text">2.1、配置 &#x2F;etc&#x2F;hosts</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E3%80%81%E9%85%8D%E7%BD%AE-ssh-%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95"><span class="toc-text">2.2、配置 ssh 免密登录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3%E3%80%81%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99%E5%92%8C-selinux"><span class="toc-text">2.3、关闭防火墙和 selinux</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4%E3%80%81%E9%85%8D%E7%BD%AE%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5"><span class="toc-text">2.4、配置时间同步</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5%E3%80%81%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%8C%85"><span class="toc-text">2.5、安装依赖包</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2"><span class="toc-text">三、集群部署</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E3%80%81%E8%A7%A3%E5%8E%8B%E5%B9%B6%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E5%8C%85"><span class="toc-text">3.1、解压并安装软件包</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1%E3%80%81%E8%A7%A3%E5%8E%8B%E5%B9%B6%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E5%8C%85"><span class="toc-text">3.1.1、解压并安装软件包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2%E3%80%81%E4%BB%85%E8%A7%A3%E5%8E%8B%E8%BD%AF%E4%BB%B6%E5%8C%85"><span class="toc-text">3.1.2、仅解压软件包</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2%E3%80%81%E4%BF%AE%E6%94%B9%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-text">3.2、修改环境变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3%E3%80%81%E6%9E%84%E5%BB%BAGPFS%E5%8F%AF%E7%A7%BB%E6%A4%8D%E5%B1%82"><span class="toc-text">3.3、构建GPFS可移植层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4%E3%80%81%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4"><span class="toc-text">3.4、创建集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5%E3%80%81%E5%88%9B%E5%BB%BANSD"><span class="toc-text">3.5、创建NSD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6%E3%80%81%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-text">3.6、创建文件系统</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7%E3%80%81%E6%8C%82%E8%BD%BD%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-text">3.7、挂载文件系统</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-8%E3%80%81%E9%85%8D%E7%BD%AEDashboard"><span class="toc-text">3.8、配置Dashboard</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-9%E3%80%81%E9%85%8D%E7%BD%AECES"><span class="toc-text">3.9、配置CES</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4"><span class="toc-text">四、集群运维</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1%E3%80%81%E6%96%B0%E5%A2%9E%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">4.1、新增客户端</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2%E3%80%81GUI%E8%BF%90%E7%BB%B4%E6%93%8D%E4%BD%9C"><span class="toc-text">4.2、GUI运维操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1%E3%80%81%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C"><span class="toc-text">4.2.1、命令操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2%E3%80%81API%E6%93%8D%E4%BD%9C"><span class="toc-text">4.2.2、API操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3%E3%80%81%E7%A7%BB%E9%99%A4%E9%9B%86%E7%BE%A4"><span class="toc-text">4.3、移除集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4%E3%80%81CES%E8%BF%90%E7%BB%B4"><span class="toc-text">4.4、CES运维</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%98%E6%9B%B4"><span class="toc-text">4.5、配置变更</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6%E3%80%81Fileset%E8%BF%90%E7%BB%B4"><span class="toc-text">4.6、Fileset运维</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-7%E3%80%81%E6%96%87%E4%BB%B6-x2F-%E7%9B%AE%E5%BD%95%E5%B1%9E%E6%80%A7"><span class="toc-text">4.7、文件&#x2F;目录属性</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-text">五、参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 设计实现深入解析"/></a><div class="content"><a class="title" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析">Ceph Crimson 设计实现深入解析</a><time datetime="2025-05-31T18:30:00.000Z" title="发表于 2025-06-01 00:00:00">2025-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 集群搭建指南"/></a><div class="content"><a class="title" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南">Ceph Crimson 集群搭建指南</a><time datetime="2025-01-11T18:30:00.000Z" title="发表于 2025-01-12 00:00:00">2025-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/01/cephfs-samba/" title="CephFS 对接 Samba 使用教程"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CephFS 对接 Samba 使用教程"/></a><div class="content"><a class="title" href="/2024/12/01/cephfs-samba/" title="CephFS 对接 Samba 使用教程">CephFS 对接 Samba 使用教程</a><time datetime="2024-11-30T18:30:00.000Z" title="发表于 2024-12-01 00:00:00">2024-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph QoS 机制深入分析"/></a><div class="content"><a class="title" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析">Ceph QoS 机制深入分析</a><time datetime="2024-10-24T18:30:00.000Z" title="发表于 2024-10-25 00:00:00">2024-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/01/gpfs-csi/" title="GPFS CSI 对接 K8S 指南"><img src="/assets/images/bg/gpfs.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GPFS CSI 对接 K8S 指南"/></a><div class="content"><a class="title" href="/2024/09/01/gpfs-csi/" title="GPFS CSI 对接 K8S 指南">GPFS CSI 对接 K8S 指南</a><time datetime="2024-08-31T18:30:00.000Z" title="发表于 2024-09-01 00:00:00">2024-09-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By bugwz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: '6af3be16b94cec39bcf6',
      clientSecret: '13a5202ff773ffcea6300b6c8ff25f455566737c',
      repo: 'bugwz.github.io',
      owner: 'bugwz',
      admin: ['bugwz'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || '546f0352dc2f857ad2f50f9a673d0147'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><div class="docsearch-wrap"><div id="docsearch" style="display:none"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css/dist/style.min.css"/><script src="https://cdn.jsdelivr.net/npm/@docsearch/js/dist/umd/index.min.js"></script><script>(() => {
  docsearch(Object.assign({
    appId: 'PFB3WGSSCO',
    apiKey: '3e9cd446e41d93f2f130b91698b699f7',
    indexName: 'bugwz',
    container: '#docsearch',
    placeholder: '请输入要搜索的内容',
  }, {"maxResultsPerGroup":10}))

  const handleClick = () => {
    document.querySelector('.DocSearch-Button').click()
  }

  const searchClickFn = () => {
    btf.addEventListenerPjax(document.querySelector('#search-button > .search'), 'click', handleClick)
  }

  searchClickFn()
  window.addEventListener('pjax:complete', searchClickFn)
})()</script></div></div></body></html>