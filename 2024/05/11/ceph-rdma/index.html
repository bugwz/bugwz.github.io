<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Ceph RDMA 集群部署教程 | 咕咕</title><meta name="author" content="bugwz"><meta name="copyright" content="bugwz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文介绍了Ceph RDMA技术的应用与部署实践，重点探讨了RDMA（远程直接内存访问）在提升存储性能和降低延迟方面的优势。文章详细记录了在CentOS环境下使用ceph-ansible和cephadm部署Ceph集群的过程，包括RDMA环境初始化、配置优化以及常见问题的解决方案。尽管RDMA功能尚处于实验阶段，但通过合理的配置和调试，可以显著提升存储系统的效率。文章还总结了部署过程中遇到的错误及">
<meta property="og:type" content="article">
<meta property="og:title" content="Ceph RDMA 集群部署教程">
<meta property="og:url" content="https://bugwz.com/2024/05/11/ceph-rdma/index.html">
<meta property="og:site_name" content="咕咕">
<meta property="og:description" content="本文介绍了Ceph RDMA技术的应用与部署实践，重点探讨了RDMA（远程直接内存访问）在提升存储性能和降低延迟方面的优势。文章详细记录了在CentOS环境下使用ceph-ansible和cephadm部署Ceph集群的过程，包括RDMA环境初始化、配置优化以及常见问题的解决方案。尽管RDMA功能尚处于实验阶段，但通过合理的配置和调试，可以显著提升存储系统的效率。文章还总结了部署过程中遇到的错误及">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bugwz.com/assets/images/bg/ceph.png">
<meta property="article:published_time" content="2024-05-10T16:00:00.000Z">
<meta property="article:modified_time" content="2025-08-09T06:56:17.568Z">
<meta property="article:author" content="bugwz">
<meta property="article:tag" content="Ceph">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bugwz.com/assets/images/bg/ceph.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Ceph RDMA 集群部署教程",
  "url": "https://bugwz.com/2024/05/11/ceph-rdma/",
  "image": "https://bugwz.com/assets/images/bg/ceph.png",
  "datePublished": "2024-05-10T16:00:00.000Z",
  "dateModified": "2025-08-09T06:56:17.568Z",
  "author": [
    {
      "@type": "Person",
      "name": "bugwz",
      "url": "https://bugwz.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/assets/images/bg/favicon.png"><link rel="canonical" href="https://bugwz.com/2024/05/11/ceph-rdma/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ceph RDMA 集群部署教程',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/self/github-dark.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">133</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">135</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/assets/images/bg/ceph.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">咕咕</span></a><a class="nav-page-title" href="/"><span class="site-name">Ceph RDMA 集群部署教程</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Ceph RDMA 集群部署教程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-10T16:00:00.000Z" title="发表于 2024-05-11 00:00:00">2024-05-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-09T06:56:17.568Z" title="更新于 2025-08-09 14:56:17">2025-08-09</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7.7k</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="一、Ceph-RDMA-介绍"><a href="#一、Ceph-RDMA-介绍" class="headerlink" title="一、Ceph RDMA 介绍"></a>一、Ceph RDMA 介绍</h1><p>RDMA（Remote Direct Memory Access）是一种远程直接内存访问技术，它允许客户端系统将数据从存储服务器的内存直接复制到该客户端自己的内存中。这种内存直通技术可以提升存储带宽，降低访问时延，同时还可以减少客户端和存储的 CPU 负载。</p>
<p>按照 Ceph 文档给出的介绍，目前虽然 Ceph 已经支持 RDMA 功能，但是除了其功能可能处于实验阶段，并且支持的能力可能受限，<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/network-config-ref/#confval-ms_type">参考文档</a> 。所以我的意见是并不建议在生产环境中使用。</p>
<h2 id="1-2、RDMA-环境初始化"><a href="#1-2、RDMA-环境初始化" class="headerlink" title="1.2、RDMA 环境初始化"></a>1.2、RDMA 环境初始化</h2><p>以下测试工具均基于 CentOS 8.5.2111 进行测试，不同系统类型版本对应的软件包及命令可能存在差异。</p>
<p><strong>查看 RDMA 硬件及驱动信息:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># RDMA 相关软件</span><br>dnf install -y infiniband-diags rdma-core rdma-core-devel perftest \<br>               librdmacm librdmacm-utils libibverbs libibverbs-utils iproute<br><br><span class="hljs-comment"># 查看 ib 网卡信息</span><br><span class="hljs-comment"># 来自软件包 infiniband-diags</span><br>ibstatus<br>ibstat<br><br><span class="hljs-comment"># 查看本机 ib 设备</span><br><span class="hljs-comment"># 来自软件包 libibverbs-utils</span><br>ibv_devices<br>ibv_devinfo<br><br><span class="hljs-comment"># 查看网卡信息</span><br>lspci | grep Mellanox<br><br><span class="hljs-comment"># 查询 ib 网络设备 GID、Port 等信息</span><br><span class="hljs-comment"># 来自 Mellanox 网卡驱动软件包 MLNX_OFED</span><br>show_gids<br><br><span class="hljs-comment"># 查看 rdma 内核模块的状态</span><br><span class="hljs-comment"># 来自 Mellanox 网卡驱动软件包 MLNX_OFED</span><br>/etc/init.d/openibd status<br></code></pre></td></tr></table></figure>

<p><strong>测试 RDMA 网络:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 带宽测试，工具来自于 perftest</span><br><span class="hljs-comment"># 服务器</span><br>ib_send_bw -a -n 1000000 -c RC -d mlx5_bond_0 -q 10 -i 1<br><span class="hljs-comment"># 客户端</span><br>ib_send_bw -a -n 1000000 -c RC -d mlx5_bond_0 -q 10 -i 1 10.10.10.1<br><br><br><span class="hljs-comment"># 延迟测试，工具来自于 perftest</span><br><span class="hljs-comment"># 服务器</span><br>ib_send_lat -a -d mlx5_bond_0 -F -n 1000 -p 18515<br><span class="hljs-comment"># 客户端</span><br>ib_send_lat -a -d mlx5_bond_0 10.10.10.1 -F -n 1000 -p 18515<br></code></pre></td></tr></table></figure>

<p><strong>RDMA 流量带宽监测脚本:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br>DEVICE=<span class="hljs-string">&quot;mlx5_bond_0&quot;</span>   <span class="hljs-comment"># 替换为你的 RDMA 设备名（通过 `ibstat` 查看）</span><br>PORT=1                 <span class="hljs-comment"># 替换为端口号（通过 `show_gids` 查看）</span><br>INTERVAL=1             <span class="hljs-comment"># 刷新间隔（秒）</span><br><br><span class="hljs-comment"># 初始化计数器</span><br>prev_rcv=$(<span class="hljs-built_in">cat</span> /sys/class/infiniband/<span class="hljs-variable">$DEVICE</span>/ports/<span class="hljs-variable">$PORT</span>/counters/port_rcv_data)<br>prev_xmit=$(<span class="hljs-built_in">cat</span> /sys/class/infiniband/<span class="hljs-variable">$DEVICE</span>/ports/<span class="hljs-variable">$PORT</span>/counters/port_xmit_data)<br><br><span class="hljs-keyword">while</span> <span class="hljs-literal">true</span>; <span class="hljs-keyword">do</span><br>  <span class="hljs-built_in">sleep</span> <span class="hljs-variable">$INTERVAL</span><br>  <span class="hljs-comment"># 读取当前计数器</span><br>  curr_rcv=$(<span class="hljs-built_in">cat</span> /sys/class/infiniband/<span class="hljs-variable">$DEVICE</span>/ports/<span class="hljs-variable">$PORT</span>/counters/port_rcv_data)<br>  curr_xmit=$(<span class="hljs-built_in">cat</span> /sys/class/infiniband/<span class="hljs-variable">$DEVICE</span>/ports/<span class="hljs-variable">$PORT</span>/counters/port_xmit_data)<br><br>  <span class="hljs-comment"># 计算差值并转换为速率（单位：MB/s）</span><br>  <span class="hljs-comment"># RDMA 计数器单位为 4 字节</span><br>  rcv_bytes=$(( (curr_rcv - prev_rcv) * <span class="hljs-number">4</span> ))<br>  xmit_bytes=$(( (curr_xmit - prev_xmit) * <span class="hljs-number">4</span> ))<br>  <span class="hljs-comment"># 转换为 MB/s</span><br>  rcv_rate=$(<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;scale=2; <span class="hljs-variable">$rcv_bytes</span> / <span class="hljs-variable">$INTERVAL</span> / 1000000&quot;</span> | bc)<br>  xmit_rate=$(<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;scale=2; <span class="hljs-variable">$xmit_bytes</span> / <span class="hljs-variable">$INTERVAL</span> / 1000000&quot;</span> | bc)<br><br>  <span class="hljs-comment"># 输出结果</span><br>  <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;RX: <span class="hljs-variable">$&#123;rcv_rate&#125;</span> MB/s \t TX: <span class="hljs-variable">$&#123;xmit_rate&#125;</span> MB/s&quot;</span><br><br>  <span class="hljs-comment"># 更新计数器</span><br>  prev_rcv=<span class="hljs-variable">$curr_rcv</span><br>  prev_xmit=<span class="hljs-variable">$curr_xmit</span><br><span class="hljs-keyword">done</span><br><br><br><br></code></pre></td></tr></table></figure>

<h1 id="二、使用-ceph-ansible-部署"><a href="#二、使用-ceph-ansible-部署" class="headerlink" title="二、使用 ceph-ansible 部署"></a>二、使用 ceph-ansible 部署</h1><blockquote>
<p><strong>注意:</strong> 本次部署出现了很多问题，最终并没有成功部署至可使用的状态。但是本文总结了一些部署过程及过程中需要注释的事项，供读者参考。</p>
</blockquote>
<p>本次测试环境的机器系统为 CentOS 8.5.2111 ，使用 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-ansible/tree/stable-6.0">ceph-ansible&#x2F;stable-6.0</a> 来部署 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph/tree/v16.2.15">Ceph v16.2.15</a> 版本进行测试。</p>
<p><strong>在执行实际的部署安装前，我们可以使用下面的命令来查看对应的 ceph 软件包是否支持 rdma 。</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 验证 ceph bin 是否链接了 rdma 动态库</span><br>ldd /bin/ceph-osd | egrep <span class="hljs-string">&quot;rdma|verbs&quot;</span><br>ldd /bin/ceph-mon | egrep <span class="hljs-string">&quot;rdma|verbs&quot;</span><br>ldd /bin/ceph-mgr | egrep <span class="hljs-string">&quot;rdma|verbs&quot;</span><br>ldd /bin/ceph-mds | egrep <span class="hljs-string">&quot;rdma|verbs&quot;</span><br><br><span class="hljs-comment"># 验证 ceph bin 是否包含 rdma 的符号</span><br>strings /bin/ceph-osd | grep -i rdma<br>strings /bin/ceph-mon | grep -i rdma<br>strings /bin/ceph-mgr | grep -i rdma<br>strings /bin/ceph-mds | grep -i rdma<br></code></pre></td></tr></table></figure>

<h2 id="2-1、配置初始化"><a href="#2-1、配置初始化" class="headerlink" title="2.1、配置初始化"></a>2.1、配置初始化</h2><h3 id="2-1-1、部署配置初始化"><a href="#2-1-1、部署配置初始化" class="headerlink" title="2.1.1、部署配置初始化"></a>2.1.1、部署配置初始化</h3><blockquote>
<p><strong>注意:</strong> 我们在部署 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph/tree/v16.2.15">Ceph v16.2.15</a> 版本的时候，将 <code>ms_public_type</code> 设置为 <code>async+posix</code> ，但是发现 <code>ceph-mgr</code> 在这种配置下会频繁报错: <code>Infiniband to_dead failed to send a beacon: (115) Operation now in progress</code> 。修改配置 <code>debug ms = 20/20</code> 分析发现 <code>Infiniband recv_cm_meta got bad length (26)</code> 相关报错信息。该错误后续有详细日志介绍，本文仍未解决该问题。</p>
</blockquote>
<p><strong>修改 ceph-ansible 中的 .&#x2F;group_vars&#x2F;all.yml 文件:</strong></p>
<figure class="highlight yml"><table><tr><td class="code"><pre><code class="hljs yml"><span class="hljs-comment"># ceph repo</span><br><span class="hljs-attr">ceph_origin:</span> <span class="hljs-string">repository</span><br><span class="hljs-attr">ceph_repository:</span> <span class="hljs-string">custom</span><br><span class="hljs-attr">ceph_custom_repo:</span> <span class="hljs-string">http://xxxxxxxxxxx/ceph-v16.2.15.repo</span><br><br><span class="hljs-comment"># conf</span><br><span class="hljs-attr">ceph_conf_overrides:</span><br>  <span class="hljs-attr">global:</span><br>    <span class="hljs-attr">ms_type:</span> <span class="hljs-string">async+rdma</span><br>    <span class="hljs-attr">ms_cluster_type:</span> <span class="hljs-string">async+rdma</span><br>    <span class="hljs-attr">ms_public_type:</span> <span class="hljs-string">async+posix</span><br>    <span class="hljs-attr">ms_async_rdma_cm:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">ms_bind_ipv4:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">ms_bind_ipv6:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">ms_async_rdma_type:</span> <span class="hljs-string">ib</span><br>    <span class="hljs-attr">ms_async_rdma_device_name:</span> <span class="hljs-string">mlx5_bond_0</span><br>    <span class="hljs-attr">ms_async_rdma_port_num:</span> <span class="hljs-number">1</span><br>    <span class="hljs-attr">ms_async_rdma_gid_idx:</span> <span class="hljs-number">3</span><br></code></pre></td></tr></table></figure>

<p><strong>使用 aliyun ceph 源的配置文件 ceph-v16.2.15.repo :</strong> (需要将该文件上传到一个 http 服务中供 ceph-ansible 下载)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs conf">[ceph]<br>name=ceph<br>baseurl=https://mirrors.aliyun.com/ceph/rpm-16.2.15/el8/x86_64<br>enabled=1<br>gpgcheck=0<br><br>[ceph-noarch]<br>name=ceph noarch<br>baseurl=https://mirrors.aliyun.com/ceph/rpm-16.2.15/el8/noarch<br>enabled=1<br>gpgcheck=0<br><br>[ceph-source]<br>name=ceph source<br>baseurl=https://mirrors.aliyun.com/ceph/rpm-16.2.15/el8/SRPMS<br>enabled=1<br>gpgcheck=0<br></code></pre></td></tr></table></figure>

<blockquote>
<p>注意: 本次是采用非容器化的部署方式。安装 ceph 软件后，对应的 systemd 的配置文件位于 <code>/usr/lib/systemd/system/</code> 目录中。如果我们在部署时配置了 <code>ceph_&lt;service_name&gt;_systemd_overrides</code> 参数，那么 ceph-ansible 在部署集群时会在部署节点的 <code>/etc/systemd/system/</code> 目录中创建对应服务的配置目录（<code>/etc/systemd/system/ceph-&lt;service_name&gt;@.service.d/</code>）及对应的配置文件（<code>ceph-&lt;service_name&gt;-systemd-overrides.conf</code>），用来覆盖一些特定的参数。</p>
</blockquote>
<p><strong>修改 ceph-ansible 中各 ceph 服务的配置:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 修改 mon systemd 配置文件模板</span><br>vi ./group_vars/mons.yml<br>ceph_mon_systemd_overrides:<br>  Service:<br>    LimitMEMLOCK: infinity<br>    PrivateDevices: no<br><br><span class="hljs-comment"># 修改 mgr systemd 配置文件模板</span><br>vi ./group_vars/mgrs.yml<br>ceph_mgr_systemd_overrides:<br>  Service:<br>    LimitMEMLOCK: infinity<br>    PrivateDevices: no<br><br><span class="hljs-comment"># 修改 osd systemd 配置文件模板</span><br>vi ./group_vars/osds.yml<br>ceph_osd_systemd_overrides:<br>  Service:<br>    LimitMEMLOCK: infinity<br>    PrivateDevices: no<br><br><span class="hljs-comment"># 修改 mds systemd 配置文件模板</span><br>vi ./group_vars/mdss.yml<br>ceph_mds_systemd_overrides:<br>  Service:<br>    LimitMEMLOCK: infinity<br>    PrivateDevices: no<br></code></pre></td></tr></table></figure>


<h3 id="2-1-2、配置部署节点环境"><a href="#2-1-2、配置部署节点环境" class="headerlink" title="2.1.2、配置部署节点环境"></a>2.1.2、配置部署节点环境</h3><p>由于 RDMA 通信要求固定计算机的物理内存（也就是说，当整个计算机在可用内存上启动不足时，内核不允许将该内存交换到分页文件）。固定内存通常是非常特权的操作。为了允许 root 之外的用户运行大型 RDMA 应用程序，可能需要增加非 root 用户在系统中被允许的内存量。这可以通过在 <code>/etc/security/limits.d/</code> 目录中添加一个自定义配置文件来实现。参考配置文件内容: <a target="_blank" rel="noopener" href="https://enterprise-support.nvidia.com/s/article/bring-up-ceph-rdma---developer-s-guide">Bring Up Ceph RDMA - Developer’s Guide</a></p>
<p><strong>修改 Ceph 部署机器上的 &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;rdma.conf 配置:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> /etc/security/limits.d/rdma.conf<br><span class="hljs-comment"># configuration for rdma tuning</span><br>*       soft    memlock         unlimited<br>*       hard    memlock         unlimited<br><span class="hljs-comment"># rdma tuning end</span><br></code></pre></td></tr></table></figure>


<h2 id="2-2、部署集群"><a href="#2-2、部署集群" class="headerlink" title="2.2、部署集群"></a>2.2、部署集群</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 探测节点</span><br>ansible -i hosts.ini -m ping all<br><br><span class="hljs-comment"># 部署集群</span><br>ansible-playbook -vvvv -i hosts.ini site.yml<br><br><span class="hljs-comment"># 销毁集群</span><br>ansible-playbook -vvvv -i hosts.ini infrastructure-playbooks/purge-cluster.yml<br><br><span class="hljs-comment"># 查看进程</span><br>watch -n 1 <span class="hljs-string">&quot;ps xau | grep ceph; podman ps&quot;</span><br><br><span class="hljs-comment"># 查看进程和 ceph 状态</span><br>watch -n 1 <span class="hljs-string">&quot;ps xau | grep ceph; podman ps; ceph -s&quot;</span><br></code></pre></td></tr></table></figure>


<h2 id="2-3、客户端挂载使用"><a href="#2-3、客户端挂载使用" class="headerlink" title="2.3、客户端挂载使用"></a>2.3、客户端挂载使用</h2><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 挂载 cephfs</span><br><span class="hljs-built_in">mkdir</span> -p /mnt/cephfs<br>mount -t ceph 10.10.10.1:6789,10.10.10.1:6789,10.10.10.1:6789:/ /mnt/cephfs -o name=admin,secret=AQANl5VobL1iKxAAF49gUb79LeHCnsftT2rV+g==<br><span class="hljs-built_in">ls</span> -al /mnt/cephfs/<br><br><span class="hljs-comment"># 读写测试</span><br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero bs=1M count=1000 | pv -L 3M | <span class="hljs-built_in">dd</span> of=/mnt/cephfs/testfile oflag=direct status=progress<br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/mnt/cephfs/testfile bs=1M count=1000 iflag=direct | pv -L 1M | <span class="hljs-built_in">dd</span> of=/dev/null status=progress<br></code></pre></td></tr></table></figure>

<h1 id="三、使用-cephadm-部署"><a href="#三、使用-cephadm-部署" class="headerlink" title="三、使用 cephadm 部署"></a>三、使用 cephadm 部署</h1><blockquote>
<p><strong>注意:</strong> 由于 CentOS 8 已经没有 v19.x.x 版本的 cephadm 软件包供使用。但是我们通过在 CentOS 9 Stream 上编译打包 RPM 后提供 CentoS 8 使用。之后 cephadm 部署集群的时候使用官方最新的 Ceph 容器镜像，这里使用的是 Ceph v19.2.3 的容器镜像。</p>
</blockquote>
<p>本次测试环境的机器系统为 CentOS 8.5.2111 ，使用 Ceph <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph/tree/v19.2.3">v19.2.3</a> 版本进行测试。</p>
<h2 id="3-1、环境配置"><a href="#3-1、环境配置" class="headerlink" title="3.1、环境配置"></a>3.1、环境配置</h2><h3 id="3-1-1、配置-cephadm"><a href="#3-1-1、配置-cephadm" class="headerlink" title="3.1.1、配置 cephadm"></a>3.1.1、配置 cephadm</h3><p>在部署集群之前，我们需要指定一些集群配置以启用 RDMA 特性。这里仅介绍和 RDMA 有关的一些配置，其他的部署集群所需要的配置这里并没有介绍，仍需要你在部署集群前配置 ok 。</p>
<p><strong>&#x2F;root&#x2F;ceph.conf 配置文件内容如下:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs conf">[global]<br>ms_type = async+rdma<br>ms_cluster_type = async+rdma<br>ms_public_type = async+posix<br>ms_async_rdma_cm = false<br>ms_bind_ipv4 = true<br>ms_bind_ipv6 = false<br>ms_async_rdma_type = ib<br>ms_async_rdma_device_name = mlx5_bond_0<br>ms_async_rdma_port_num = 1<br>ms_async_rdma_gid_idx = 3<br></code></pre></td></tr></table></figure>

<p><strong>配置解析:</strong></p>
<ul>
<li><code>ms_type</code> : 消息传输类型，支持 async+posix , async+dpdk 和 async+rdma 三种类型，其中 async+posix 为默认的传输类型，其他两种是实验性的并且支持可能受限。</li>
<li><code>ms_cluster_type</code> : 集群内部消息传输类型，如果未指定默认为 <code>ms_type</code> 。</li>
<li><code>ms_public_type</code> : 集群内部消息传输类型，如果未指定默认为 <code>ms_type</code> 。</li>
<li><code>ms_async_rdma_cm</code> : 是否启用 RDMA CM 方式管理 RDMA 连接，默认为 false ，如果未启用则使用 Verbs 方式管理 RDMA 连接。<ul>
<li>该参数需要和 ms_async_rdma_type 配合使用，如果该参数为 true ，则 ms_async_rdma_type 需要要设置为 iwarp ，否则会出错。</li>
</ul>
</li>
<li><code>ms_async_rdma_type</code> : RDMA 实现协议类型，可选值为 iwarp 或 ib ，默认为 ib 。</li>
<li><code>ms_async_rdma_device_name</code> : RDMA 设备名称。</li>
<li><code>ms_async_rdma_port_num</code> :  RDMA 设备上的端口号。一块网络卡可能有多个端口，每个端口都能独立地进行网络通信。port_num 参数用于选择具体哪个端口用于 RDMA 通讯。</li>
<li><code>ms_async_rdma_gid_idx</code> : RDMA 设备全局标识符。用于在 InfiniBand 网络中唯一标识设备。gid_idx 是 GID 表中的索引，用于选择特定的 GID。这在配置 RoCE（RDMA over Converged Ethernet）连接时尤其重要，可以根据需求选择使用 RoCE v1 或 v2。</li>
</ul>
<h3 id="3-1-2、配置部署节点环境"><a href="#3-1-2、配置部署节点环境" class="headerlink" title="3.1.2、配置部署节点环境"></a>3.1.2、配置部署节点环境</h3><p>由于 RDMA 通信要求固定计算机的物理内存（也就是说，当整个计算机在可用内存上启动不足时，内核不允许将该内存交换到分页文件）。固定内存通常是非常特权的操作。为了允许 root 之外的用户运行大型 RDMA 应用程序，可能需要增加非 root 用户在系统中被允许的内存量。这可以通过在 <code>/etc/security/limits.d/</code> 目录中添加一个自定义配置文件来实现。参考配置文件内容: <a target="_blank" rel="noopener" href="https://enterprise-support.nvidia.com/s/article/bring-up-ceph-rdma---developer-s-guide">Bring Up Ceph RDMA - Developer’s Guide</a></p>
<p><strong>修改 Ceph 部署机器上的 &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;rdma.conf 配置:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> /etc/security/limits.d/rdma.conf<br><span class="hljs-comment"># configuration for rdma tuning</span><br>*       soft    memlock         unlimited<br>*       hard    memlock         unlimited<br><span class="hljs-comment"># rdma tuning end</span><br></code></pre></td></tr></table></figure>


<h2 id="3-2、部署集群"><a href="#3-2、部署集群" class="headerlink" title="3.2、部署集群"></a>3.2、部署集群</h2><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 部署测试</span><br>cephadm bootstrap --config /root/ceph.conf --mon-ip 10.10.10.1 --initial-dashboard-password admin --allow-fqdn-hostname --no-minimize-config<br><br><span class="hljs-comment"># 启用文件日志（可选）</span><br>ceph config <span class="hljs-built_in">set</span> global log_to_file <span class="hljs-literal">true</span><br>ceph config <span class="hljs-built_in">set</span> global mon_cluster_log_to_file <span class="hljs-literal">true</span><br>ceph config <span class="hljs-built_in">set</span> global log_to_stderr <span class="hljs-literal">false</span><br>ceph config <span class="hljs-built_in">set</span> global mon_cluster_log_to_stderr <span class="hljs-literal">false</span><br>ceph config <span class="hljs-built_in">set</span> global log_to_journald <span class="hljs-literal">false</span><br>ceph config <span class="hljs-built_in">set</span> global mon_cluster_log_to_journald <span class="hljs-literal">false</span><br><br><span class="hljs-comment"># 初始化环境配置：新主机安装集群 SSH 公钥</span><br>ssh-copy-id -f -i /etc/ceph/ceph.pub root@host02<br>ssh-copy-id -f -i /etc/ceph/ceph.pub root@host03<br><br><span class="hljs-comment"># 添加主机到集群</span><br>ceph orch host add host02 10.10.10.2<br>ceph orch host add host02 10.10.10.3<br><br><span class="hljs-comment"># 添加 OSD 存储</span><br><span class="hljs-comment"># 注意: 由于我们配置调整 /etc/systemd/system/ceph-&lt;cluster_id&gt;@.service 配置文件，这会导致 osd 启动失败，</span><br><span class="hljs-comment">#       不过没有关系，我们会在下面统一调整该配置文件，之后在重启 osd 组件即可。</span><br>ceph orch device <span class="hljs-built_in">ls</span><br>ceph orch daemon add osd host01:/dev/nvme0n1,/dev/nvme1n1,/dev/nvme2n1<br>ceph orch daemon add osd host02:/dev/nvme0n1,/dev/nvme1n1,/dev/nvme2n1<br>ceph orch daemon add osd host03:/dev/nvme0n1,/dev/nvme1n1,/dev/nvme2n1<br><br><span class="hljs-comment"># 传输 ceph 集群和密钥配置文件（可选）</span><br>scp /etc/ceph/ceph.conf host02:/etc/ceph/<br>scp /etc/ceph/ceph.conf host02:/etc/ceph/<br>scp /etc/ceph/ceph.client.admin.keyring host02:/etc/ceph/<br>scp /etc/ceph/ceph.client.admin.keyring host03:/etc/ceph/<br><br><span class="hljs-comment"># 创建文件系统</span><br>ceph fs volume create cephfs<br><br><span class="hljs-comment"># 调整文件系统数据池副本为 2 （可选）</span><br>ceph osd pool <span class="hljs-built_in">ls</span> detail<br>ceph osd pool <span class="hljs-built_in">set</span> cephfs.cephfs.meta min_size 1<br>ceph osd pool <span class="hljs-built_in">set</span> cephfs.cephfs.meta size 2 --yes-i-really-mean-it<br>ceph osd pool <span class="hljs-built_in">set</span> cephfs.cephfs.data min_size 1<br>ceph osd pool <span class="hljs-built_in">set</span> cephfs.cephfs.data size 2 --yes-i-really-mean-it<br><br><span class="hljs-comment"># 查看集群配置</span><br>ceph tell osd.* config get ms_type<br>ceph tell osd.* config get ms_cluster_type<br>ceph tell osd.* config get ms_public_type<br>ceph tell osd.* config get ms_async_rdma_cm<br><br><span class="hljs-comment"># 销毁集群</span><br>cephadm rm-cluster --force --zap-osds --fsid b07cea80-741f-11f0-a76e-946dae8f5dda<br></code></pre></td></tr></table></figure>


<h2 id="3-3、集群配置调整"><a href="#3-3、集群配置调整" class="headerlink" title="3.3、集群配置调整"></a>3.3、集群配置调整</h2><blockquote>
<p><strong>注意:</strong> 当每次执行 <code>ceph orch daemon add *</code> 操作的时候，都会重新更新对应机器上的 <code>/etc/systemd/system/ceph-&lt;cluster_id&gt;@.service</code> 配置文件，我们可以从 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph/blob/v19.2.3/src/cephadm/cephadmlib/systemd_unit.py#L137">src&#x2F;cephadm&#x2F;cephadmlib&#x2F;systemd_unit.py</a> 和 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph/blob/v19.2.3/src/cephadm/cephadmlib/templating.py">src&#x2F;cephadm&#x2F;cephadmlib&#x2F;templating.py</a> 中找到相关的实现。由于这些模板会被打包成一个 zipapp 文件（即 cephadm 可执行文件），因此我们无法在执行 cephadm 的时候修改本地的一些文件来尝试在远程机器的 <code>/etc/systemd/system/ceph-&lt;cluster_id&gt;@.service</code> 文件中应用新的配置。</p>
</blockquote>
<p><strong>修改 &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ceph-<cluster_id>@.service 配置文件中的内容:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[Service]<br>LimitMEMLOCK=infinity<br>PrivateDevices=no<br></code></pre></td></tr></table></figure>

<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查询所有 ceph 相关的 systemd 文件的位置</span><br>systemctl show -p FragmentPath ceph-*<br><br><span class="hljs-comment"># 重新加载 systemd 的服务配置文件</span><br>systemctl daemon-reload<br><br><span class="hljs-comment"># 获取集群id</span><br>fsid=$(cephadm shell -- ceph fsid)<br><br><span class="hljs-comment"># 重启 osd 服务</span><br><span class="hljs-comment"># 需要在每个部署 osd 的节点上执行</span><br><span class="hljs-built_in">ls</span> /var/lib/ceph/<span class="hljs-variable">$fsid</span>/ | grep osd | <span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> <span class="hljs-built_in">dir</span>; <span class="hljs-keyword">do</span> systemctl restart ceph-<span class="hljs-variable">$fsid</span>@<span class="hljs-variable">$dir</span>.service; <span class="hljs-keyword">done</span><br><br><br><span class="hljs-comment"># 检查 cephadm 部分内容</span><br>[root@host01 data]<span class="hljs-comment"># unzip -l /usr/sbin/cephadm | grep templates</span><br>warning [/usr/sbin/cephadm]:  2 extra bytes at beginning or within zipfile<br>  (attempting to process anyway)<br>        0  02-01-2024 07:14   cephadmlib/templates/<br>     1488  02-01-2024 07:14   cephadmlib/templates/init_containers.run.j2<br>      205  02-01-2024 07:14   cephadmlib/templates/dropin.service.j2<br>     1264  02-01-2024 07:14   cephadmlib/templates/init_ctr.service.j2<br>      133  02-01-2024 07:14   cephadmlib/templates/cephadm.logrotate.config.j2<br>      508  02-01-2024 07:14   cephadmlib/templates/sidecar.run.j2<br>     1031  02-01-2024 07:14   cephadmlib/templates/sidecar.service.j2<br>     1198  02-01-2024 07:14   cephadmlib/templates/ceph.service.j2<br>      307  02-01-2024 07:14   cephadmlib/templates/agent.service.j2<br>      280  02-01-2024 07:14   cephadmlib/templates/cluster.logrotate.config.j2<br><br><span class="hljs-comment"># 查看 cephadm 部分内容</span><br><span class="hljs-built_in">mkdir</span> ./cephadm_templates<br>unzip /usr/sbin/cephadm <span class="hljs-string">&#x27;cephadmlib/templates/*&#x27;</span> -d ./cephadm_templates<br><span class="hljs-built_in">cat</span> ./cephadm_templates/cephadmlib/templates/ceph.service.j2<br></code></pre></td></tr></table></figure>


<h2 id="3-4、客户端挂载使用"><a href="#3-4、客户端挂载使用" class="headerlink" title="3.4、客户端挂载使用"></a>3.4、客户端挂载使用</h2><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 挂载 cephfs</span><br><span class="hljs-built_in">mkdir</span> -p /mnt/cephfs<br>mount -t ceph 10.10.10.1:6789,10.10.10.1:6789,10.10.10.1:6789:/ /mnt/cephfs -o name=admin,secret=AQANl5VobL1iKxAAF49gUb79LeHCnsftT2rV+g==<br><span class="hljs-built_in">ls</span> -al /mnt/cephfs/<br><br><span class="hljs-comment"># 读写测试</span><br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero bs=1M count=1000 | pv -L 3M | <span class="hljs-built_in">dd</span> of=/mnt/cephfs/testfile oflag=direct status=progress<br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/mnt/cephfs/testfile bs=1M count=1000 iflag=direct | pv -L 1M | <span class="hljs-built_in">dd</span> of=/dev/null status=progress<br></code></pre></td></tr></table></figure>



<h1 id="四、相关问题"><a href="#四、相关问题" class="headerlink" title="四、相关问题"></a>四、相关问题</h1><h2 id="4-1、Infiniband-to-dead-failed-to-send-a-beacon-错误"><a href="#4-1、Infiniband-to-dead-failed-to-send-a-beacon-错误" class="headerlink" title="4.1、Infiniband to_dead failed to send a beacon 错误"></a>4.1、Infiniband to_dead failed to send a beacon 错误</h2><p><strong>问题记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">2024-08-09T13:34:49.108+0800 7fcdd4ea8700 -1 Infiniband to_dead failed to send a beacon: (115) Operation now <span class="hljs-keyword">in</span> progress<br>2024-08-09T13:34:50.702+0800 7fcdd56a9700 -1 Infiniband to_dead failed to send a beacon: (115) Operation now <span class="hljs-keyword">in</span> progress<br>2024-08-09T13:34:50.703+0800 7fcdd5eaa700 -1 Infiniband to_dead failed to send a beacon: (115) Operation now <span class="hljs-keyword">in</span> progress<br>2024-08-09T13:34:50.703+0800 7fcdd4ea8700 -1 Infiniband to_dead failed to send a beacon: (115) Operation now <span class="hljs-keyword">in</span> progress<br>2024-08-09T13:34:50.905+0800 7fcdd5eaa700 -1 Infiniband to_dead failed to send a beacon: (115) Operation now <span class="hljs-keyword">in</span> progress<br><br><br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 EpollDriver.del_event del event fd=41 cur_mask=3 delmask=3 to 27<br>2024-08-09T14:06:56.631+0800 7fbe29599700 10 Infiniband send_cm_meta sending: 0, 13049, 2116118, 0, 00000000000000000000ffff0a5a1833<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20  RDMAConnectedSocketImpl try_connect tcp_fd: 43<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 Event(0x55df39caa300 nevent=5000 time_id=2).create_file_event create event started fd=43 mask=3 original mask is 0<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 EpollDriver.add_event add event fd=43 cur_mask=0 add_mask=3 to 24<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 Event(0x55df39caa300 nevent=5000 time_id=2).create_file_event create event end fd=43 mask=3 current mask is 3<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 Event(0x55df39caa300 nevent=5000 time_id=2).create_file_event create event started fd=39 mask=1 original mask is 0<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 EpollDriver.add_event add event fd=39 cur_mask=0 add_mask=1 to 24<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 Event(0x55df39caa300 nevent=5000 time_id=2).create_file_event create event end fd=39 mask=1 current mask is 1<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 Event(0x55df39caa800 nevent=5000 time_id=2).create_file_event create event started fd=41 mask=1 original mask is 0<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 EpollDriver.add_event add event fd=41 cur_mask=0 add_mask=1 to 27<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 Event(0x55df39caa800 nevent=5000 time_id=2).create_file_event create event end fd=41 mask=1 current mask is 1<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20  RDMAConnectedSocketImpl handle_connection_established finish<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 10 --  &gt;&gt; [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] conn(0x55df3909ac00 msgr2=0x55df39009e00 unknown :-1 s=STATE_CONNECTING_RE l=0).process nonblock connect inprogress<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20  RDMAConnectedSocketImpl handle_connection_established start<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 EpollDriver.del_event del event fd=42 cur_mask=3 delmask=3 to 21<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 10 Infiniband send_cm_meta sending: 0, 13050, 5515815, 0, 00000000000000000000ffff0a5a1833<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20  RDMAConnectedSocketImpl handle_connection_established start<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 EpollDriver.del_event del event fd=43 cur_mask=3 delmask=3 to 24<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 10 Infiniband send_cm_meta sending: 0, 13048, 0, 0, 00000000000000000000ffff0a5a1833<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 Event(0x55df39caa080 nevent=5000 time_id=2).create_file_event create event started fd=42 mask=1 original mask is 0<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 EpollDriver.add_event add event fd=42 cur_mask=0 add_mask=1 to 21<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 Event(0x55df39caa080 nevent=5000 time_id=2).create_file_event create event end fd=42 mask=1 current mask is 1<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20  RDMAConnectedSocketImpl handle_connection QP: 13049 tcp_fd: 41 notify_fd: 38<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20  RDMAConnectedSocketImpl handle_connection_established finish<br>2024-08-09T14:06:56.631+0800 7fbe29599700  1 Infiniband recv_cm_meta got bad length (26)<br>2024-08-09T14:06:56.631+0800 7fbe29599700  1  RDMAConnectedSocketImpl handle_connection recv handshake msg failed.<br>2024-08-09T14:06:56.631+0800 7fbe29599700  1  RDMAConnectedSocketImpl fault tcp fd 41<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 Event(0x55df39caa300 nevent=5000 time_id=2).create_file_event create event started fd=43 mask=1 original mask is 0<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 EpollDriver.add_event add event fd=43 cur_mask=0 add_mask=1 to 24<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 Event(0x55df39caa300 nevent=5000 time_id=2).create_file_event create event end fd=43 mask=1 current mask is 1<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20  RDMAConnectedSocketImpl handle_connection_established finish<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 --  &gt;&gt; [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] conn(0x55df3909a800 msgr2=0x55df3900a300 unknown :-1 s=STATE_CONNECTING_RE l=0).process<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 EpollDriver.del_event del event fd=38 cur_mask=1 delmask=2 to 27<br>2024-08-09T14:06:56.631+0800 7fbe29599700 10 --  &gt;&gt; [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] conn(0x55df3909a800 msgr2=0x55df3900a300 unknown :-1 s=STATE_CONNECTING_RE l=0).process connect successfully, ready to send banner<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 --2-  &gt;&gt; [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] conn(0x55df3909a800 0x55df3900a300 unknown :-1 s=START_CONNECT pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0).read_event<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 --2-  &gt;&gt; [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] conn(0x55df3909a800 0x55df3900a300 unknown :-1 s=START_CONNECT pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0).start_client_banner_exchange<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 --2-  &gt;&gt; [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] conn(0x55df3909a800 0x55df3900a300 unknown :-1 s=BANNER_CONNECTING pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0)._banner_exchange<br>2024-08-09T14:06:56.631+0800 7fbe29599700  1 --  &gt;&gt; [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] conn(0x55df3909a800 msgr2=0x55df3900a300 unknown :-1 s=STATE_CONNECTION_ESTABLISHED l=0)._try_send send error: (32) Broken pipe<br>2024-08-09T14:06:56.631+0800 7fbe29599700  1 --2-  &gt;&gt; [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] conn(0x55df3909a800 0x55df3900a300 unknown :-1 s=BANNER_CONNECTING pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0).write banner write failed r=-32 ((32) Broken pipe)<br>2024-08-09T14:06:56.631+0800 7fbe29599700 10 --2-  &gt;&gt; [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] conn(0x55df3909a800 0x55df3900a300 unknown :-1 s=BANNER_CONNECTING pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0)._fault<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 EpollDriver.del_event del event fd=38 cur_mask=1 delmask=3 to 27<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20  RDMAConnectedSocketImpl ~RDMAConnectedSocketImpl destruct.<br>2024-08-09T14:06:56.631+0800 7fbe29599700 20 EpollDriver.del_event del event fd=41 cur_mask=1 delmask=3 to 27<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20  RDMAConnectedSocketImpl handle_connection QP: 13050 tcp_fd: 42 notify_fd: 40<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700  1 Infiniband recv_cm_meta got bad length (26)<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700  1  RDMAConnectedSocketImpl handle_connection recv handshake msg failed.<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700  1  RDMAConnectedSocketImpl fault tcp fd 42<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20  RDMAConnectedSocketImpl handle_connection QP: 13048 tcp_fd: 43 notify_fd: 39<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700  1 Infiniband recv_cm_meta got bad length (26)<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700  1  RDMAConnectedSocketImpl handle_connection recv handshake msg failed.<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 --  &gt;&gt; [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] conn(0x55df3909a400 msgr2=0x55df3900a800 unknown :-1 s=STATE_CONNECTING_RE l=0).process<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700  1  RDMAConnectedSocketImpl fault tcp fd 43<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 EpollDriver.del_event del event fd=40 cur_mask=1 delmask=2 to 21<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 10 --  &gt;&gt; [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] conn(0x55df3909a400 msgr2=0x55df3900a800 unknown :-1 s=STATE_CONNECTING_RE l=0).process connect successfully, ready to send banner<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20  RDMAConnectedSocketImpl handle_connection QP: 13048 tcp_fd: 43 notify_fd: 39<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 10 Infiniband recv_cm_meta got disconnect message<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700  1  RDMAConnectedSocketImpl handle_connection recv handshake msg failed.<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 --2-  &gt;&gt; [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] conn(0x55df3909a400 0x55df3900a800 unknown :-1 s=START_CONNECT pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0).read_event<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700  1  RDMAConnectedSocketImpl fault tcp fd 43<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 --2-  &gt;&gt; [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] conn(0x55df3909a400 0x55df3900a800 unknown :-1 s=START_CONNECT pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0).start_client_banner_exchange<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 --  &gt;&gt; [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] conn(0x55df3909ac00 msgr2=0x55df39009e00 unknown :-1 s=STATE_CONNECTING_RE l=0).process<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 EpollDriver.del_event del event fd=39 cur_mask=1 delmask=2 to 24<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 --2-  &gt;&gt; [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] conn(0x55df3909a400 0x55df3900a800 unknown :-1 s=BANNER_CONNECTING pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0)._banner_exchange<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 10 --  &gt;&gt; [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] conn(0x55df3909ac00 msgr2=0x55df39009e00 unknown :-1 s=STATE_CONNECTING_RE l=0).process connect successfully, ready to send banner<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 --2-  &gt;&gt; [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] conn(0x55df3909ac00 0x55df39009e00 unknown :-1 s=START_CONNECT pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0).read_event<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 --2-  &gt;&gt; [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] conn(0x55df3909ac00 0x55df39009e00 unknown :-1 s=START_CONNECT pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0).start_client_banner_exchange<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 20 --2-  &gt;&gt; [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] conn(0x55df3909ac00 0x55df39009e00 unknown :-1 s=BANNER_CONNECTING pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0)._banner_exchange<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700  1 --  &gt;&gt; [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] conn(0x55df3909a400 msgr2=0x55df3900a800 unknown :-1 s=STATE_CONNECTION_ESTABLISHED l=0)._try_send send error: (32) Broken pipe<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700  1 --2-  &gt;&gt; [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] conn(0x55df3909a400 0x55df3900a800 unknown :-1 s=BANNER_CONNECTING pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0).write banner write failed r=-32 ((32) Broken pipe)<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 10 --2-  &gt;&gt; [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] conn(0x55df3909a400 0x55df3900a800 unknown :-1 s=BANNER_CONNECTING pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0)._fault<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 EpollDriver.del_event del event fd=40 cur_mask=1 delmask=3 to 21<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700  1 --  &gt;&gt; [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] conn(0x55df3909ac00 msgr2=0x55df39009e00 unknown :-1 s=STATE_CONNECTION_ESTABLISHED l=0)._try_send send error: (32) Broken pipe<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20  RDMAConnectedSocketImpl ~RDMAConnectedSocketImpl destruct.<br>2024-08-09T14:06:56.631+0800 7fbe2a59b700 20 EpollDriver.del_event del event fd=42 cur_mask=1 delmask=3 to 21<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700  1 --2-  &gt;&gt; [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] conn(0x55df3909ac00 0x55df39009e00 unknown :-1 s=BANNER_CONNECTING pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0).write banner write failed r=-32 ((32) Broken pipe)<br>2024-08-09T14:06:56.631+0800 7fbe29d9a700 10 --2-  &gt;&gt; [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] conn(0x55df3909ac00 0x55df39009e00 unknown :-1 s=BANNER_CONNECTING pgs=0 cs=0 l=0 rev1=0 rx=0 tx=0)._fault<br></code></pre></td></tr></table></figure>


<h2 id="4-2、未配置-LimitMEMLOCK-和-PrivateDevices-报错"><a href="#4-2、未配置-LimitMEMLOCK-和-PrivateDevices-报错" class="headerlink" title="4.2、未配置 LimitMEMLOCK 和 PrivateDevices 报错"></a>4.2、未配置 LimitMEMLOCK 和 PrivateDevices 报错</h2><p>该问题由于未修改 osd 启动进程的 <code>/etc/systemd/system/ceph-&lt;cluster_id&gt;@.service</code> 配置文件，导致启动 osd 启动报错，按照上面规则修改后可解决该问题。</p>
<p><strong>问题记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB <span class="hljs-built_in">read</span>, 0.00 MB/s <span class="hljs-built_in">read</span>, 0.0 seconds<br>Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB <span class="hljs-built_in">read</span>, 0.00 MB/s <span class="hljs-built_in">read</span>, 0.0 seconds<br>Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop <span class="hljs-keyword">for</span> pending_compaction_bytes, 0 slowdown <span class="hljs-keyword">for</span> pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count<br>Block cache BinnedLRUCache@0x55bcc84eb350<span class="hljs-comment">#7 capacity: 1.35 GB usage: 2.47 KB table_size: 0 occupancy: 18446744073709551615 collections: 1 last_copies: 8 last_secs: 3.2e-05 secs_since: 0</span><br>Block cache entry stats(count,size,portion): FilterBlock(11,1.20 KB,8.49918e-05%) IndexBlock(11,1.27 KB,8.9407e-05%) Misc(1,0.00 KB,0%)<br><br>** File Read Latency Histogram By Level [P] **<br><br>2025-08-10T06:56:02.560+0000 7fd21006b740  0 &lt;cls&gt; /home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos9/DIST/centos9/MACHINE_SIZE/gigantic/release/19.2.3/rpm/el9/BUILD/ceph-19.2.3/src/cls/hello/cls_hello.cc:316: loadi<br>ng cls_hello<br>2025-08-10T06:56:02.561+0000 7fd21006b740  0 &lt;cls&gt; /home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos9/DIST/centos9/MACHINE_SIZE/gigantic/release/19.2.3/rpm/el9/BUILD/ceph-19.2.3/src/cls/cephfs/cls_cephfs.cc:201: loa<br>ding cephfs<br>2025-08-10T06:56:02.562+0000 7fd21006b740  0 _get_class not permitted to load sdk<br>2025-08-10T06:56:02.565+0000 7fd21006b740  0 _get_class not permitted to load lua<br>2025-08-10T06:56:02.565+0000 7fd21006b740  0 osd.3 0 crush map has features 288232575208783872, adjusting msgr requires <span class="hljs-keyword">for</span> clients<br>2025-08-10T06:56:02.565+0000 7fd21006b740  0 osd.3 0 crush map has features 288232575208783872 was 8705, adjusting msgr requires <span class="hljs-keyword">for</span> mons<br>2025-08-10T06:56:02.565+0000 7fd21006b740  0 osd.3 0 crush map has features 288232575208783872, adjusting msgr requires <span class="hljs-keyword">for</span> osds<br>2025-08-10T06:56:02.565+0000 7fd21006b740  0 osd.3 0 load_pgs<br>2025-08-10T06:56:02.565+0000 7fd21006b740  0 osd.3 0 load_pgs opened 0 pgs<br>2025-08-10T06:56:02.565+0000 7fd21006b740 -1 osd.3 0 log_to_monitors <span class="hljs-literal">true</span><br>2025-08-10T06:56:02.574+0000 7fd20d002640 -1 /home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos9/DIST/centos9/MACHINE_SIZE/gigantic/release/19.2.3/rpm/el9/BUILD/ceph-19.2.3/src/msg/async/rdma/Infiniband.cc: In functi<br>on <span class="hljs-string">&#x27;int Infiniband::MemoryManager::Cluster::fill(uint32_t)&#x27;</span> thread 7fd20d002640 time 2025-08-10T06:56:02.571743+0000<br>/home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos9/DIST/centos9/MACHINE_SIZE/gigantic/release/19.2.3/rpm/el9/BUILD/ceph-19.2.3/src/msg/async/rdma/Infiniband.cc: 783: FAILED ceph_assert(m)<br><br> ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable)<br> 1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x113) [0x55bcc517f85d]<br> 2: /usr/bin/ceph-osd(+0x401a14) [0x55bcc517fa14]<br> 3: /usr/bin/ceph-osd(+0x45669a) [0x55bcc51d469a]<br> 4: (Infiniband::init()+0x2fb) [0x55bcc5b6428b]<br> 5: (RDMAWorker::listen(entity_addr_t&amp;, unsigned int, SocketOptions const&amp;, ServerSocket*)+0x2d) [0x55bcc59d652d]<br> 6: /usr/bin/ceph-osd(+0xc24ccd) [0x55bcc59a2ccd]<br> 7: (EventCenter::process_events(unsigned int, std::chrono::duration&lt;unsigned long, std::ratio&lt;1l, 1000000000l&gt; &gt;*)+0x75d) [0x55bcc59d0d1d]<br> 8: /usr/bin/ceph-osd(+0xc53086) [0x55bcc59d1086]<br> 9: /lib64/libstdc++.so.6(+0xdbae4) [0x7fd21087fae4]<br> 10: /lib64/libc.so.6(+0x8a4da) [0x7fd21052f4da]<br> 11: <span class="hljs-built_in">clone</span>()<br><br>2025-08-10T06:56:02.588+0000 7fd20d002640 -1 *** Caught signal (Aborted) **<br> <span class="hljs-keyword">in</span> thread 7fd20d002640 thread_name:msgr-worker-0<br><br> ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable)<br> 1: /lib64/libc.so.6(+0x3ebf0) [0x7fd2104e3bf0]<br> 2: /lib64/libc.so.6(+0x8c21c) [0x7fd21053121c]<br> 3: raise()<br> 4: abort()<br> 5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x169) [0x55bcc517f8b3]<br> 6: /usr/bin/ceph-osd(+0x401a14) [0x55bcc517fa14]<br> 7: /usr/bin/ceph-osd(+0x45669a) [0x55bcc51d469a]<br> 8: (Infiniband::init()+0x2fb) [0x55bcc5b6428b]<br> 9: (RDMAWorker::listen(entity_addr_t&amp;, unsigned int, SocketOptions const&amp;, ServerSocket*)+0x2d) [0x55bcc59d652d]<br> 10: /usr/bin/ceph-osd(+0xc24ccd) [0x55bcc59a2ccd]<br> 11: (EventCenter::process_events(unsigned int, std::chrono::duration&lt;unsigned long, std::ratio&lt;1l, 1000000000l&gt; &gt;*)+0x75d) [0x55bcc59d0d1d]<br> 12: /usr/bin/ceph-osd(+0xc53086) [0x55bcc59d1086]<br> 13: /lib64/libstdc++.so.6(+0xdbae4) [0x7fd21087fae4]<br> 14: /lib64/libc.so.6(+0x8a4da) [0x7fd21052f4da]<br> 15: <span class="hljs-built_in">clone</span>()<br> NOTE: a copy of the executable, or `objdump -rdS &lt;executable&gt;` is needed to interpret this.<br><br>--- begin dump of recent events ---<br> -2747&gt; 2025-08-10T06:55:56.424+0000 7fd21006b740  5 asok(0x55bcc8524000) register_command assert hook 0x55bcc845ace0<br> -2746&gt; 2025-08-10T06:55:56.424+0000 7fd21006b740  5 asok(0x55bcc8524000) register_command abort hook 0x55bcc845ace0<br> -2745&gt; 2025-08-10T06:55:56.424+0000 7fd21006b740  5 asok(0x55bcc8524000) register_command leak_some_memory hook 0x55bcc845ace0<br> -2744&gt; 2025-08-10T06:55:56.424+0000 7fd21006b740  5 asok(0x55bcc8524000) register_command perfcounters_dump hook 0x55bcc845ace0<br> -2743&gt; 2025-08-10T06:55:56.424+0000 7fd21006b740  5 asok(0x55bcc8524000) register_command 1 hook 0x55bcc845ace0<br></code></pre></td></tr></table></figure>


<h2 id="4-3、osd-进程异常"><a href="#4-3、osd-进程异常" class="headerlink" title="4.3、osd 进程异常"></a>4.3、osd 进程异常</h2><p>当通过 cephadm 完成集群部署，配置变更后，仍出现该问题，目前问题不明。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">2025-08-10T07:02:24.266+0000 7f8177a94740  0 &lt;cls&gt; /home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos9/DIST/centos9/MACHINE_SIZE/gigantic/release/19.2.3/rpm/el9/BUILD/ceph-19.2.3/src/cls/cephfs/cls_cephfs.cc:201: loa<br>ding cephfs<br>2025-08-10T07:02:24.267+0000 7f8177a94740  0 _get_class not permitted to load sdk<br>2025-08-10T07:02:24.270+0000 7f8177a94740  0 _get_class not permitted to load lua<br>2025-08-10T07:02:24.270+0000 7f8177a94740  0 osd.4 54 crush map has features 288514051259236352, adjusting msgr requires <span class="hljs-keyword">for</span> clients<br>2025-08-10T07:02:24.271+0000 7f8177a94740  0 osd.4 54 crush map has features 288514051259236352 was 8705, adjusting msgr requires <span class="hljs-keyword">for</span> mons<br>2025-08-10T07:02:24.271+0000 7f8177a94740  0 osd.4 54 crush map has features 3314933000852226048, adjusting msgr requires <span class="hljs-keyword">for</span> osds<br>2025-08-10T07:02:24.271+0000 7f8177a94740  1 osd.4 54 check_osdmap_features require_osd_release unknown -&gt; squid<br>2025-08-10T07:02:24.271+0000 7f8177a94740  0 osd.4 54 load_pgs<br>2025-08-10T07:02:24.271+0000 7f8177a94740  0 osd.4 54 load_pgs opened 0 pgs<br>2025-08-10T07:02:24.271+0000 7f8177a94740 -1 osd.4 54 log_to_monitors <span class="hljs-literal">true</span><br>2025-08-10T07:02:24.634+0000 7f8177a94740  1 bluestore(/var/lib/ceph/osd/ceph-4) collect_metadata devices span numa nodes 0<br>2025-08-10T07:02:25.468+0000 7f8177a94740  0 osd.4 54 <span class="hljs-keyword">done</span> with init, starting boot process<br>2025-08-10T07:02:25.468+0000 7f8177a94740  1 osd.4 54 start_boot<br>2025-08-10T07:02:25.469+0000 7f8177a94740  1 osd.4 54 maybe_override_options_for_qos osd_max_backfills <span class="hljs-built_in">set</span> to 1<br>2025-08-10T07:02:25.469+0000 7f8177a94740  1 osd.4 54 maybe_override_options_for_qos osd_recovery_max_active <span class="hljs-built_in">set</span> to 0<br>2025-08-10T07:02:25.469+0000 7f8177a94740  1 osd.4 54 maybe_override_options_for_qos osd_recovery_max_active_hdd <span class="hljs-built_in">set</span> to 3<br>2025-08-10T07:02:25.469+0000 7f8177a94740  1 osd.4 54 maybe_override_options_for_qos osd_recovery_max_active_ssd <span class="hljs-built_in">set</span> to 10<br>2025-08-10T07:02:25.469+0000 7f8177a94740  1 osd.4 54 maybe_override_max_osd_capacity_for_qos default_iops: 21500.00 cur_iops: 26142.86. Skip OSD benchmark <span class="hljs-built_in">test</span>.<br>2025-08-10T07:02:25.474+0000 7f816a035640  1 osd.4 54 set_numa_affinity storage numa node 0<br>2025-08-10T07:02:25.474+0000 7f816a035640 -1 osd.4 54 set_numa_affinity unable to identify public interface <span class="hljs-string">&#x27;&#x27;</span> numa node: (2) No such file or directory<br>2025-08-10T07:02:25.474+0000 7f816a035640  1 osd.4 54 set_numa_affinity not setting numa affinity<br>2025-08-10T07:02:25.474+0000 7f816a035640  1 bluestore(/var/lib/ceph/osd/ceph-4) collect_metadata devices span numa nodes 0<br>2025-08-10T07:02:25.474+0000 7f816a035640  1 bluestore(/var/lib/ceph/osd/ceph-4) collect_metadata devices span numa nodes 0<br>2025-08-10T07:02:25.680+0000 7f816c039640  1 osd.4 54 tick checking mon <span class="hljs-keyword">for</span> new map<br>2025-08-10T07:02:26.468+0000 7f8161e13640  1 osd.4 61 state: booting -&gt; active<br>2025-08-10T07:02:27.167+0000 7f8174a2b640 -1 Infiniband to_dead failed to send a beacon: (11) Resource temporarily unavailable<br>2025-08-10T07:02:33.507+0000 7f8173a17640 -1 Infiniband modify_qp_to_rtr failed to transition to RTR state: (22) Invalid argument<br>2025-08-10T07:02:33.509+0000 7f8173a17640 -1 /home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos9/DIST/centos9/MACHINE_SIZE/gigantic/release/19.2.3/rpm/el9/BUILD/ceph-19.2.3/src/msg/async/rdma/RDMAConnectedSocketImpl.<br>cc: In <span class="hljs-keyword">function</span> <span class="hljs-string">&#x27;void RDMAConnectedSocketImpl::handle_connection()&#x27;</span> thread 7f8173a17640 time 2025-08-10T07:02:33.508351+0000<br>/home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos9/DIST/centos9/MACHINE_SIZE/gigantic/release/19.2.3/rpm/el9/BUILD/ceph-19.2.3/src/msg/async/rdma/RDMAConnectedSocketImpl.cc: 231: FAILED ceph_assert(!r)<br><br> ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable)<br> 1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x113) [0x5627245fc85d]<br> 2: /usr/bin/ceph-osd(+0x401a14) [0x5627245fca14]<br> 3: /usr/bin/ceph-osd(+0x45b95a) [0x56272465695a]<br> 4: (EventCenter::process_events(unsigned int, std::chrono::duration&lt;unsigned long, std::ratio&lt;1l, 1000000000l&gt; &gt;*)+0x1cd) [0x562724e4d78d]<br> 5: /usr/bin/ceph-osd(+0xc53086) [0x562724e4e086]<br> 6: /lib64/libstdc++.so.6(+0xdbae4) [0x7f81782a8ae4]<br> 7: /lib64/libc.so.6(+0x8a4da) [0x7f8177f584da]<br> 8: <span class="hljs-built_in">clone</span>()<br><br>2025-08-10T07:02:33.512+0000 7f8173a17640 -1 *** Caught signal (Aborted) **<br> <span class="hljs-keyword">in</span> thread 7f8173a17640 thread_name:msgr-worker-2<br><br> ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable)<br> 1: /lib64/libc.so.6(+0x3ebf0) [0x7f8177f0cbf0]<br> 2: /lib64/libc.so.6(+0x8c21c) [0x7f8177f5a21c]<br> 3: raise()<br> 4: abort()<br> 5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x169) [0x5627245fc8b3]<br> 6: /usr/bin/ceph-osd(+0x401a14) [0x5627245fca14]<br> 7: /usr/bin/ceph-osd(+0x45b95a) [0x56272465695a]<br> 8: (EventCenter::process_events(unsigned int, std::chrono::duration&lt;unsigned long, std::ratio&lt;1l, 1000000000l&gt; &gt;*)+0x1cd) [0x562724e4d78d]<br> 9: /usr/bin/ceph-osd(+0xc53086) [0x562724e4e086]<br> 10: /lib64/libstdc++.so.6(+0xdbae4) [0x7f81782a8ae4]<br> 11: /lib64/libc.so.6(+0x8a4da) [0x7f8177f584da]<br> 12: <span class="hljs-built_in">clone</span>()<br> NOTE: a copy of the executable, or `objdump -rdS &lt;executable&gt;` is needed to interpret this.<br><br>--- begin dump of recent events ---<br> -2984&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command assert hook 0x562727300ce0<br> -2983&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command abort hook 0x562727300ce0<br> -2982&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command leak_some_memory hook 0x562727300ce0<br> -2981&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command perfcounters_dump hook 0x562727300ce0<br> -2980&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command 1 hook 0x562727300ce0<br> -2979&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command perf dump hook 0x562727300ce0<br> -2978&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command perfcounters_schema hook 0x562727300ce0<br> -2977&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command perf histogram dump hook 0x562727300ce0<br> -2976&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command 2 hook 0x562727300ce0<br> -2975&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command perf schema hook 0x562727300ce0<br> -2974&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command counter dump hook 0x562727300ce0<br> -2973&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command counter schema hook 0x562727300ce0<br> -2972&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command perf histogram schema hook 0x562727300ce0<br> -2971&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command perf reset hook 0x562727300ce0<br> -2970&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command config show hook 0x562727300ce0<br> -2969&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command config <span class="hljs-built_in">help</span> hook 0x562727300ce0<br> -2968&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command config <span class="hljs-built_in">set</span> hook 0x562727300ce0<br> -2967&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command config <span class="hljs-built_in">unset</span> hook 0x562727300ce0<br> -2966&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command config get hook 0x562727300ce0<br> -2965&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command config diff hook 0x562727300ce0<br> -2964&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command config diff get hook 0x562727300ce0<br> -2963&gt; 2025-08-10T07:02:19.690+0000 7f8177a94740  5 asok(0x5627273ca000) register_command injectargs hook 0x562727300ce0<br></code></pre></td></tr></table></figure>

<h1 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h1><ul>
<li><a target="_blank" rel="noopener" href="https://enterprise-support.nvidia.com/s/article/bring-up-ceph-rdma---developer-s-guide">https://enterprise-support.nvidia.com/s/article/bring-up-ceph-rdma---developer-s-guide</a></li>
<li><a target="_blank" rel="noopener" href="https://jianyue.tech/posts/devops-ceph-rdma/">https://jianyue.tech/posts/devops-ceph-rdma/</a></li>
<li><a target="_blank" rel="noopener" href="https://forum.proxmox.com/threads/ceph-in-pve-7-3-can-not-working-with-rdma-roce.132637/">https://forum.proxmox.com/threads/ceph-in-pve-7-3-can-not-working-with-rdma-roce.132637/</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/network-config-ref/#confval-ms_type">https://docs.ceph.com/en/latest/rados/configuration/network-config-ref/#confval-ms_type</a></li>
<li><a target="_blank" rel="noopener" href="https://www.spinics.net/lists/ceph-users/msg69925.html">https://www.spinics.net/lists/ceph-users/msg69925.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.spinics.net/lists/ceph-users/msg69930.html">https://www.spinics.net/lists/ceph-users/msg69930.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.spinics.net/lists/ceph-users/msg75006.html">https://www.spinics.net/lists/ceph-users/msg75006.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.spinics.net/lists/ceph-users/msg75014.html">https://www.spinics.net/lists/ceph-users/msg75014.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.spinics.net/lists/ceph-users/msg75034.html">https://www.spinics.net/lists/ceph-users/msg75034.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.spinics.net/lists/ceph-users/msg75180.html">https://www.spinics.net/lists/ceph-users/msg75180.html</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/71080451/why-error-shows-usr-include-c-8-bits-stl-vector-h932-while-start-using-cha">https://stackoverflow.com/questions/71080451/why-error-shows-usr-include-c-8-bits-stl-vector-h932-while-start-using-cha</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/VDrift/vdrift/issues/163">https://github.com/VDrift/vdrift/issues/163</a></li>
<li><a target="_blank" rel="noopener" href="https://bugzilla.redhat.com/show_bug.cgi?id=1573253">https://bugzilla.redhat.com/show_bug.cgi?id=1573253</a></li>
<li><a target="_blank" rel="noopener" href="https://tracker.ceph.com/issues/56863">https://tracker.ceph.com/issues/56863</a></li>
<li><a target="_blank" rel="noopener" href="https://tracker.ceph.com/issues/56789">https://tracker.ceph.com/issues/56789</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://bugwz.com">bugwz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://bugwz.com/2024/05/11/ceph-rdma/">https://bugwz.com/2024/05/11/ceph-rdma/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://bugwz.com" target="_blank">咕咕</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Ceph/">Ceph</a></div><div class="post-share"><div class="social-share" data-image="/assets/images/bg/ceph.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/03/05/ceph-csi/" title="Ceph CSI 对接 K8S 指南"><img class="cover" src="/assets/images/bg/ceph.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Ceph CSI 对接 K8S 指南</div></div><div class="info-2"><div class="info-item-1">一、介绍1.1、Ceph CSI 介绍Ceph CSI 插件实现了支持 CSI 的容器编排器 (CO) 与 Ceph 集群之间的接口。它们支持动态配置 Ceph 卷并将其附加到工作负载。项目地址: https://github.com/ceph/ceph-csi 。该仓库包含用于 RBD、CephFS 和 Kubernetes sidecar 部署 YAML 的 Ceph 容器存储接口 (CSI) 驱动程序，以支持 CSI 功能：provisioner、attacher、resizer、driver-registrar 和 snapper。 本文基于 Ceph CSI v3.14.1 版本进行测试。 Ceph CSI 驱动与测试过的 Kubernetes 版本信息表: (参考 known-to-work-co-platforms)    Ceph CSI 版本 Kubernetes...</div></div></div></a><a class="pagination-related" href="/2024/08/01/gpfs/" title="GPFS 集群部署与运维记录"><img class="cover" src="/assets/images/bg/gpfs.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">GPFS 集群部署与运维记录</div></div><div class="info-2"><div class="info-item-1">一、GPFS 介绍IBM GPFS (General Parallel File System ,GPFS)是一款并行的文件系统，它保证在资源组内的所有节点可以并行访问整个文件系统，而且针对此文件系统的服务操作，可以同时安全地在此文件系统的多个节点上实现。GPFS 允许客户共享文件，而这些文件可能分布在不同节点的不同硬盘上，保证了数据的一致性和完整性。GPFS支持多种平台的部署，如Windows、Linux、AIX，每种环境部署方式相同，降低了软件部署的复杂度。 二、环境准备环境拓扑介绍:    节点名称 节点IP 节点角色    node01 10.10.0.1 Server，GUI(Dashboard)   node02 10.10.0.2 Server，GUI(Dashboard)，CES   node03 10.10.0.3 Server，CES   相关操作步骤如下:  配置 /etc/hosts : 用于节点间的 hostname 相互识别； 配置 ssh 免密登录 : 用于节点间的相互通信； 关闭防火墙和 selinux :...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2023/04/12/ceph-ansible/" title="ceph-ansible 集群部署运维指南"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-12</div><div class="info-item-2">ceph-ansible 集群部署运维指南</div></div><div class="info-2"><div class="info-item-1">本文详细介绍了使用 ceph-ansible 部署和运维 Ceph 集群的过程，包括各版本及其依赖的 Ansible 版本的对应关系、自定义模块与任务的结构、集群部署、运维操作及相关示例。特别强调了环境配置、节点连通性验证、MDS 和 OSD 组件的管理，以及安全和性能优化注意事项。 一、项目介绍以下分析基于 ceph-ansible stable-6.0 分支代码。 1.1、版本与对应关系目前 ceph-ansible 采用不同的代码分支来支持部署不同版本的 ceph 集群，且每个代码分支需要特定的 ansible 版本支持，具体的对应关系如下（以下对应关系更新于 2025&#x2F;05&#x2F;23 ）：    ceph-ansible 分支 支持的 ceph 版本 依赖的 ansible 核心版本 依赖的 ansible 发布版本包    stable-3.0 Jewel(V10), Luminous(V12) 2.4 -   stable-3.1 Luminous(V12), Mimic(V13) 2.4 -   stable-3.2 Luminous(V12),...</div></div></div></a><a class="pagination-related" href="/2023/05/01/ceph-cmd/" title="Ceph 常用命令汇总"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-01</div><div class="info-item-2">Ceph 常用命令汇总</div></div><div class="info-2"><div class="info-item-1">一、常用命令1.1、Pool# 查看 poolceph osd pool ls detail# 创建 poolceph osd pool create testpool 32 32ceph osd pool set testpool pg_autoscale_mode off# 调整 pool pg/pgp , 并关闭自动调整ceph osd pool set testpool pg_num 32ceph osd pool set testpool pgp_num 32ceph osd pool set testpool pg_autoscale_mode off# 设置 pool 最小副本ceph osd pool set testpool min_size 1ceph osd pool set testpool size 1 --yes-i-really-mean-it# 移除 poolceph tell mon.\* injectargs &#x27;--mon-allow-pool-delete=true&#x27;ceph osd pool delete...</div></div></div></a><a class="pagination-related" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-01</div><div class="info-item-2">Ceph Crimson 设计实现深入解析</div></div><div class="info-2"><div class="info-item-1">Crimson 是 Crimson OSD 的代码名称，它是下一代用于多核心可扩展性的 OSD 。它通过快速网络和存储设备提高性能，采用包括 DPDK 和 SPDK 的顶级技术。BlueStore 继续支持 HDD 和 SSD。Crimson 旨在与早期版本的 OSD 守护进程与类 Ceph OSD 兼容。 Crimson 基于 SeaStar C++ 框架构建，是核心 Ceph 对象存储守护进程 OSD 组件的新实现，并替换了 Ceph OSD 。Crimson OSD 最小化延迟并增加 CPU 处理器用量。它使用高性能异步 IO 和新的线程架构，旨在最小化上下文切换和用于跨通信的操作间的线程通信。 以下分析基于 v19.2.1 进行分析。 一、架构对比Ceph OSD 是 Ceph 集群的一部分，负责通过网络提供对象访问、维护冗余和高可用性，并将对象持久化到本地存储设备。作为 Classic OSD 的重写版本，Crimson OSD 从客户端和其他 OSD 的角度兼容现有的 RADOS 协议，提供相同的接口和功能。Ceph OSD 的模块（例如 Messenger、OSD...</div></div></div></a><a class="pagination-related" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群部署教程"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-12</div><div class="info-item-2">Ceph Crimson 集群部署教程</div></div><div class="info-2"><div class="info-item-1">当前 ceph 集群搭建部署的方式主要有三种: ceph-ansible, vstart.sh 和 cephadm 。 其中 vstart.sh 脚本用于在开发环境中快速搭建测试集群。 ceph-ansible 是之前推荐的部署 ceph 集群的方式，支持在直接在宿主机上部署或者通过容器部署的方式，目前社区已不推荐使用。 cephadm 是当前最新的支持部署生产集群的方式，仅支持容器部署。接下来主要介绍通过 vstart.sh 和 cephadm 部署 crimson 集群的方式。 一、vstart.sh 搭建集群通过这种方式部署的时候理论上对于 Ceph 版本没有特殊的要求，本文中使用的版本为 v19.2.1 。 vstart.sh 常用于在开发环境环境中快速搭建集群，且在部署集群前我们需要编译出对应的二进制包。由于编译环境可能会有各种依赖缺失，版本异常等问题，这里推荐使用 bugwz&#x2F;ceph-images 中提供的 CentOS Stream 9 的编译打包环境。同时后续的集群的搭建也可以在容器内部进行。 搭建集群操作步骤如下:  软件编译:...</div></div></div></a><a class="pagination-related" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-25</div><div class="info-item-2">Ceph QoS 机制深入分析</div></div><div class="info-2"><div class="info-item-1">一、CephFS QoS社区的相关实现：  基于 tokenbucket 算法的目录 QoS : https://github.com/ceph/ceph/pull/29266 基于 dmclock 算法的 subvolume QoS : 来自日本的 line 公司提出的想法，https://github.com/ceph/ceph/pull/38506 ， https://github.com/ceph/ceph/pull/52147  1.1、基于 TokenBucket 算法的目录 QoS该实现并未合并到主分支。  相关材料：  社区的原始PR: https://github.com/ceph/ceph/pull/29266  实现特点：  基于 TokenBucketThrottle 类在客户端侧实现的 TokenBucket 类型的 QoS，用于约束每个独立的客户端的访问请求； QoS 的限制粒度为每个独立的客户端，没有全局的QoS限制； 用于限制目录级别的操作 QoS； 支持 IOPS 和 BPS 的 QoS 限制，且支持突发流量； 仅支持 FUSE...</div></div></div></a><a class="pagination-related" href="/2024/03/05/ceph-csi/" title="Ceph CSI 对接 K8S 指南"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-05</div><div class="info-item-2">Ceph CSI 对接 K8S 指南</div></div><div class="info-2"><div class="info-item-1">一、介绍1.1、Ceph CSI 介绍Ceph CSI 插件实现了支持 CSI 的容器编排器 (CO) 与 Ceph 集群之间的接口。它们支持动态配置 Ceph 卷并将其附加到工作负载。项目地址: https://github.com/ceph/ceph-csi 。该仓库包含用于 RBD、CephFS 和 Kubernetes sidecar 部署 YAML 的 Ceph 容器存储接口 (CSI) 驱动程序，以支持 CSI 功能：provisioner、attacher、resizer、driver-registrar 和 snapper。 本文基于 Ceph CSI v3.14.1 版本进行测试。 Ceph CSI 驱动与测试过的 Kubernetes 版本信息表: (参考 known-to-work-co-platforms)    Ceph CSI 版本 Kubernetes...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">bugwz</div><div class="author-info-description">持续学习，持续进步</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">133</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">135</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/bugwz" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Ceph-RDMA-%E4%BB%8B%E7%BB%8D"><span class="toc-text">一、Ceph RDMA 介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2%E3%80%81RDMA-%E7%8E%AF%E5%A2%83%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">1.2、RDMA 环境初始化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%BD%BF%E7%94%A8-ceph-ansible-%E9%83%A8%E7%BD%B2"><span class="toc-text">二、使用 ceph-ansible 部署</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1%E3%80%81%E9%85%8D%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">2.1、配置初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1%E3%80%81%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">2.1.1、部署配置初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2%E3%80%81%E9%85%8D%E7%BD%AE%E9%83%A8%E7%BD%B2%E8%8A%82%E7%82%B9%E7%8E%AF%E5%A2%83"><span class="toc-text">2.1.2、配置部署节点环境</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E3%80%81%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4"><span class="toc-text">2.2、部署集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3%E3%80%81%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%8C%82%E8%BD%BD%E4%BD%BF%E7%94%A8"><span class="toc-text">2.3、客户端挂载使用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E4%BD%BF%E7%94%A8-cephadm-%E9%83%A8%E7%BD%B2"><span class="toc-text">三、使用 cephadm 部署</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E3%80%81%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-text">3.1、环境配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1%E3%80%81%E9%85%8D%E7%BD%AE-cephadm"><span class="toc-text">3.1.1、配置 cephadm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2%E3%80%81%E9%85%8D%E7%BD%AE%E9%83%A8%E7%BD%B2%E8%8A%82%E7%82%B9%E7%8E%AF%E5%A2%83"><span class="toc-text">3.1.2、配置部署节点环境</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2%E3%80%81%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4"><span class="toc-text">3.2、部署集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3%E3%80%81%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E8%B0%83%E6%95%B4"><span class="toc-text">3.3、集群配置调整</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4%E3%80%81%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%8C%82%E8%BD%BD%E4%BD%BF%E7%94%A8"><span class="toc-text">3.4、客户端挂载使用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98"><span class="toc-text">四、相关问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1%E3%80%81Infiniband-to-dead-failed-to-send-a-beacon-%E9%94%99%E8%AF%AF"><span class="toc-text">4.1、Infiniband to_dead failed to send a beacon 错误</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2%E3%80%81%E6%9C%AA%E9%85%8D%E7%BD%AE-LimitMEMLOCK-%E5%92%8C-PrivateDevices-%E6%8A%A5%E9%94%99"><span class="toc-text">4.2、未配置 LimitMEMLOCK 和 PrivateDevices 报错</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3%E3%80%81osd-%E8%BF%9B%E7%A8%8B%E5%BC%82%E5%B8%B8"><span class="toc-text">4.3、osd 进程异常</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-text">五、参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 设计实现深入解析"/></a><div class="content"><a class="title" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析">Ceph Crimson 设计实现深入解析</a><time datetime="2025-05-31T16:00:00.000Z" title="发表于 2025-06-01 00:00:00">2025-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/23/3fs-deploy/" title="3FS 集群部署笔记"><img src="/assets/images/bg/deepseek.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="3FS 集群部署笔记"/></a><div class="content"><a class="title" href="/2025/05/23/3fs-deploy/" title="3FS 集群部署笔记">3FS 集群部署笔记</a><time datetime="2025-05-22T16:00:00.000Z" title="发表于 2025-05-23 00:00:00">2025-05-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群部署教程"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 集群部署教程"/></a><div class="content"><a class="title" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群部署教程">Ceph Crimson 集群部署教程</a><time datetime="2025-01-11T16:00:00.000Z" title="发表于 2025-01-12 00:00:00">2025-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/01/cephfs-samba/" title="CephFS 对接 Samba 使用教程"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CephFS 对接 Samba 使用教程"/></a><div class="content"><a class="title" href="/2024/12/01/cephfs-samba/" title="CephFS 对接 Samba 使用教程">CephFS 对接 Samba 使用教程</a><time datetime="2024-11-30T16:00:00.000Z" title="发表于 2024-12-01 00:00:00">2024-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph QoS 机制深入分析"/></a><div class="content"><a class="title" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析">Ceph QoS 机制深入分析</a><time datetime="2024-10-24T16:00:00.000Z" title="发表于 2024-10-25 00:00:00">2024-10-25</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/assets/images/bg/ceph.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By bugwz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: '6af3be16b94cec39bcf6',
      clientSecret: '13a5202ff773ffcea6300b6c8ff25f455566737c',
      repo: 'bugwz.github.io',
      owner: 'bugwz',
      admin: ['bugwz'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || '300471e7dae5b8e611d774f97870d0f9'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><div class="docsearch-wrap"><div id="docsearch" style="display:none"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css/dist/style.min.css"/><script src="https://cdn.jsdelivr.net/npm/@docsearch/js/dist/umd/index.min.js"></script><script>(() => {
  docsearch(Object.assign({
    appId: 'PFB3WGSSCO',
    apiKey: '3e9cd446e41d93f2f130b91698b699f7',
    indexName: 'bugwz',
    container: '#docsearch',
    placeholder: '请输入要搜索的内容',
  }, {"maxResultsPerGroup":10}))

  const handleClick = () => {
    document.querySelector('.DocSearch-Button').click()
  }

  const searchClickFn = () => {
    btf.addEventListenerPjax(document.querySelector('#search-button > .search'), 'click', handleClick)
  }

  searchClickFn()
  window.addEventListener('pjax:complete', searchClickFn)
})()</script></div></div></body></html>