<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Ceph CSI 对接 K8S 指南 | 咕咕</title><meta name="author" content="bugwz"><meta name="copyright" content="bugwz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Ceph CSI 插件实现了支持 CSI 的容器编排器 (CO) 与 Ceph 集群之间的接口。它们支持动态配置 Ceph 卷并将其附加到工作负载。项目地址: https:&#x2F;&#x2F;github.com&#x2F;ceph&#x2F;ceph-csi 。该仓库包含用于 RBD、CephFS 和 Kubernetes sidecar 部署 YAML 的 Ceph 容器存储接口 (CSI) 驱动程序，以支持 CSI 功能：pr">
<meta property="og:type" content="article">
<meta property="og:title" content="Ceph CSI 对接 K8S 指南">
<meta property="og:url" content="https://bugwz.com/2024/03/05/ceph-csi/index.html">
<meta property="og:site_name" content="咕咕">
<meta property="og:description" content="Ceph CSI 插件实现了支持 CSI 的容器编排器 (CO) 与 Ceph 集群之间的接口。它们支持动态配置 Ceph 卷并将其附加到工作负载。项目地址: https:&#x2F;&#x2F;github.com&#x2F;ceph&#x2F;ceph-csi 。该仓库包含用于 RBD、CephFS 和 Kubernetes sidecar 部署 YAML 的 Ceph 容器存储接口 (CSI) 驱动程序，以支持 CSI 功能：pr">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bugwz.com/assets/images/bg/ceph.png">
<meta property="article:published_time" content="2024-03-04T16:00:00.000Z">
<meta property="article:modified_time" content="2025-07-10T13:57:23.063Z">
<meta property="article:author" content="bugwz">
<meta property="article:tag" content="Ceph">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bugwz.com/assets/images/bg/ceph.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Ceph CSI 对接 K8S 指南",
  "url": "https://bugwz.com/2024/03/05/ceph-csi/",
  "image": "https://bugwz.com/assets/images/bg/ceph.png",
  "datePublished": "2024-03-04T16:00:00.000Z",
  "dateModified": "2025-07-10T13:57:23.063Z",
  "author": [
    {
      "@type": "Person",
      "name": "bugwz",
      "url": "https://bugwz.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/assets/images/bg/favicon.png"><link rel="canonical" href="https://bugwz.com/2024/03/05/ceph-csi/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ceph CSI 对接 K8S 指南',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">128</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">134</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/assets/images/bg/ceph.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">咕咕</span></a><a class="nav-page-title" href="/"><span class="site-name">Ceph CSI 对接 K8S 指南</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Ceph CSI 对接 K8S 指南</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-04T16:00:00.000Z" title="发表于 2024-03-05 00:00:00">2024-03-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-10T13:57:23.063Z" title="更新于 2025-07-10 21:57:23">2025-07-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">11.9k</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h1><h2 id="1-1、Ceph-CSI-介绍"><a href="#1-1、Ceph-CSI-介绍" class="headerlink" title="1.1、Ceph CSI 介绍"></a>1.1、Ceph CSI 介绍</h2><p>Ceph CSI 插件实现了支持 CSI 的容器编排器 (CO) 与 Ceph 集群之间的接口。它们支持动态配置 Ceph 卷并将其附加到工作负载。项目地址: <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi">https://github.com/ceph/ceph-csi</a> 。该仓库包含用于 RBD、CephFS 和 Kubernetes sidecar 部署 YAML 的 Ceph 容器存储接口 (CSI) 驱动程序，以支持 CSI 功能：provisioner、attacher、resizer、driver-registrar 和 snapper。</p>
<p>本文基于 Ceph CSI <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/tree/v3.14.1">v3.14.1</a> 版本进行测试。</p>
<p><strong>Ceph CSI 驱动与测试过的 Kubernetes 版本信息表:</strong> (参考 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/tree/v3.14.1?tab=readme-ov-file#known-to-work-co-platforms">known-to-work-co-platforms</a>)</p>
<table>
<thead>
<tr>
<th align="center">Ceph CSI 版本</th>
<th align="center">Kubernetes 版本</th>
</tr>
</thead>
<tbody><tr>
<td align="center">v3.14.1</td>
<td align="center">v1.30、v1.31、v1.32</td>
</tr>
<tr>
<td align="center">v3.14.0</td>
<td align="center">v1.30、v1.31、v1.32</td>
</tr>
<tr>
<td align="center">v3.13.1</td>
<td align="center">v1.29、v1.30、v1.31</td>
</tr>
<tr>
<td align="center">v3.13.0</td>
<td align="center">v1.29、v1.30、v1.31</td>
</tr>
</tbody></table>
<p><strong>Ceph-CSI RBD 功能和可用版本信息表:</strong> (参考 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/tree/v3.14.1?tab=readme-ov-file#ceph-csi-features-and-available-versions">ceph-csi-features-and-available-versions</a>)</p>
<table>
<thead>
<tr>
<th align="center">特性</th>
<th align="center">状态</th>
<th align="center">CSI 驱动版本</th>
<th align="center">CSI 规范版本</th>
<th align="center">Ceph 集群版本</th>
<th align="center">Kubernetes 版本</th>
</tr>
</thead>
<tbody><tr>
<td align="center">动态&#x2F;取消配置块模式 RWO 卷</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消配置块模式 RWX 卷</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消配置文件模式 RWO 卷</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">创建&#x2F;删除快照</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.17.0</td>
</tr>
<tr>
<td align="center">从快照配置卷</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.17.0</td>
</tr>
<tr>
<td align="center">从另一个卷配置卷</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.16.0</td>
</tr>
<tr>
<td align="center">文件模式卷的卷&#x2F;PV 指标</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.2.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.15.0</td>
</tr>
<tr>
<td align="center">区块模式卷的卷&#x2F;PV 指标</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.2.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.21.0</td>
</tr>
<tr>
<td align="center">扩大卷</td>
<td align="center">Beta版本</td>
<td align="center">&gt;&#x3D; v2.0.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.15.0</td>
</tr>
<tr>
<td align="center">拓扑感知配置支持</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v2.1.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">从快照配置文件模式 ROX 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.17.0</td>
</tr>
<tr>
<td align="center">从另一个卷配置文件模式 ROX 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.16.0</td>
</tr>
<tr>
<td align="center">从快照配置块模式 ROX 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.17.0</td>
</tr>
<tr>
<td align="center">从另一个卷提供块模式 ROX 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.16.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消文件模式 RWOP 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.5.0</td>
<td align="center">&gt;&#x3D; v1.5.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.22.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消配置块模式 RWOP 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.5.0</td>
<td align="center">&gt;&#x3D; v1.5.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.22.0</td>
</tr>
</tbody></table>
<p><strong>Ceph-CSI CephFS 功能和可用版本信息表:</strong> (参考 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/tree/v3.14.1?tab=readme-ov-file#ceph-csi-features-and-available-versions">ceph-csi-features-and-available-versions</a>)</p>
<table>
<thead>
<tr>
<th align="center">特性</th>
<th align="center">状态</th>
<th align="center">CSI 驱动版本</th>
<th align="center">CSI 规范版本</th>
<th align="center">Ceph 集群版本</th>
<th align="center">Kubernetes 版本</th>
</tr>
</thead>
<tbody><tr>
<td align="center">动态&#x2F;取消配置文件模式 RWO 卷</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消配置文件模式 RWX 卷</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">创建和删除快照</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v3.1.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.17.0</td>
</tr>
<tr>
<td align="center">从快照配置卷</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v3.1.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.17.0</td>
</tr>
<tr>
<td align="center">从另一个卷配置卷</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v3.1.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.16.0</td>
</tr>
<tr>
<td align="center">文件模式卷的卷&#x2F;PV 指标</td>
<td align="center">正式版本</td>
<td align="center">&gt;&#x3D; v1.2.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.15.0</td>
</tr>
<tr>
<td align="center">扩大卷</td>
<td align="center">Beta版本</td>
<td align="center">&gt;&#x3D; v2.0.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.15.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消配置文件模式 ROX 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.0.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消配置文件模式 RWOP 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.5.0</td>
<td align="center">&gt;&#x3D; v1.5.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.22.0</td>
</tr>
<tr>
<td align="center">创建和删除卷组快照</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.11.0</td>
<td align="center">&gt;&#x3D; v1.9.0</td>
<td align="center">Squid (&gt;&#x3D;v19.0.0)</td>
<td align="center">&gt;&#x3D; v1.31.0</td>
</tr>
</tbody></table>
<p><strong>Ceph-CSI NFS 功能和可用版本信息表:</strong> (参考 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/tree/v3.14.1?tab=readme-ov-file#ceph-csi-features-and-available-versions">ceph-csi-features-and-available-versions</a>)</p>
<table>
<thead>
<tr>
<th align="center">特性</th>
<th align="center">状态</th>
<th align="center">CSI 驱动版本</th>
<th align="center">CSI 规范版本</th>
<th align="center">Ceph 集群版本</th>
<th align="center">Kubernetes 版本</th>
</tr>
</thead>
<tbody><tr>
<td align="center">动态&#x2F;取消配置文件模式 RWO 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.6.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消配置文件模式 RWX 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.6.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消配置文件模式 ROX 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.6.0</td>
<td align="center">&gt;&#x3D; v1.0.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.14.0</td>
</tr>
<tr>
<td align="center">动态&#x2F;取消配置文件模式 RWOP 卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.6.0</td>
<td align="center">&gt;&#x3D; v1.5.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.22.0</td>
</tr>
<tr>
<td align="center">扩大卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.7.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.15.0</td>
</tr>
<tr>
<td align="center">创建和删除快照</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.7.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.17.0</td>
</tr>
<tr>
<td align="center">从快照配置卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.7.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.17.0</td>
</tr>
<tr>
<td align="center">从另一个卷配置卷</td>
<td align="center">Alpha版本</td>
<td align="center">&gt;&#x3D; v3.7.0</td>
<td align="center">&gt;&#x3D; v1.1.0</td>
<td align="center">Pacific (&gt;&#x3D;v16.2.0)</td>
<td align="center">&gt;&#x3D; v1.16.0</td>
</tr>
</tbody></table>
<h2 id="1-2、资源拓扑情况"><a href="#1-2、资源拓扑情况" class="headerlink" title="1.2、资源拓扑情况"></a>1.2、资源拓扑情况</h2><p>本文中机器资源部署拓扑情况如下所示:</p>
<table>
<thead>
<tr>
<th align="center">机器节点</th>
<th align="center">机器IP地址</th>
<th align="center">角色</th>
</tr>
</thead>
<tbody><tr>
<td align="center">node01</td>
<td align="center">10.10.0.1</td>
<td align="center">CephServer</td>
</tr>
<tr>
<td align="center">node02</td>
<td align="center">10.10.0.2</td>
<td align="center">CephServer</td>
</tr>
<tr>
<td align="center">node03</td>
<td align="center">10.10.0.3</td>
<td align="center">CephServer&#x2F;minikube</td>
</tr>
</tbody></table>
<p><strong>字段解释:</strong></p>
<ul>
<li><code>CephServer</code>: 其中部署了 Ceph Monitor&#x2F;Manager&#x2F;OSD等组件的节点，使用的 Ceph 版本为 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph/tree/v19.2.1">v19.2.1</a> 。</li>
<li><code>minikube</code>: 搭建 K8S 测试集群的节点；</li>
</ul>
<h1 id="二、配置-Ceph-环境"><a href="#二、配置-Ceph-环境" class="headerlink" title="二、配置 Ceph 环境"></a>二、配置 Ceph 环境</h1><p>我们可以使用 cephadm 来搭建部署一个新的集群，并在已部署的 Ceph 集群之上确保初始化如下配置环境，用于支持后续通过 Ceph CSI 组件来访问 Ceph 集群数据。</p>
<h2 id="2-1、初始化数据访问环境"><a href="#2-1、初始化数据访问环境" class="headerlink" title="2.1、初始化数据访问环境"></a>2.1、初始化数据访问环境</h2><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化 CephFS 环境</span></span><br><span class="line"><span class="comment"># 如果集群未创建 MDS 组件并启用 CephFS ，可使用下面的命令创建并启用</span></span><br><span class="line"><span class="comment"># 创建一个名为 cephfs 的文件系统，对应的两个存储池为 cephfs.cephfs.data 和 cephfs.cephfs.meta</span></span><br><span class="line">ceph fs volume create cephfs</span><br><span class="line">ceph fs status</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 CephRBD 环境</span></span><br><span class="line"><span class="comment"># 创建并初始化一个名为 cephrbd 的存储池</span></span><br><span class="line">ceph osd pool create cephrbd</span><br><span class="line">ceph osd pool application <span class="built_in">enable</span> cephrbd rbd</span><br><span class="line">rbd pool init cephrbd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 RBD Image ，用于支持 Ceph CSI 静态配置 RBD</span></span><br><span class="line">rbd create -p cephrbd --image cephrbdimg01 --size 5G</span><br></pre></td></tr></table></figure>


<h2 id="2-2、新增访问密钥"><a href="#2-2、新增访问密钥" class="headerlink" title="2.2、新增访问密钥"></a>2.2、新增访问密钥</h2><p>按照 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/capabilities.md#cephfs">Ceph-CSI CephFS Capabilities</a> 和 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/capabilities.md#rbd">Ceph-CSI CephRBD Capabilities</a> 中所描述的访问密钥的所需能力，我们执行如下操作创建对应密钥。</p>
<p><strong>要求的权限能力:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># for CephFS</span></span><br><span class="line">mgr <span class="string">&quot;allow rw&quot;</span></span><br><span class="line">osd <span class="string">&quot;allow rw tag cephfs metadata=&lt;cephfs_name&gt;, allow rw tag cephfs data=&lt;cephfs_name&gt;&quot;</span></span><br><span class="line">mds <span class="string">&quot;allow r fsname=&lt;cephfs_name&gt; path=/volumes, allow rws fsname=&lt;cephfs_name&gt; path=/volumes/csi&quot;</span></span><br><span class="line">mon <span class="string">&quot;allow r fsname=&lt;cephfs_name&gt;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for CephRBD</span></span><br><span class="line">mgr <span class="string">&quot;profile rbd pool=&lt;rbd_pool_name&gt;&quot;</span></span><br><span class="line">osd <span class="string">&quot;profile rbd pool=&lt;rbd_pool_name&gt;&quot;</span></span><br><span class="line">mon <span class="string">&quot;profile rbd&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 新增 CephFS 访问用户</span></span><br><span class="line"><span class="comment"># 创建一个 csifsuser 的用户，赋予其访问名为 cephfs 文件系统的能力</span></span><br><span class="line">ceph auth get-or-create client.csifsuser \</span><br><span class="line">     mgr <span class="string">&quot;allow rw&quot;</span> \</span><br><span class="line">     osd <span class="string">&quot;allow rw tag cephfs metadata=cephfs, allow rw tag cephfs data=cephfs&quot;</span> \</span><br><span class="line">     mds <span class="string">&quot;allow r fsname=cephfs path=/volumes, allow rws fsname=cephfs path=/volumes/csi&quot;</span> \</span><br><span class="line">     mon <span class="string">&quot;allow r fsname=cephfs&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增 CephRBD 访问用户</span></span><br><span class="line"><span class="comment"># 创建一个 csirbduser 的用户，赋予其访问名为 cephrbd 存储池的能力</span></span><br><span class="line">ceph auth get-or-create client.csirbduser \</span><br><span class="line">     mgr <span class="string">&quot;profile rbd pool=cephrbd&quot;</span> \</span><br><span class="line">     osd <span class="string">&quot;profile rbd pool=cephrbd&quot;</span> \</span><br><span class="line">     mon <span class="string">&quot;profile rbd&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取用户信息</span></span><br><span class="line">ceph auth get client.csifsuser</span><br><span class="line">ceph auth get client.csirbduser</span><br></pre></td></tr></table></figure>

<h2 id="2-3、挂载-Ceph-服务"><a href="#2-3、挂载-Ceph-服务" class="headerlink" title="2.3、挂载 Ceph 服务"></a>2.3、挂载 Ceph 服务</h2><p><strong>相关命令:</strong> (位于 10.10.0.3 机器上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Kernel 方式挂载 CephFS</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /mnt/cephfs</span><br><span class="line">mount -t ceph 10.10.0.1:6789,10.10.0.2:6789,10.10.0.3:6789:/ /mnt/cephfs -o name=admin,secret=AQC3D25odVnvHRAAGDHbtmq7OaNiCa/oaZ4K3g==</span><br></pre></td></tr></table></figure>


<h1 id="三、搭建-K8S-集群"><a href="#三、搭建-K8S-集群" class="headerlink" title="三、搭建 K8S 集群"></a>三、搭建 K8S 集群</h1><h2 id="3-1、安装基础工具"><a href="#3-1、安装基础工具" class="headerlink" title="3.1、安装基础工具"></a>3.1、安装基础工具</h2><p>为了进行测试，这里使用 minikube 工具搭建单节点的 K8S 测试集群。为此我们需要安装 kubectl 和 minikube 工具。</p>
<p><strong>相关命令:</strong> (位于 10.10.0.3 机器上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装 kubectl</span></span><br><span class="line">curl -LO <span class="string">&quot;https://dl.k8s.io/release/v1.32.6/bin/linux/amd64/kubectl&quot;</span></span><br><span class="line">sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</span><br><span class="line">kubectl version --client</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 minikube</span></span><br><span class="line">curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64</span><br><span class="line">sudo install minikube-linux-amd64 /usr/local/bin/minikube &amp;&amp; <span class="built_in">rm</span> minikube-linux-amd64</span><br><span class="line">minikube version </span><br></pre></td></tr></table></figure>

<h2 id="3-2、搭建集群"><a href="#3-2、搭建集群" class="headerlink" title="3.2、搭建集群"></a>3.2、搭建集群</h2><p>如果测试机器上使用的驱动是 docker&#x2F;podman ，在使用 minikube 部署集群的时候建议使用非 root 用户执行。</p>
<blockquote>
<p><strong>注意:</strong> 对接 cephfs 的时候，需要在 minikube 搭建的 k8s 集群节点上挂载对应的 cephfs 目录，为此需要在外部挂载的目录映射到 minikube 启动的容器中。 </p>
</blockquote>
<blockquote>
<p><strong>注意:</strong> 对接 cephrbd 的时候，minikube 部署的 k8s 集群的节点中的 &#x2F;sys 挂载点需要具有写权限，同时由于 rbd map 操作会访问 &#x2F;dev&#x2F;rbd* 文件，因此该节点需要能够访问 &#x2F;dev 目录，这就需要将 &#x2F;dev 映射到 minikube 启动的容器中。</p>
</blockquote>
<p><strong>相关命令:</strong> (位于 10.10.0.3 机器上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过 minikube 搭建单节点集群（适用于 cephfs ）</span></span><br><span class="line"><span class="comment"># 注意: 由于 minikube 是创建了虚拟机节点，因此为了能够成功启动对应的 Pod ，我们需要将对应路径 mount 到虚拟机节点内部</span></span><br><span class="line">minikube start --driver=<span class="string">&quot;docker&quot;</span> \</span><br><span class="line">               --container-runtime=<span class="string">&quot;containerd&quot;</span> \</span><br><span class="line">               --mount=<span class="literal">true</span> \</span><br><span class="line">               --mount-string=<span class="string">&quot;/mnt/cephfs:/mnt/cephfs&quot;</span> \</span><br><span class="line">               --kubernetes-version=<span class="string">&quot;v1.32.6&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 minikube 搭建单节点集群（适用于 cephrbd ）</span></span><br><span class="line">minikube start --driver=<span class="string">&quot;docker&quot;</span> \</span><br><span class="line">               --container-runtime=<span class="string">&quot;containerd&quot;</span> \</span><br><span class="line">               --mount=<span class="literal">true</span> \</span><br><span class="line">               --mount-string=<span class="string">&quot;/dev:/dev&quot;</span> \</span><br><span class="line">               --kubernetes-version=<span class="string">&quot;v1.32.6&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看容器内部挂载目录信息</span></span><br><span class="line">minikube ssh <span class="string">&quot;hostname; df -h&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 minikube 启动的容器是否使用特权模式运行</span></span><br><span class="line">docker inspect --format=<span class="string">&#x27;&#123;&#123;.HostConfig.Privileged&#125;&#125;&#x27;</span> minikube</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整容器内部 /sys 目录挂载权限</span></span><br><span class="line"><span class="comment"># 该操作用于支持后续 rbd map 操作，否则可能会出现 map failed: (30) Read-only file system 报错</span></span><br><span class="line">minikube ssh -- <span class="string">&quot;mount | grep &#x27;/sys type&#x27;&quot;</span></span><br><span class="line">minikube ssh -- <span class="string">&quot;sudo mount -o remount,rw /sys&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 dashboard（单独 shell 窗口执行，该命令会前台运行）</span></span><br><span class="line">minikube dashboard --url=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 proxy 代理，用于浏览器窗口访问</span></span><br><span class="line">kubectl proxy --port=8000 --address=<span class="string">&#x27;10.10.0.3&#x27;</span> --accept-hosts=<span class="string">&#x27;^.*&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 浏览器访问 dashboard</span></span><br><span class="line">http://10.10.0.3:8000/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure>



<p><strong>相关输出信息:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 data]$ minikube start --mount=<span class="literal">true</span> --mount-string=<span class="string">&quot;/mnt/cephfs:/mnt/cephfs&quot;</span> --kubernetes-version=<span class="string">&quot;v1.32.6&quot;</span> --image-mirror-country=<span class="string">&quot;cn&quot;</span></span><br><span class="line">* minikube v1.36.0 on Centos 8.5.2111</span><br><span class="line">==== AUTHENTICATING FOR org.libvirt.unix.manage ====</span><br><span class="line">System policy prevents management of <span class="built_in">local</span> virtualized systems</span><br><span class="line">Multiple identities can be used <span class="keyword">for</span> authentication:</span><br><span class="line"> 1.  admin</span><br><span class="line"> 2.  bugwz</span><br><span class="line">Choose identity to authenticate as (1-2): 2</span><br><span class="line">Password:</span><br><span class="line">==== AUTHENTICATION COMPLETE ====</span><br><span class="line">* Automatically selected the podman driver. Other choices: none, ssh</span><br><span class="line">* Using image repository registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">* Using Podman driver with root privileges</span><br><span class="line">* Starting <span class="string">&quot;minikube&quot;</span> primary control-plane node <span class="keyword">in</span> <span class="string">&quot;minikube&quot;</span> cluster</span><br><span class="line">* Pulling base image v0.0.47 ...</span><br><span class="line">E0709 20:10:41.669773  249209 cache.go:225] Error downloading kic artifacts:  not yet implemented, see issue <span class="comment">#8426</span></span><br><span class="line">* Creating podman container (CPUs=2, Memory=3900MB) ...</span><br><span class="line">* Preparing Kubernetes v1.32.6 on Docker 28.1.1 ...</span><br><span class="line">  - Generating certificates and keys ...</span><br><span class="line">  - Booting up control plane ...</span><br><span class="line">  - Configuring RBAC rules ...</span><br><span class="line">* Configuring bridge CNI (Container Networking Interface) ...</span><br><span class="line">* Verifying Kubernetes components...</span><br><span class="line">  - Using image registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5</span><br><span class="line">* Enabled addons: storage-provisioner, default-storageclass</span><br><span class="line">* Done! kubectl is now configured to use <span class="string">&quot;minikube&quot;</span> cluster and <span class="string">&quot;default&quot;</span> namespace by default</span><br></pre></td></tr></table></figure>


<h1 id="四、部署-CSI-服务"><a href="#四、部署-CSI-服务" class="headerlink" title="四、部署 CSI 服务"></a>四、部署 CSI 服务</h1><p>以下使用的配置文件位于 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/tree/v3.14.1">ceph-csi v3.14.1</a> 项目中。</p>
<h2 id="4-1、部署-CephFS-CSI-服务"><a href="#4-1、部署-CephFS-CSI-服务" class="headerlink" title="4.1、部署 CephFS CSI 服务"></a>4.1、部署 CephFS CSI 服务</h2><p><strong>相关命令:</strong> (以下操作参考 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/cephfs/deploy.md">cephfs deploy</a>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 csi driver 对象</span></span><br><span class="line">kubectl create -f ./deploy/cephfs/kubernetes/csidriver.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为 sidecar 容器和节点插件部署 RBAC</span></span><br><span class="line"><span class="comment"># 这些清单部署了服务帐户、集群角色和集群角色绑定。</span></span><br><span class="line"><span class="comment"># rbd 和 cephfs csi 插件共享这些清单，因为它们需要相同的权限。</span></span><br><span class="line">kubectl create -f ./deploy/cephfs/kubernetes/csi-provisioner-rbac.yaml</span><br><span class="line">kubectl create -f ./deploy/cephfs/kubernetes/csi-nodeplugin-rbac.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为 csi 插件部署 config map 【需修改该文件】</span></span><br><span class="line"><span class="comment"># config map 会部署一个空的 csi 配置，该配置会以卷的形式挂载到 Ceph CSI 插件 Pod 中</span></span><br><span class="line">kubectl create -f ./deploy/cephfs/kubernetes/csi-config-map.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为 csi pod 部署 ceph 配置 config map</span></span><br><span class="line">kubectl create -f ./deploy/ceph-conf.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 部署 csi sidecar 容器</span></span><br><span class="line">kubectl create -f ./deploy/cephfs/kubernetes/csi-cephfsplugin-provisioner.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 部署 csi cephfs 驱动程序</span></span><br><span class="line">kubectl create -f ./deploy/cephfs/kubernetes/csi-cephfsplugin.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决配置依赖问题</span></span><br><span class="line"><span class="comment"># 问题详见 https://github.com/ceph/ceph-csi/issues/834 </span></span><br><span class="line">kubectl apply -f ./examples/kms/vault/kms-config.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证部署情况</span></span><br><span class="line"><span class="comment"># 由于镜像拉取速度受限于网络情况，所以各组件的初始化过程耗时可能较长</span></span><br><span class="line">kubectl get all -n default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看异常 pod 情况</span></span><br><span class="line">kubectl describe pod csi-cephfsplugin-provisioner-7968db74cb-5d4kn -n default</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>注意:</strong> 由于 csi-cephfsplugin-provisioner.yaml 内部配置了 podAntiAffinity ，所以会导致 csi-cephfsplugin-provisioner pod 会分布在不同的 pod 中，但是由于测试环境中仅有一个 minikube 节点，所以会导致只有一个 csi-cephfsplugin-provisioner pod 处于运行状态。可以通过 <code>kubectl describe pod &lt;name&gt;</code> 指令来查看对应的 pod 详细信息，其中可以看到对应的 <code>0/1 nodes are available: 1 node(s) didn&#39;t match pod anti-affinity rules. preemption: 0/1 nodes are available: 1 node(s) didn&#39;t match pod anti-affinity rules.</code> 消息。</p>
</blockquote>
<p><strong>csi-config-map.yaml 文件内容示例:</strong> (参考示例文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/deploy/csi-config-map-sample.yaml">.&#x2F;deploy&#x2F;csi-config-map-sample.yaml</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;ceph-csi-config&quot;</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.json:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        &quot;clusterID&quot;: &quot;13db9fce-5c90-11f0-8c5e-005056854af3&quot;,</span></span><br><span class="line"><span class="string">        &quot;monitors&quot;: [</span></span><br><span class="line"><span class="string">          &quot;10.10.0.1:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;10.10.0.2.58:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;10.10.0.3.59:6789&quot;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br></pre></td></tr></table></figure>

<p><strong>文件解析:</strong></p>
<ul>
<li><code>csidriver.yaml</code> : 原始文件内容 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/deploy/cephfs/kubernetes/csidriver.yaml">csidriver.yaml</a></li>
<li><code>csi-provisioner-rbac.yaml</code> : 原始文件内容 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/deploy/cephfs/kubernetes/csi-provisioner-rbac.yaml">csi-provisioner-rbac.yaml</a></li>
<li><code>csi-nodeplugin-rbac.yaml</code> : 原始文件内容 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/deploy/cephfs/kubernetes/csi-nodeplugin-rbac.yaml">csi-nodeplugin-rbac.yaml</a></li>
<li><code>csi-config-map.yaml</code> : 原始文件内容 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/deploy/cephfs/kubernetes/csi-config-map.yaml">csi-config-map.yaml</a></li>
<li><code>ceph-conf.yaml</code> : 原始文件内容 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/deploy/ceph-conf.yaml">ceph-conf.yaml</a></li>
<li><code>csi-cephfsplugin-provisioner.yaml</code> : 原始文件内容 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/deploy/cephfs/kubernetes/csi-cephfsplugin-provisioner.yaml">csi-cephfsplugin-provisioner.yaml</a></li>
<li><code>csi-cephfsplugin.yaml</code> : 原始文件内容 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/deploy/cephfs/kubernetes/csi-cephfsplugin.yaml">csi-cephfsplugin.yaml</a></li>
<li><code>kms-config.yaml</code>: 原始文件内容 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/examples/kms/vault/kms-config.yaml">kms-config.yaml</a></li>
</ul>
<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./deploy/cephfs/kubernetes/csidriver.yaml</span><br><span class="line">csidriver.storage.k8s.io/cephfs.csi.ceph.com created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./deploy/cephfs/kubernetes/csi-provisioner-rbac.yaml</span><br><span class="line">serviceaccount/cephfs-csi-provisioner created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/cephfs-external-provisioner-runner created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/cephfs-csi-provisioner-role created</span><br><span class="line">role.rbac.authorization.k8s.io/cephfs-external-provisioner-cfg created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/cephfs-csi-provisioner-role-cfg created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./deploy/cephfs/kubernetes/csi-nodeplugin-rbac.yaml</span><br><span class="line">serviceaccount/cephfs-csi-nodeplugin created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/cephfs-csi-nodeplugin created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/cephfs-csi-nodeplugin created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./deploy/cephfs/kubernetes/csi-config-map.yaml</span><br><span class="line">configmap/ceph-csi-config created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./deploy/ceph-conf.yaml</span><br><span class="line">configmap/ceph-config created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./deploy/cephfs/kubernetes/csi-cephfsplugin-provisioner.yaml</span><br><span class="line">service/csi-cephfsplugin-provisioner created</span><br><span class="line">deployment.apps/csi-cephfsplugin-provisioner created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./deploy/cephfs/kubernetes/csi-cephfsplugin.yaml</span><br><span class="line">daemonset.apps/csi-cephfsplugin created</span><br><span class="line">service/csi-metrics-cephfsplugin created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./examples/kms/vault/kms-config.yaml</span><br><span class="line">configmap/ceph-csi-encryption-kms-config created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all -n default</span><br><span class="line">NAME                                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-cephfsplugin-68knr                          3/3     Running   0          44m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-2xn6p   5/5     Running   0          44m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5d4kn   0/5     Pending   0          44m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5dhlm   0/5     Pending   0          44m</span><br><span class="line"></span><br><span class="line">NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-cephfsplugin-provisioner   ClusterIP   10.101.130.172   &lt;none&gt;        8080/TCP   44m</span><br><span class="line">service/csi-metrics-cephfsplugin       ClusterIP   10.106.150.166   &lt;none&gt;        8080/TCP   44m</span><br><span class="line">service/kubernetes                     ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    53m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-cephfsplugin   1         1         1       1            1           &lt;none&gt;          44m</span><br><span class="line"></span><br><span class="line">NAME                                           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-cephfsplugin-provisioner   1/3     3            1           44m</span><br><span class="line"></span><br><span class="line">NAME                                                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-cephfsplugin-provisioner-7968db74cb   3         3         1       44m</span><br></pre></td></tr></table></figure>


<h2 id="4-2、部署-CephRBD-CSI-服务"><a href="#4-2、部署-CephRBD-CSI-服务" class="headerlink" title="4.2、部署 CephRBD CSI 服务"></a>4.2、部署 CephRBD CSI 服务</h2><p><strong>相关命令:</strong> (以下操作参考 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/rbd/deploy.md">cephrbd deploy</a>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 csi driver 对象</span></span><br><span class="line">kubectl create -f ./deploy/rbd/kubernetes/csidriver.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为 sidecar 容器和节点插件部署 RBAC</span></span><br><span class="line"><span class="comment"># 这些清单部署了服务帐户、集群角色和集群角色绑定。</span></span><br><span class="line"><span class="comment"># rbd 和 cephfs csi 插件共享这些清单，因为它们需要相同的权限。</span></span><br><span class="line">kubectl create -f ./deploy/rbd/kubernetes/csi-provisioner-rbac.yaml</span><br><span class="line">kubectl create -f ./deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为 csi 插件部署 config map 【需修改该文件】</span></span><br><span class="line"><span class="comment"># config map 会部署一个空的 csi 配置，该配置会以卷的形式挂载到 Ceph CSI 插件 Pod 中</span></span><br><span class="line">kubectl create -f ./deploy/rbd/kubernetes/csi-config-map.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为 csi pod 部署 ceph 配置 config map</span></span><br><span class="line">kubectl create -f ./deploy/ceph-conf.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 部署 csi sidecar 容器</span></span><br><span class="line">kubectl create -f ./deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 部署 csi rbd 驱动程序</span></span><br><span class="line">kubectl create -f ./deploy/rbd/kubernetes/csi-rbdplugin.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决配置依赖问题</span></span><br><span class="line"><span class="comment"># 问题详见 https://github.com/ceph/ceph-csi/issues/834 </span></span><br><span class="line">kubectl apply -f ./examples/kms/vault/kms-config.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证部署情况</span></span><br><span class="line"><span class="comment"># 由于镜像拉取速度受限于网络情况，所以各组件的初始化过程耗时可能较长</span></span><br><span class="line">kubectl get all -n default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看异常 pod 情况</span></span><br><span class="line">kubectl describe pod csi-rbdplugin-8gpf7 -n default</span><br></pre></td></tr></table></figure>


<p><strong>csi-config-map.yaml 文件内容示例:</strong> (参考示例文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/deploy/csi-config-map-sample.yaml">.&#x2F;deploy&#x2F;csi-config-map-sample.yaml</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;ceph-csi-config&quot;</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.json:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        &quot;clusterID&quot;: &quot;13db9fce-5c90-11f0-8c5e-005056854af3&quot;,</span></span><br><span class="line"><span class="string">        &quot;monitors&quot;: [</span></span><br><span class="line"><span class="string">          &quot;10.10.0.1:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;10.10.0.2:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;10.10.0.3:6789&quot;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br></pre></td></tr></table></figure>

<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all -n default</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbdplugin-8gpf7                         3/3     Running   0          33m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-cfgft   0/7     Pending   0          33m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-qmvrz   0/7     Pending   0          33m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-z4rgc   7/7     Running   0          33m</span><br><span class="line"></span><br><span class="line">NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-metrics-rbdplugin       ClusterIP   10.101.143.224   &lt;none&gt;        8080/TCP   33m</span><br><span class="line">service/csi-rbdplugin-provisioner   ClusterIP   10.104.36.46     &lt;none&gt;        8080/TCP   33m</span><br><span class="line">service/kubernetes                  ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    37m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-rbdplugin   1         1         1       1            1           &lt;none&gt;          33m</span><br><span class="line"></span><br><span class="line">NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-rbdplugin-provisioner   1/3     3            1           33m</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-rbdplugin-provisioner-967b7f495   3         3         1       33m</span><br></pre></td></tr></table></figure>


<h1 id="五、CephFS-对接-CSI"><a href="#五、CephFS-对接-CSI" class="headerlink" title="五、CephFS 对接 CSI"></a>五、CephFS 对接 CSI</h1><blockquote>
<p><strong>注意:</strong> 动态配置卷会在收到请求时由驱动程序删除。对于静态配置卷（插件版本低于或等于 1.0.0），在执行删除操作时不会执行任何操作，预计会由用户在 Ceph 集群上删除。</p>
</blockquote>
<h2 id="5-1、创建Secret"><a href="#5-1、创建Secret" class="headerlink" title="5.1、创建Secret"></a>5.1、创建Secret</h2><p>无论是使用静态配置还是动态配置，都需要创建 Secret ，因此这里统一设置。</p>
<p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 应用 secret.yaml</span></span><br><span class="line"><span class="comment"># 根据上面创建的 CephFS 的密钥修改对应的参数</span></span><br><span class="line">kubectl apply -f ./examples/cephfs/secret.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 secret</span></span><br><span class="line">kubectl get secret -n default</span><br></pre></td></tr></table></figure>

<p><strong>secret.yaml 文件示例:</strong> (参考文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/examples/cephfs/secret.yaml">.&#x2F;examples&#x2F;cephfs&#x2F;secret.yaml</a>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: csi-cephfs-secret</span><br><span class="line">  namespace: default</span><br><span class="line">stringData:</span><br><span class="line">  userID: csifsuser</span><br><span class="line">  userKey: AQBlF25o2MkeDBAAGccHVhMXE+ZKy/b7hLuZLw==</span><br><span class="line">  encryptionPassphrase: test_passphrase</span><br></pre></td></tr></table></figure>

<p><strong>配置解析:</strong></p>
<ul>
<li><code>userID</code> : 创建的访问 CephFS 的用户名；</li>
<li><code>userKey</code> : 创建的访问 CephFS 的用户密码；</li>
<li><code>encryptionPassphrase</code> : 加密密码；</li>
</ul>
<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl apply -f ./examples/cephfs/secret.yaml</span><br><span class="line">secret/csi-cephfs-secret created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get secret -n default</span><br><span class="line">NAME                TYPE     DATA   AGE</span><br><span class="line">csi-cephfs-secret   Opaque   3      9m55s</span><br></pre></td></tr></table></figure>

<h2 id="5-2、静态配置"><a href="#5-2、静态配置" class="headerlink" title="5.2、静态配置"></a>5.2、静态配置</h2><p>我们可以将手动创建的 CephFS 子卷或卷挂载到应用程序并卸载，以下步骤显示如何创建 CephFS 子卷或卷、静态 PV 和静态 PVC。<a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/static-pvc.md">参考文档</a></p>
<h3 id="5-2-1、创建子卷"><a href="#5-2-1、创建子卷" class="headerlink" title="5.2.1、创建子卷"></a>5.2.1、创建子卷</h3><blockquote>
<p><strong>注意:</strong> 静态配置方式中，删除 PV 和 PVC 不会删除后端 CephFS 子卷或卷，如果需要，用户需要手动删除 CephFS 子卷或卷。</p>
</blockquote>
<p><strong>相关命令:</strong> (位于 10.10.0.1 节点上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建子卷组</span></span><br><span class="line"><span class="comment"># cephfs 文件系统名称，将在其中创建对应的子卷组</span></span><br><span class="line"><span class="comment"># cephfsgroup 创建的子卷组名称</span></span><br><span class="line">ceph fs subvolumegroup create cephfs cephfsgroup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建子卷</span></span><br><span class="line"><span class="comment"># k8ssubvolume 子卷名称，大小为 1GB</span></span><br><span class="line">ceph fs subvolume create cephfs csisubvolume cephfsgroup --size=1073741824</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取子卷路径信息</span></span><br><span class="line">ceph fs subvolume getpath cephfs csisubvolume cephfsgroup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件系统中的子卷组信息</span></span><br><span class="line">ceph fs subvolumegroup <span class="built_in">ls</span> cephfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件系统中特定子卷组中的子卷信息</span></span><br><span class="line">ceph fs subvolume <span class="built_in">ls</span> cephfs cephfsgroup</span><br></pre></td></tr></table></figure>

<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node01 data]<span class="comment"># ceph fs subvolumegroup create cephfs cephfsgroup</span></span><br><span class="line"></span><br><span class="line">[bugwz@node01 data]<span class="comment"># ceph fs subvolume create cephfs csisubvolume cephfsgroup --size=1073741824</span></span><br><span class="line"></span><br><span class="line">[bugwz@node01 data]<span class="comment"># ceph fs subvolume getpath cephfs csisubvolume cephfsgroup</span></span><br><span class="line">/volumes/cephfsgroup/csisubvolume/7f646a62-f63d-42b7-8b15-7af9c8072788</span><br><span class="line"></span><br><span class="line">[bugwz@node01 data]<span class="comment"># ceph fs subvolumegroup ls cephfs</span></span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;cephfsgroup&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">[bugwz@node01 data]<span class="comment"># ceph fs subvolume ls cephfs cephfsgroup</span></span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;csisubvolume&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>


<h3 id="5-2-2、创建PV"><a href="#5-2-2、创建PV" class="headerlink" title="5.2.2、创建PV"></a>5.2.2、创建PV</h3><p><strong>相关命令:</strong> (位于 10.10.0.3 节点上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 static-pv.yaml</span></span><br><span class="line">vi ./examples/cephfs/static-pv.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 pv</span></span><br><span class="line">kubectl create -f ./examples/cephfs/static-pv.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,pv -n default</span><br></pre></td></tr></table></figure>

<p><strong>static-pv.yaml 示例文件:</strong> (参考资料 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/static-pvc.md#create-cephfs-static-pv">create-cephfs-static-pv</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cephfs-static-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">csi:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">cephfs.csi.ceph.com</span></span><br><span class="line">    <span class="attr">nodeStageSecretRef:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">csi-cephfs-secret</span></span><br><span class="line">      <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">volumeAttributes:</span></span><br><span class="line">      <span class="attr">fsName:</span> <span class="string">&quot;cephfs&quot;</span></span><br><span class="line">      <span class="attr">clusterID:</span> <span class="string">&quot;13db9fce-5c90-11f0-8c5e-005056854af3&quot;</span></span><br><span class="line">      <span class="attr">staticVolume:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">rootPath:</span> <span class="string">/volumes/cephfsgroup/csisubvolume</span></span><br><span class="line">    <span class="attr">volumeHandle:</span> <span class="string">cephfs-static-pv</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br></pre></td></tr></table></figure>

<p><strong>配置解析:</strong></p>
<ul>
<li><code>nodeStageSecretRef</code> : <ul>
<li><code>name</code> : 之前创建的密钥名。必需参数。</li>
<li><code>namespace</code> : 之前创建的密钥所在的命名空间。必需参数。</li>
</ul>
</li>
<li><code>volumeAttributes</code> : 卷相关属性。<ul>
<li><code>fsName</code> : 待挂载的 CephFS 文件系统名称。不传递此选项将挂载默认文件系统。可选参数。</li>
<li><code>clusterID</code> : Ceph 集群 ID 。必需参数。</li>
<li><code>staticVolume</code> : 必须将值设置为 true 才能挂载和卸载静态 CephFS PVC 。必需参数。</li>
<li><code>rootPath</code> : Ceph 集群中子卷的实际路径，或者卷的文件夹路径。必需参数。</li>
</ul>
</li>
<li><code>volumeHandle</code> : 可以是任何内容，不需要与 PV 名称或卷名称相同。为了简洁起见保持相同。</li>
<li><code>persistentVolumeReclaimPolicy</code> : Ceph-CSI 不支持删除静态 PV 的 CephFS 子卷。所以该参数必须设置为 Retain ，以避免在 csi-provisioner 中尝试删除 PV 。</li>
</ul>
<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./examples/cephfs/static-pv.yaml</span><br><span class="line">persistentvolume/cephfs-static-pv created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,pv -n default</span><br><span class="line">NAME                                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-cephfsplugin-68knr                          3/3     Running   0          63m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-2xn6p   5/5     Running   0          63m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5d4kn   0/5     Pending   0          63m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5dhlm   0/5     Pending   0          63m</span><br><span class="line"></span><br><span class="line">NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-cephfsplugin-provisioner   ClusterIP   10.101.130.172   &lt;none&gt;        8080/TCP   63m</span><br><span class="line">service/csi-metrics-cephfsplugin       ClusterIP   10.106.150.166   &lt;none&gt;        8080/TCP   63m</span><br><span class="line">service/kubernetes                     ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    72m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-cephfsplugin   1         1         1       1            1           &lt;none&gt;          63m</span><br><span class="line"></span><br><span class="line">NAME                                           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-cephfsplugin-provisioner   1/3     3            1           63m</span><br><span class="line"></span><br><span class="line">NAME                                                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-cephfsplugin-provisioner-7968db74cb   3         3         1       63m</span><br><span class="line"></span><br><span class="line">NAME                                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE</span><br><span class="line">persistentvolume/cephfs-static-pv   1Gi        RWX            Retain           Available                          &lt;<span class="built_in">unset</span>&gt;                          3m39s</span><br></pre></td></tr></table></figure>


<h3 id="5-2-3、创建PVC"><a href="#5-2-3、创建PVC" class="headerlink" title="5.2.3、创建PVC"></a>5.2.3、创建PVC</h3><p><strong>相关命令:</strong> (位于 10.10.0.3 节点上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 static-pvc.yaml</span></span><br><span class="line">vi ./examples/cephfs/static-pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 pvc</span></span><br><span class="line">kubectl create -f ./examples/cephfs/static-pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,pv,pvc -n default</span><br></pre></td></tr></table></figure>


<p><strong>static-pvc.yaml 示例文件:</strong> (参考资料 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/static-pvc.md#create-cephfs-static-pvc">create-cephfs-static-pvc</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cephfs-static-pvc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">volumeName:</span> <span class="string">cephfs-static-pv</span></span><br></pre></td></tr></table></figure>


<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./examples/cephfs/static-pvc.yaml</span><br><span class="line">persistentvolumeclaim/cephfs-static-pvc created</span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,pv,pvc -n default</span><br><span class="line">NAME                                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-cephfsplugin-68knr                          3/3     Running   0          66m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-2xn6p   5/5     Running   0          66m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5d4kn   0/5     Pending   0          66m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5dhlm   0/5     Pending   0          66m</span><br><span class="line"></span><br><span class="line">NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-cephfsplugin-provisioner   ClusterIP   10.101.130.172   &lt;none&gt;        8080/TCP   66m</span><br><span class="line">service/csi-metrics-cephfsplugin       ClusterIP   10.106.150.166   &lt;none&gt;        8080/TCP   66m</span><br><span class="line">service/kubernetes                     ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    75m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-cephfsplugin   1         1         1       1            1           &lt;none&gt;          66m</span><br><span class="line"></span><br><span class="line">NAME                                           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-cephfsplugin-provisioner   1/3     3            1           66m</span><br><span class="line"></span><br><span class="line">NAME                                                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-cephfsplugin-provisioner-7968db74cb   3         3         1       66m</span><br><span class="line"></span><br><span class="line">NAME                                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE</span><br><span class="line">persistentvolume/cephfs-static-pv   1Gi        RWX            Retain           Bound    default/cephfs-static-pvc                  &lt;<span class="built_in">unset</span>&gt;                          6m53s</span><br><span class="line"></span><br><span class="line">NAME                                      STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE</span><br><span class="line">persistentvolumeclaim/cephfs-static-pvc   Bound    cephfs-static-pv   1Gi        RWX                           &lt;<span class="built_in">unset</span>&gt;                 26s</span><br></pre></td></tr></table></figure>

<h3 id="5-2-4、创建Pod"><a href="#5-2-4、创建Pod" class="headerlink" title="5.2.4、创建Pod"></a>5.2.4、创建Pod</h3><p><strong>相关命令:</strong> (位于 10.10.0.3 节点上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 static-pod.yaml</span></span><br><span class="line">vi ./examples/cephfs/static-pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 pod</span></span><br><span class="line">kubectl create -f ./examples/cephfs/static-pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,pv,pvc,pod -n default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证 pvc 是否已经成功挂载</span></span><br><span class="line">kubectl <span class="built_in">exec</span> cephfs-static-pod -- <span class="built_in">df</span> -h /data/pvc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 ceph 对应的目录数据</span></span><br><span class="line">tree /mnt/cephfs</span><br></pre></td></tr></table></figure>


<p><strong>static-pod.yaml 示例文件:</strong> (参考资料 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/static-pvc.md#verify-cephfs-static-pvc">verify-cephfs-static-pvc</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cephfs-static-pod</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">busybox</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">busybox:latest</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">static-pvc</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/data/pvc</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;sleep&quot;</span>, <span class="string">&quot;3600&quot;</span>]</span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">static-pvc</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">cephfs-static-pvc</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./examples/cephfs/static-pod.yaml</span><br><span class="line">pod/cephfs-static-pod created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,pv,pvc -n default</span><br><span class="line">NAME                                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/cephfs-static-pod                               1/1     Running   0          38s</span><br><span class="line">pod/csi-cephfsplugin-68knr                          3/3     Running   0          68m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-2xn6p   5/5     Running   0          68m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5d4kn   0/5     Pending   0          68m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5dhlm   0/5     Pending   0          68m</span><br><span class="line"></span><br><span class="line">NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-cephfsplugin-provisioner   ClusterIP   10.101.130.172   &lt;none&gt;        8080/TCP   68m</span><br><span class="line">service/csi-metrics-cephfsplugin       ClusterIP   10.106.150.166   &lt;none&gt;        8080/TCP   68m</span><br><span class="line">service/kubernetes                     ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    77m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-cephfsplugin   1         1         1       1            1           &lt;none&gt;          68m</span><br><span class="line"></span><br><span class="line">NAME                                           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-cephfsplugin-provisioner   1/3     3            1           68m</span><br><span class="line"></span><br><span class="line">NAME                                                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-cephfsplugin-provisioner-7968db74cb   3         3         1       68m</span><br><span class="line"></span><br><span class="line">NAME                                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE</span><br><span class="line">persistentvolume/cephfs-static-pv   1Gi        RWX            Retain           Bound    default/cephfs-static-pvc                  &lt;<span class="built_in">unset</span>&gt;                          9m3s</span><br><span class="line"></span><br><span class="line">NAME                                      STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE</span><br><span class="line">persistentvolumeclaim/cephfs-static-pvc   Bound    cephfs-static-pv   1Gi        RWX                           &lt;<span class="built_in">unset</span>&gt;                 2m36s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl <span class="built_in">exec</span> cephfs-static-pod -- <span class="built_in">df</span> -h /data/pvc</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">10.10.0.1:6789,10.10.0.2:6789,10.10.0.3:6789:/volumes/cephfsgroup/csisubvolume</span><br><span class="line">                        189.9G         0    189.9G   0% /data/pvc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ tree /mnt/cephfs</span><br><span class="line">/mnt/cephfs</span><br><span class="line">└── volumes</span><br><span class="line">    ├── cephfsgroup</span><br><span class="line">    │   └── csisubvolume</span><br><span class="line">    │       └── 7f646a62-f63d-42b7-8b15-7af9c8072788</span><br><span class="line">    └── _cephfsgroup:csisubvolume.meta</span><br><span class="line"></span><br><span class="line">4 directories, 1 file</span><br></pre></td></tr></table></figure>


<h2 id="5-3、动态配置"><a href="#5-3、动态配置" class="headerlink" title="5.3、动态配置"></a>5.3、动态配置</h2><h3 id="5-3-1、创建子卷组"><a href="#5-3-1、创建子卷组" class="headerlink" title="5.3.1、创建子卷组"></a>5.3.1、创建子卷组</h3><blockquote>
<p><strong>注意:</strong> 动态配置的时候，我们需要一个名为 csi 的子卷组，用于后续自动化的创建子卷。</p>
</blockquote>
<p><strong>相关命令:</strong> (位于 10.10.0.1 节点上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建子卷组</span></span><br><span class="line"><span class="comment"># cephfs 文件系统名称，将在其中创建对应的子卷组</span></span><br><span class="line"><span class="comment"># csi 创建的子卷组名称</span></span><br><span class="line">ceph fs subvolumegroup create cephfs csi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件系统中的子卷组信息</span></span><br><span class="line">ceph fs subvolumegroup <span class="built_in">ls</span> cephfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件系统中特定子卷组中的子卷信息</span></span><br><span class="line">ceph fs subvolume <span class="built_in">ls</span> cephfs csi</span><br></pre></td></tr></table></figure>

<h3 id="5-3-2、创建StorageClass"><a href="#5-3-2、创建StorageClass" class="headerlink" title="5.3.2、创建StorageClass"></a>5.3.2、创建StorageClass</h3><p><strong>相关命令:</strong> (位于 10.10.0.3 节点上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 编辑 ./examples/cephfs/storageclass.yaml</span></span><br><span class="line">vi ./examples/cephfs/storageclass.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/cephfs/storageclass.yaml</span></span><br><span class="line">kubectl apply -f ./examples/cephfs/storageclass.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,storageclass -n default</span><br></pre></td></tr></table></figure>

<p><strong>storageclass.yaml 文件示例:</strong> (参考文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/examples/cephfs/storageclass.yaml">.&#x2F;examples&#x2F;cephfs&#x2F;storageclass.yaml</a>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: csi-cephfs-sc</span><br><span class="line">provisioner: cephfs.csi.ceph.com</span><br><span class="line">parameters:</span><br><span class="line">  clusterID: 13db9fce-5c90-11f0-8c5e-005056854af3</span><br><span class="line">  fsName: cephfs</span><br><span class="line">  <span class="comment"># pool: &lt;cephfs-data-pool&gt;</span></span><br><span class="line">  <span class="comment"># fuseMountOptions: debug</span></span><br><span class="line">  <span class="comment"># kernelMountOptions: readdir_max_bytes=1048576,norbytes</span></span><br><span class="line">  csi.storage.k8s.io/provisioner-secret-name: csi-cephfs-secret</span><br><span class="line">  csi.storage.k8s.io/provisioner-secret-namespace: default</span><br><span class="line">  csi.storage.k8s.io/controller-expand-secret-name: csi-cephfs-secret</span><br><span class="line">  csi.storage.k8s.io/controller-expand-secret-namespace: default</span><br><span class="line">  csi.storage.k8s.io/node-stage-secret-name: csi-cephfs-secret</span><br><span class="line">  csi.storage.k8s.io/node-stage-secret-namespace: default</span><br><span class="line">  <span class="comment"># mounter: kernel</span></span><br><span class="line">  <span class="comment"># volumeNamePrefix: &quot;foo-bar-&quot;</span></span><br><span class="line">  <span class="comment"># backingSnapshot: &quot;true&quot;</span></span><br><span class="line">  <span class="comment"># encrypted: &quot;false&quot;</span></span><br><span class="line">  <span class="comment"># encryptionKMSID: &lt;kms-config-id&gt;</span></span><br><span class="line">reclaimPolicy: Delete</span><br><span class="line">allowVolumeExpansion: <span class="literal">true</span></span><br><span class="line"><span class="comment"># mountOptions:</span></span><br><span class="line"><span class="comment">#   - context=&quot;system_u:object_r:container_file_t:s0:c0,c1&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>配置解析:</strong></p>
<ul>
<li><code>clusterID</code> : Ceph 集群 ID 。确保与 csi-config-map.yaml 中的集群 ID 保持一致。必需参数。</li>
<li><code>fsName</code> : CephFS 文件系统名称，将会在其中创建卷。必需参数。</li>
<li><code>pool</code> : Ceph 池名称，将会在其中存储卷数据。可选参数。</li>
<li><code>fuseMountOptions</code> : Ceph-Fuse 挂载选项字符串，使用逗号分隔。可选参数。</li>
<li><code>kernelMountOptions</code> : Cephfs 内核挂载选项字符串，使用逗号分隔。可选参数。</li>
<li><code>mounter</code> : 挂载方式，可选值为 kernel&#x2F;fuse 。默认将自动检测使用命令确定。可选参数。</li>
<li><code>volumeNamePrefix</code> : 子卷命名前缀。默认为 csi-vol- 。可选参数。</li>
<li><code>backingSnapshot</code> : 启用时 PVC 将由其数据源中指定的 CephFS 快照支持，这时不应配置 <code>pool</code> 参数。默认为 true 。可选参数。</li>
<li><code>encrypted</code> : 是否加密卷。默认为 false 。可选参数。</li>
<li><code>encryptionKMSID</code> : 通过指定与 KMS ConfigMap 匹配的唯一 ID 来使用外部密钥管理系统进行加密密码。可选参数。</li>
</ul>
<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl apply -f ./examples/cephfs/storageclass.yaml</span><br><span class="line">storageclass.storage.k8s.io/csi-cephfs-sc created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,storageclass -n default</span><br><span class="line">NAME                                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/cephfs-static-pod                               1/1     Running   0          7m59s</span><br><span class="line">pod/csi-cephfsplugin-68knr                          3/3     Running   0          76m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-2xn6p   5/5     Running   0          76m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5d4kn   0/5     Pending   0          76m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5dhlm   0/5     Pending   0          76m</span><br><span class="line"></span><br><span class="line">NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-cephfsplugin-provisioner   ClusterIP   10.101.130.172   &lt;none&gt;        8080/TCP   76m</span><br><span class="line">service/csi-metrics-cephfsplugin       ClusterIP   10.106.150.166   &lt;none&gt;        8080/TCP   76m</span><br><span class="line">service/kubernetes                     ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    85m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-cephfsplugin   1         1         1       1            1           &lt;none&gt;          76m</span><br><span class="line"></span><br><span class="line">NAME                                           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-cephfsplugin-provisioner   1/3     3            1           76m</span><br><span class="line"></span><br><span class="line">NAME                                                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-cephfsplugin-provisioner-7968db74cb   3         3         1       76m</span><br><span class="line"></span><br><span class="line">NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">storageclass.storage.k8s.io/csi-cephfs-sc        cephfs.csi.ceph.com        Delete          Immediate           <span class="literal">true</span>                   24s</span><br><span class="line">storageclass.storage.k8s.io/standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           <span class="literal">false</span>                  85m</span><br></pre></td></tr></table></figure>


<h3 id="5-3-3、创建PVC"><a href="#5-3-3、创建PVC" class="headerlink" title="5.3.3、创建PVC"></a>5.3.3、创建PVC</h3><p><strong>相关命令:</strong> (位于 10.10.0.3 节点上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改 ./examples/cephfs/pvc.yaml</span></span><br><span class="line">vi ./examples/cephfs/pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/cephfs/pvc.yaml</span></span><br><span class="line">kubectl apply -f ./examples/cephfs/pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,storageclass,pvc -n default</span><br></pre></td></tr></table></figure>

<p><strong>pvc.yaml 文件示例:</strong> (参考文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/examples/cephfs/pvc.yaml">.&#x2F;examples&#x2F;cephfs&#x2F;pvc.yaml</a>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: csi-cephfs-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: csi-cephfs-sc</span><br></pre></td></tr></table></figure>

<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl apply -f ./examples/cephfs/pvc.yaml</span><br><span class="line">persistentvolumeclaim/csi-cephfs-pvc created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,storageclass,pvc -n default</span><br><span class="line">NAME                                                READY   STATUS    RESTARTS      AGE</span><br><span class="line">pod/cephfs-static-pod                               1/1     Running   1 (25m ago)   85m</span><br><span class="line">pod/csi-cephfsplugin-68knr                          3/3     Running   0             153m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-2xn6p   5/5     Running   0             153m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5d4kn   0/5     Pending   0             153m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5dhlm   0/5     Pending   0             153m</span><br><span class="line"></span><br><span class="line">NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-cephfsplugin-provisioner   ClusterIP   10.101.130.172   &lt;none&gt;        8080/TCP   153m</span><br><span class="line">service/csi-metrics-cephfsplugin       ClusterIP   10.106.150.166   &lt;none&gt;        8080/TCP   153m</span><br><span class="line">service/kubernetes                     ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    162m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-cephfsplugin   1         1         1       1            1           &lt;none&gt;          153m</span><br><span class="line"></span><br><span class="line">NAME                                           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-cephfsplugin-provisioner   1/3     3            1           153m</span><br><span class="line"></span><br><span class="line">NAME                                                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-cephfsplugin-provisioner-7968db74cb   3         3         1       153m</span><br><span class="line"></span><br><span class="line">NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">storageclass.storage.k8s.io/csi-cephfs-sc        cephfs.csi.ceph.com        Delete          Immediate           <span class="literal">true</span>                   77m</span><br><span class="line">storageclass.storage.k8s.io/standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           <span class="literal">false</span>                  162m</span><br><span class="line"></span><br><span class="line">NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    VOLUMEATTRIBUTESCLASS   AGE</span><br><span class="line">persistentvolumeclaim/cephfs-static-pvc   Bound    cephfs-static-pv                           1Gi        RWX                            &lt;<span class="built_in">unset</span>&gt;                 87m</span><br><span class="line">persistentvolumeclaim/csi-cephfs-pvc      Bound    pvc-99d82063-6768-421b-90cb-e0227de6a9b6   1Gi        RWX            csi-cephfs-sc   &lt;<span class="built_in">unset</span>&gt;                 74m</span><br></pre></td></tr></table></figure>


<h3 id="5-3-4、创建Pod"><a href="#5-3-4、创建Pod" class="headerlink" title="5.3.4、创建Pod"></a>5.3.4、创建Pod</h3><p><strong>相关命令:</strong> (位于 10.10.0.3 节点上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改 ./examples/cephfs/pod.yaml</span></span><br><span class="line">vi ./examples/cephfs/pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/cephfs/pod.yaml</span></span><br><span class="line">kubectl apply -f ./examples/cephfs/pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,storageclass,pvc,pod -n default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证 pvc 是否已经成功挂载</span></span><br><span class="line">kubectl <span class="built_in">exec</span> csi-cephfs-pod -- <span class="built_in">df</span> -h /var/lib/www</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 ceph 对应的目录数据</span></span><br><span class="line">tree /mnt/cephfs/volumes/csi</span><br></pre></td></tr></table></figure>

<p><strong>pod.yaml 文件示例:</strong> (参考文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/examples/cephfs/pod.yaml">.&#x2F;examples&#x2F;cephfs&#x2F;pod.yaml</a>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: csi-cephfs-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: web-server</span><br><span class="line">      image: docker.io/library/nginx:latest</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: mypvc</span><br><span class="line">          mountPath: /var/lib/www</span><br><span class="line">  volumes:</span><br><span class="line">    - name: mypvc</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: csi-cephfs-pvc</span><br><span class="line">        readOnly: <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl apply -f ./examples/cephfs/pod.yaml</span><br><span class="line">pod/csi-cephfs-pod created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,storageclass,pvc,pod -n default</span><br><span class="line">NAME                                                READY   STATUS    RESTARTS      AGE</span><br><span class="line">pod/cephfs-static-pod                               1/1     Running   1 (26m ago)   86m</span><br><span class="line">pod/csi-cephfs-pod                                  1/1     Running   0             74m</span><br><span class="line">pod/csi-cephfsplugin-68knr                          3/3     Running   0             154m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-2xn6p   5/5     Running   0             154m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5d4kn   0/5     Pending   0             154m</span><br><span class="line">pod/csi-cephfsplugin-provisioner-7968db74cb-5dhlm   0/5     Pending   0             154m</span><br><span class="line"></span><br><span class="line">NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-cephfsplugin-provisioner   ClusterIP   10.101.130.172   &lt;none&gt;        8080/TCP   154m</span><br><span class="line">service/csi-metrics-cephfsplugin       ClusterIP   10.106.150.166   &lt;none&gt;        8080/TCP   154m</span><br><span class="line">service/kubernetes                     ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    163m</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-cephfsplugin   1         1         1       1            1           &lt;none&gt;          154m</span><br><span class="line"></span><br><span class="line">NAME                                           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-cephfsplugin-provisioner   1/3     3            1           154m</span><br><span class="line"></span><br><span class="line">NAME                                                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-cephfsplugin-provisioner-7968db74cb   3         3         1       154m</span><br><span class="line"></span><br><span class="line">NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">storageclass.storage.k8s.io/csi-cephfs-sc        cephfs.csi.ceph.com        Delete          Immediate           <span class="literal">true</span>                   78m</span><br><span class="line">storageclass.storage.k8s.io/standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           <span class="literal">false</span>                  163m</span><br><span class="line"></span><br><span class="line">NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    VOLUMEATTRIBUTESCLASS   AGE</span><br><span class="line">persistentvolumeclaim/cephfs-static-pvc   Bound    cephfs-static-pv                           1Gi        RWX                            &lt;<span class="built_in">unset</span>&gt;                 88m</span><br><span class="line">persistentvolumeclaim/csi-cephfs-pvc      Bound    pvc-99d82063-6768-421b-90cb-e0227de6a9b6   1Gi        RWX            csi-cephfs-sc   &lt;<span class="built_in">unset</span>&gt;                 76m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl <span class="built_in">exec</span> csi-cephfs-pod -- <span class="built_in">df</span> -h /var/lib/www</span><br><span class="line">Filesystem                                                                                                                                         Size  Used Avail Use% Mounted on</span><br><span class="line">10.10.0.1:6789,10.10.0.2:6789,10.10.0.3:6789:/volumes/csi/csi-vol-0ebde315-c3d8-4410-a5b8-f75fe3d9951c/5bce7da0-3010-478c-9dc6-1321c1ed9b58  1.0G     0  1.0G   0% /var/lib/www</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ tree /mnt/cephfs/volumes/csi</span><br><span class="line">/mnt/cephfs/volumes/csi</span><br><span class="line">└── csi-vol-0ebde315-c3d8-4410-a5b8-f75fe3d9951c</span><br><span class="line">    └── 5bce7da0-3010-478c-9dc6-1321c1ed9b58</span><br><span class="line"></span><br><span class="line">2 directories, 0 files</span><br></pre></td></tr></table></figure>



<h1 id="六、CephRBD-对接-CSI"><a href="#六、CephRBD-对接-CSI" class="headerlink" title="六、CephRBD 对接 CSI"></a>六、CephRBD 对接 CSI</h1><h2 id="6-1、创建Secret"><a href="#6-1、创建Secret" class="headerlink" title="6.1、创建Secret"></a>6.1、创建Secret</h2><p>无论是使用静态配置还是动态配置，都需要创建 Secret ，因此这里统一设置。</p>
<p><strong>相关命令:</strong> (位于 10.10.0.3 节点上执行)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 编辑 ./examples/rbd/secret.yaml</span></span><br><span class="line">vi ./examples/rbd/secret.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/rbd/secret.yaml</span></span><br><span class="line">kubectl apply -f ./examples/rbd/secret.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,secret -n default</span><br></pre></td></tr></table></figure>

<p><strong>secret.yaml 文件示例:</strong> (参考文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/examples/rbd/secret.yaml">.&#x2F;examples&#x2F;rbd&#x2F;secret.yaml</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">userID:</span> <span class="string">csirbduser</span></span><br><span class="line">  <span class="attr">userKey:</span> <span class="string">AQBqF25oTG9cDxAAc3hGO09OCT0+J1cyUFoh6Q==</span></span><br><span class="line">  <span class="attr">encryptionPassphrase:</span> <span class="string">test_passphrase</span></span><br></pre></td></tr></table></figure>

<p><strong>配置解析:</strong></p>
<ul>
<li><code>userID</code> : 创建的访问 CephFS 的用户名；</li>
<li><code>userKey</code> : 创建的访问 CephFS 的用户密码；</li>
<li><code>encryptionPassphrase</code> : 加密密码；</li>
</ul>
<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl apply -f ./examples/rbd/secret.yaml</span><br><span class="line">secret/csi-rbd-secret created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,secret -n default</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-lrmtv   0/7     Pending   0          26m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-ltb4z   0/7     Pending   0          26m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-xx5q7   7/7     Running   0          26m</span><br><span class="line">pod/csi-rbdplugin-qhchr                         3/3     Running   0          25m</span><br><span class="line"></span><br><span class="line">NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-metrics-rbdplugin       ClusterIP   10.101.114.70   &lt;none&gt;        8080/TCP   25m</span><br><span class="line">service/csi-rbdplugin-provisioner   ClusterIP   10.111.61.159   &lt;none&gt;        8080/TCP   26m</span><br><span class="line">service/kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    27m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-rbdplugin   1         1         1       1            1           &lt;none&gt;          25m</span><br><span class="line"></span><br><span class="line">NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-rbdplugin-provisioner   1/3     3            1           26m</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-rbdplugin-provisioner-967b7f495   3         3         1       26m</span><br><span class="line"></span><br><span class="line">NAME                    TYPE     DATA   AGE</span><br><span class="line">secret/csi-rbd-secret   Opaque   3      5s</span><br></pre></td></tr></table></figure>


<h2 id="6-2、静态配置"><a href="#6-2、静态配置" class="headerlink" title="6.2、静态配置"></a>6.2、静态配置</h2><p>手动创建的 RBD 映像可以挂载到应用程序或从应用程序中卸载，以下步骤展示了如何创建 RBD 映像、静态 PV、静态 PVC 。</p>
<blockquote>
<p><strong>注意:</strong> 删除 PV 和 PVC 不会删除后端 RBD 映像，如果需要，用户需要手动删除 RBD 映像。</p>
</blockquote>
<h3 id="6-2-1、创建PV"><a href="#6-2-1、创建PV" class="headerlink" title="6.2.1、创建PV"></a>6.2.1、创建PV</h3><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建并修改 ./examples/rbd/static-pv.yaml</span></span><br><span class="line">vi ./examples/rbd/static-pv.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/rbd/static-pv.yaml</span></span><br><span class="line">kubectl create -f ./examples/rbd/static-pv.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,secret,pv -n default</span><br></pre></td></tr></table></figure>


<p><strong>static-pv.yaml 示例文件:</strong>  (参考资料: <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/static-pvc.md#create-rbd-static-pv">create-rbd-static-pv</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-static-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">csi:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">rbd.csi.ceph.com</span></span><br><span class="line">    <span class="attr">fsType:</span> <span class="string">ext4</span></span><br><span class="line">    <span class="attr">nodeStageSecretRef:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">      <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">volumeAttributes:</span></span><br><span class="line">      <span class="attr">clusterID:</span> <span class="string">&quot;13db9fce-5c90-11f0-8c5e-005056854af3&quot;</span></span><br><span class="line">      <span class="attr">pool:</span> <span class="string">&quot;cephrbd&quot;</span></span><br><span class="line">      <span class="attr">staticVolume:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">imageFeatures:</span> <span class="string">&quot;layering&quot;</span></span><br><span class="line">      <span class="comment"># mounter: rbd-nbd</span></span><br><span class="line">    <span class="attr">volumeHandle:</span> <span class="string">cephrbdimg02</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br></pre></td></tr></table></figure>

<p><strong>参数解析:</strong></p>
<ul>
<li><code>fsType</code> : </li>
<li><code>nodeStageSecretRef</code> : <ul>
<li><code>name</code> : </li>
<li><code>namespace</code> :</li>
</ul>
</li>
<li><code>volumeAttributes</code> : <ul>
<li><code>clusterID</code> : Ceph 集群 ID 。必需参数。</li>
<li><code>pool</code> : 创建 RBD 映像的池名称。必需参数。</li>
<li><code>staticVolume</code> : 必须将值设置为 true 才能挂载和卸载静态 RBD PVC 。必需参数。</li>
<li><code>imageFeatures</code> : CSI RBD 目前支持 layering,journaling,exclusive-lock 功能。如果启用了 journaling ，则还必须启用 exclusive-lock 。必需参数。</li>
<li><code>mounter</code> : 如果设置为 rbd-nbd ，则在具有 rbd-nbd 和 nbd 内核模块的节点上使用 rbd-nbd 来映射 RBD 映像。可选参数。</li>
</ul>
</li>
<li><code>volumeHandle</code> : 对应之前创建的 RBD Image ，这里为 cephrbdimg01 。</li>
<li><code>persistentVolumeReclaimPolicy</code> : Ceph-CSI 不支持删除静态 PV 的 RBD 映像。该参数必须设置为 Retain ，以避免在 csi-provisioner 中尝试删除 PV 。</li>
</ul>
<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./examples/rbd/static-pv.yaml</span><br><span class="line">persistentvolume/rbd-static-pv created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,secret,pv -n default</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-lrmtv   0/7     Pending   0          35m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-ltb4z   0/7     Pending   0          35m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-xx5q7   7/7     Running   0          35m</span><br><span class="line">pod/csi-rbdplugin-qhchr                         3/3     Running   0          34m</span><br><span class="line"></span><br><span class="line">NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-metrics-rbdplugin       ClusterIP   10.101.114.70   &lt;none&gt;        8080/TCP   34m</span><br><span class="line">service/csi-rbdplugin-provisioner   ClusterIP   10.111.61.159   &lt;none&gt;        8080/TCP   35m</span><br><span class="line">service/kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    36m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-rbdplugin   1         1         1       1            1           &lt;none&gt;          34m</span><br><span class="line"></span><br><span class="line">NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-rbdplugin-provisioner   1/3     3            1           35m</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-rbdplugin-provisioner-967b7f495   3         3         1       35m</span><br><span class="line"></span><br><span class="line">NAME                    TYPE     DATA   AGE</span><br><span class="line">secret/csi-rbd-secret   Opaque   3      9m2s</span><br><span class="line"></span><br><span class="line">NAME                             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE</span><br><span class="line">persistentvolume/rbd-static-pv   1Gi        RWO            Retain           Available                          &lt;<span class="built_in">unset</span>&gt;                          5s</span><br></pre></td></tr></table></figure>

<h3 id="6-2-2、创建PVC"><a href="#6-2-2、创建PVC" class="headerlink" title="6.2.2、创建PVC"></a>6.2.2、创建PVC</h3><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建并修改 ./examples/rbd/static-pvc.yaml</span></span><br><span class="line">vi ./examples/rbd/static-pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/rbd/static-pvc.yaml</span></span><br><span class="line">kubectl create -f ./examples/rbd/static-pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,secret,pv,pvc -n default</span><br></pre></td></tr></table></figure>

<p><strong>static-pvc.yaml 示例文件:</strong>  (参考资料: <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/static-pvc.md#create-rbd-static-pvc">create-rbd-static-pvc</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-static-pvc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">volumeName:</span> <span class="string">rbd-static-pv</span></span><br></pre></td></tr></table></figure>


<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./examples/rbd/static-pvc.yaml</span><br><span class="line">persistentvolumeclaim/rbd-static-pvc created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,secret,pv,pvc -n default</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-lrmtv   0/7     Pending   0          35m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-ltb4z   0/7     Pending   0          35m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-xx5q7   7/7     Running   0          35m</span><br><span class="line">pod/csi-rbdplugin-qhchr                         3/3     Running   0          35m</span><br><span class="line"></span><br><span class="line">NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-metrics-rbdplugin       ClusterIP   10.101.114.70   &lt;none&gt;        8080/TCP   35m</span><br><span class="line">service/csi-rbdplugin-provisioner   ClusterIP   10.111.61.159   &lt;none&gt;        8080/TCP   35m</span><br><span class="line">service/kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    36m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-rbdplugin   1         1         1       1            1           &lt;none&gt;          35m</span><br><span class="line"></span><br><span class="line">NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-rbdplugin-provisioner   1/3     3            1           35m</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-rbdplugin-provisioner-967b7f495   3         3         1       35m</span><br><span class="line"></span><br><span class="line">NAME                    TYPE     DATA   AGE</span><br><span class="line">secret/csi-rbd-secret   Opaque   3      9m39s</span><br><span class="line"></span><br><span class="line">NAME                             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE</span><br><span class="line">persistentvolume/rbd-static-pv   1Gi        RWO            Retain           Bound    default/rbd-static-pvc                  &lt;<span class="built_in">unset</span>&gt;                          42s</span><br><span class="line"></span><br><span class="line">NAME                                   STATUS   VOLUME          CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE</span><br><span class="line">persistentvolumeclaim/rbd-static-pvc   Bound    rbd-static-pv   1Gi        RWO                           &lt;<span class="built_in">unset</span>&gt;                 14s</span><br></pre></td></tr></table></figure>


<h3 id="6-2-3、创建Pod"><a href="#6-2-3、创建Pod" class="headerlink" title="6.2.3、创建Pod"></a>6.2.3、创建Pod</h3><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建并修改 ./examples/rbd/static-pod.yaml</span></span><br><span class="line">vi ./examples/rbd/static-pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/rbd/static-pod.yaml</span></span><br><span class="line">kubectl create -f ./examples/rbd/static-pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,secret,pv,pvc,pod -n default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证 pvc 是否已经在 pod 中挂载</span></span><br><span class="line">kubectl <span class="built_in">exec</span> rbd-static-pod -- <span class="built_in">df</span> -h /var/lib/www/html</span><br></pre></td></tr></table></figure>

<p><strong>static-pod.yaml 示例文件:</strong>  (参考资料: <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/docs/static-pvc.md#verify-rbd-static-pvc">verify-rbd-static-pvc</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-static-pod</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web-server</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">docker.io/library/nginx:latest</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">static-pvc</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/lib/www/html</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">static-pvc</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">rbd-static-pvc</span></span><br></pre></td></tr></table></figure>


<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl create -f ./examples/rbd/static-pod.yaml</span><br><span class="line">pod/rbd-static-pod created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,secret,pv,pvc,pod -n default</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-lrmtv   0/7     Pending   0          38m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-ltb4z   0/7     Pending   0          38m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-xx5q7   7/7     Running   0          38m</span><br><span class="line">pod/csi-rbdplugin-qhchr                         3/3     Running   0          38m</span><br><span class="line">pod/rbd-static-pod                              1/1     Running   0          59s</span><br><span class="line"></span><br><span class="line">NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-metrics-rbdplugin       ClusterIP   10.101.114.70   &lt;none&gt;        8080/TCP   38m</span><br><span class="line">service/csi-rbdplugin-provisioner   ClusterIP   10.111.61.159   &lt;none&gt;        8080/TCP   38m</span><br><span class="line">service/kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    39m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-rbdplugin   1         1         1       1            1           &lt;none&gt;          38m</span><br><span class="line"></span><br><span class="line">NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-rbdplugin-provisioner   1/3     3            1           38m</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-rbdplugin-provisioner-967b7f495   3         3         1       38m</span><br><span class="line"></span><br><span class="line">NAME                    TYPE     DATA   AGE</span><br><span class="line">secret/csi-rbd-secret   Opaque   3      12m</span><br><span class="line"></span><br><span class="line">NAME                             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE</span><br><span class="line">persistentvolume/rbd-static-pv   1Gi        RWO            Retain           Bound    default/rbd-static-pvc                  &lt;<span class="built_in">unset</span>&gt;                          3m38s</span><br><span class="line"></span><br><span class="line">NAME                                   STATUS   VOLUME          CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE</span><br><span class="line">persistentvolumeclaim/rbd-static-pvc   Bound    rbd-static-pv   1Gi        RWO                           &lt;<span class="built_in">unset</span>&gt;                 3m10s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl <span class="built_in">exec</span> rbd-static-pod -- <span class="built_in">df</span> -h /var/lib/www/html</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/rbd1       4.9G   24K  4.9G   1% /var/lib/www/html</span><br></pre></td></tr></table></figure>


<h2 id="6-3、动态配置"><a href="#6-3、动态配置" class="headerlink" title="6.3、动态配置"></a>6.3、动态配置</h2><h3 id="6-3-1、创建StorageClass"><a href="#6-3-1、创建StorageClass" class="headerlink" title="6.3.1、创建StorageClass"></a>6.3.1、创建StorageClass</h3><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 编辑 ./examples/rbd/storageclass.yaml</span></span><br><span class="line">vi ./examples/rbd/storageclass.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/rbd/storageclass.yaml</span></span><br><span class="line">kubectl apply -f ./examples/rbd/storageclass.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,storageclass -n default</span><br></pre></td></tr></table></figure>

<p><strong>storageclass.yaml 文件示例:</strong> (参考文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/examples/rbd/storageclass.yaml">.&#x2F;examples&#x2F;rbd&#x2F;storageclass.yaml</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbd-sc</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rbd.csi.ceph.com</span></span><br><span class="line"><span class="comment"># volumeBindingMode: WaitForFirstConsumer</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">clusterID:</span> <span class="string">13db9fce-5c90-11f0-8c5e-005056854af3</span></span><br><span class="line">  <span class="comment"># dataPool: &lt;ec-data-pool&gt;</span></span><br><span class="line">  <span class="attr">pool:</span> <span class="string">cephrbd</span></span><br><span class="line">  <span class="attr">imageFeatures:</span> <span class="string">&quot;layering&quot;</span></span><br><span class="line">  <span class="comment"># mkfsOptions: &quot;-m0 -Ediscard -i1024&quot;</span></span><br><span class="line">  <span class="comment"># tryOtherMounters: false</span></span><br><span class="line">  <span class="comment"># mapOptions: &quot;krbd:lock_on_read,queue_depth=1024;nbd:try-netlink&quot;</span></span><br><span class="line">  <span class="comment"># unmapOptions: &quot;krbd:force;nbd:force&quot;</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/provisioner-secret-name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/provisioner-secret-namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/controller-expand-secret-name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/controller-expand-secret-namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/node-stage-secret-name:</span> <span class="string">csi-rbd-secret</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/node-stage-secret-namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/fstype:</span> <span class="string">ext4</span></span><br><span class="line">  <span class="comment"># mounter: rbd-nbd</span></span><br><span class="line">  <span class="comment"># cephLogDir: /var/log/ceph</span></span><br><span class="line">  <span class="comment"># cephLogStrategy: remove</span></span><br><span class="line">  <span class="comment"># volumeNamePrefix: &quot;foo-bar-&quot;</span></span><br><span class="line">  <span class="comment"># encrypted: &quot;false&quot;</span></span><br><span class="line">  <span class="comment"># encryptionType: &quot;block&quot;</span></span><br><span class="line">  <span class="comment"># encryptionKMSID: &lt;kms-config-id&gt;</span></span><br><span class="line">  <span class="comment"># topologyConstrainedPools: |</span></span><br><span class="line">  <span class="comment">#   [</span></span><br><span class="line">  <span class="comment">#     &#123;</span></span><br><span class="line">  <span class="comment">#       &quot;poolName&quot;:&quot;pool0&quot;,</span></span><br><span class="line">  <span class="comment">#       &quot;dataPool&quot;:&quot;ec-pool0&quot; # 可选，用于数据的纠删码池</span></span><br><span class="line">  <span class="comment">#       &quot;domainSegments&quot;: [</span></span><br><span class="line">  <span class="comment">#         &#123;&quot;domainLabel&quot;:&quot;region&quot;,&quot;value&quot;:&quot;east&quot;&#125;,</span></span><br><span class="line">  <span class="comment">#         &#123;&quot;domainLabel&quot;:&quot;zone&quot;,&quot;value&quot;:&quot;zone1&quot;&#125;</span></span><br><span class="line">  <span class="comment">#       ]</span></span><br><span class="line">  <span class="comment">#     &#125;,</span></span><br><span class="line">  <span class="comment">#     &#123;</span></span><br><span class="line">  <span class="comment">#       &quot;poolName&quot;:&quot;pool1&quot;,</span></span><br><span class="line">  <span class="comment">#       &quot;dataPool&quot;:&quot;ec-pool1&quot; # 可选，用于数据的纠删码池</span></span><br><span class="line">  <span class="comment">#       &quot;domainSegments&quot;:[</span></span><br><span class="line">  <span class="comment">#         &#123;&quot;domainLabel&quot;:&quot;region&quot;,&quot;value&quot;:&quot;east&quot;&#125;,</span></span><br><span class="line">  <span class="comment">#         &#123;&quot;domainLabel&quot;:&quot;zone&quot;,&quot;value&quot;:&quot;zone2&quot;&#125;</span></span><br><span class="line">  <span class="comment">#       ]</span></span><br><span class="line">  <span class="comment">#     &#125;,</span></span><br><span class="line">  <span class="comment">#     &#123;</span></span><br><span class="line">  <span class="comment">#       &quot;poolName&quot;:&quot;pool2&quot;,</span></span><br><span class="line">  <span class="comment">#       &quot;dataPool&quot;:&quot;ec-pool2&quot; # 可选，用于数据的纠删码池</span></span><br><span class="line">  <span class="comment">#       &quot;domainSegments&quot;:[</span></span><br><span class="line">  <span class="comment">#         &#123;&quot;domainLabel&quot;:&quot;region&quot;,&quot;value&quot;:&quot;west&quot;&#125;,</span></span><br><span class="line">  <span class="comment">#         &#123;&quot;domainLabel&quot;:&quot;zone&quot;,&quot;value&quot;:&quot;zone1&quot;&#125;</span></span><br><span class="line">  <span class="comment">#       ]</span></span><br><span class="line">  <span class="comment">#     &#125;</span></span><br><span class="line">  <span class="comment">#   ]</span></span><br><span class="line">  <span class="comment"># stripeUnit: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># stripeCount: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># objectSize: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># BaseReadIops: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># BaseWriteIops: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># BaseReadBytesPerSecond: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># BaseWriteBytesPerSecond: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># ReadIopsPerGiB: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># WriteIopsPerGiB: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># ReadBpsPerGiB: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># WriteBpsPerGiB: &lt;&gt;</span></span><br><span class="line">  <span class="comment"># BaseVolSizeBytes:&lt;&gt;</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line"><span class="attr">allowVolumeExpansion:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">mountOptions:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">discard</span></span><br></pre></td></tr></table></figure>

<p><strong>配置解析:</strong></p>
<ul>
<li><code>clusterID</code> : Ceph 集群 ID 。确保与 csi-config-map.yaml 中的集群 ID 保持一致。必需参数。</li>
<li><code>dataPool</code> : 如果要使用带有 RBD 的纠删码池，则需要创建两个池。一个是纠删码池，一个是复制池。该参数用于设置对应的纠删码池。可选参数。</li>
<li><code>pool</code> : Ceph 池名称，将会在其中存储卷数据。如果指定了 dataPool 的值，则该值用于设置对应额复制池，用于存储Image元数据。必需参数。</li>
<li><code>imageFeatures</code> : RBD 图像功能，可选值为 layering,journaling,exclusive-lock,object-map,fast-diff,deep-flatten 。可选参数。</li>
<li><code>mkfsOptions</code> : 在 RBD 设备上创建文件系统时传递给 <code>mkfs</code> 命令的选项。当指定值后将取代默认值。默认选项取决于 csi.storage.k8s.io&#x2F;fstype 类型。可选参数。<ul>
<li>ext4 时，默认值为 <code>-m0 -Enodiscard,lazy_itable_init=1,lazy_journal_init=1</code> ；</li>
<li>xfs 时，默认值为 <code>-K</code> ；</li>
</ul>
</li>
<li><code>tryOtherMounters</code> : 指定是否在当前挂载器无法挂载 rbd 图像时尝试其他挂载器。默认为 false 。可选参数。</li>
<li><code>mapOptions</code> : 以逗号分隔的映射选项列表。默认为空。可选参数。<ul>
<li>krbd 选项，请参阅 <a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options">https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options</a></li>
<li>nbd 选项，请参阅 <a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options">https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options</a></li>
</ul>
</li>
<li><code>unmapOptions</code> : 以逗号分隔的取消映射选项列表。默认为空。可选参数。<ul>
<li>krbd 选项，请参阅 <a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options">https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options</a></li>
<li>nbd 选项，请参阅 <a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options">https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options</a></li>
</ul>
</li>
<li><code>csi.storage.k8s.io/fstype</code> : 指定卷的文件系统类型。如果未指定。可选参数。</li>
<li><code>mounter</code> : 设置挂载器。可选参数。</li>
<li><code>cephLogDir</code> : Ceph 客户端日志位置。可选参数。</li>
<li><code>cephLogStrategy</code> : Ceph 客户端日志策略。默认为 remove 。可选参数。<ul>
<li>remove : 取消映射时删除日志；</li>
<li>compress : 取消映射时仅压缩而不删除；</li>
<li>preserve : 取消映射时保留日志文件的文本格式；</li>
</ul>
</li>
<li><code>volumeNamePrefix</code> : RBD 映像的前缀。默认为 csi-vol- 。可选参数。</li>
<li><code>encrypted</code> : 是否加密卷。默认为 false 。可选参数。</li>
<li><code>encryptionType</code> : 当启用加密卷时对应的加密类型。默认为 block 。可选参数。<ul>
<li>file : 在挂载的文件系统上启用文件加密；</li>
<li>block : 加密 RBD 块设备；</li>
</ul>
</li>
<li><code>encryptionKMSID</code> : 指定与 KMS ConfigMap 匹配的唯一 ID 使用外部密钥管理系统进行加密密码。可选参数。</li>
<li><code>topologyConstrainedPools</code> : 拓扑约束池配置，如果设置了基于拓扑的池，并且需要拓扑约束供应。可选参数。</li>
<li><code>stripeUnit</code> : 图像条带化，条带单位（以字节为单位）。可选参数。</li>
<li><code>stripeCount</code> : 图像条带化，在循环之前要条带化的对象。可选参数。</li>
<li><code>objectSize</code> : 图像条带化，对象大小（以字节为单位）。可选参数。</li>
<li><code>BaseReadIops</code> : RBD 卷 QoS ，每秒读取操作的基本限制。仅支持 rbd-nbd 挂载类型。可选参数。</li>
<li><code>BaseWriteIops</code> : RBD 卷 QoS ，每秒写入操作的基本限制。仅支持 rbd-nbd 挂载类型。可选参数。</li>
<li><code>BaseReadBytesPerSecond</code> : RBD 卷 QoS ，每秒读取字节的基本限制。仅支持 rbd-nbd 挂载类型。可选参数。</li>
<li><code>BaseWriteBytesPerSecond</code> : RBD 卷 QoS ，每秒写入字节的基本限制。仅支持 rbd-nbd 挂载类型。可选参数。</li>
<li><code>ReadIopsPerGiB</code> : RBD 卷 QoS ，每 GiB 的读取操作限制。仅支持 rbd-nbd 挂载类型。可选参数。</li>
<li><code>WriteIopsPerGiB</code> : RBD 卷 QoS ，每 GiB 的写入操作限制。仅支持 rbd-nbd 挂载类型。可选参数。</li>
<li><code>ReadBpsPerGiB</code> : RBD 卷 QoS ，每 GiB 的读取字节限制。仅支持 rbd-nbd 挂载类型。可选参数。</li>
<li><code>WriteBpsPerGiB</code> : RBD 卷 QoS ，每 GiB 的写入字节限制。仅支持 rbd-nbd 挂载类型。可选参数。</li>
<li><code>BaseVolSizeBytes</code> : RBD 卷 QoS ，用于根据容量计算 qos 的卷的最小大小。仅支持 rbd-nbd 挂载类型。可选参数。</li>
</ul>
<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl apply -f ./examples/rbd/storageclass.yaml</span><br><span class="line">storageclass.storage.k8s.io/csi-rbd-sc created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,storageclass -n default</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-lrmtv   0/7     Pending   0          42m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-ltb4z   0/7     Pending   0          42m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-xx5q7   7/7     Running   0          42m</span><br><span class="line">pod/csi-rbdplugin-qhchr                         3/3     Running   0          42m</span><br><span class="line"></span><br><span class="line">NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-metrics-rbdplugin       ClusterIP   10.101.114.70   &lt;none&gt;        8080/TCP   42m</span><br><span class="line">service/csi-rbdplugin-provisioner   ClusterIP   10.111.61.159   &lt;none&gt;        8080/TCP   42m</span><br><span class="line">service/kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    43m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-rbdplugin   1         1         1       1            1           &lt;none&gt;          42m</span><br><span class="line"></span><br><span class="line">NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-rbdplugin-provisioner   1/3     3            1           42m</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-rbdplugin-provisioner-967b7f495   3         3         1       42m</span><br><span class="line"></span><br><span class="line">NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">storageclass.storage.k8s.io/csi-rbd-sc           rbd.csi.ceph.com           Delete          Immediate           <span class="literal">true</span>                   28s</span><br><span class="line">storageclass.storage.k8s.io/standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           <span class="literal">false</span>                  43m</span><br></pre></td></tr></table></figure>



<h3 id="6-3-2、创建PVC"><a href="#6-3-2、创建PVC" class="headerlink" title="6.3.2、创建PVC"></a>6.3.2、创建PVC</h3><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改 ./examples/rbd/pvc.yaml</span></span><br><span class="line">vi ./examples/rbd/pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/rbd/pvc.yaml</span></span><br><span class="line">kubectl apply -f ./examples/rbd/pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,storageclass,pvc -n default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 rbd 中 image 列表</span></span><br><span class="line">rbd <span class="built_in">ls</span> -p cephrbd</span><br></pre></td></tr></table></figure>

<p><strong>pvc.yaml 文件示例:</strong> (参考文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/examples/rbd/pvc.yaml">.&#x2F;examples&#x2F;rbd&#x2F;pvc.yaml</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rbd-pvc</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">group:</span> <span class="string">test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">csi-rbd-sc</span></span><br></pre></td></tr></table></figure>

<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl apply -f ./examples/rbd/pvc.yaml</span><br><span class="line">persistentvolumeclaim/rbd-pvc created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,storageclass,pvc -n default</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-lrmtv   0/7     Pending   0          44m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-ltb4z   0/7     Pending   0          44m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-xx5q7   7/7     Running   0          44m</span><br><span class="line">pod/csi-rbdplugin-qhchr                         3/3     Running   0          44m</span><br><span class="line"></span><br><span class="line">NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-metrics-rbdplugin       ClusterIP   10.101.114.70   &lt;none&gt;        8080/TCP   44m</span><br><span class="line">service/csi-rbdplugin-provisioner   ClusterIP   10.111.61.159   &lt;none&gt;        8080/TCP   44m</span><br><span class="line">service/kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    45m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-rbdplugin   1         1         1       1            1           &lt;none&gt;          44m</span><br><span class="line"></span><br><span class="line">NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-rbdplugin-provisioner   1/3     3            1           44m</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-rbdplugin-provisioner-967b7f495   3         3         1       44m</span><br><span class="line"></span><br><span class="line">NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">storageclass.storage.k8s.io/csi-rbd-sc           rbd.csi.ceph.com           Delete          Immediate           <span class="literal">true</span>                   2m31s</span><br><span class="line">storageclass.storage.k8s.io/standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           <span class="literal">false</span>                  45m</span><br><span class="line"></span><br><span class="line">NAME                            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE</span><br><span class="line">persistentvolumeclaim/rbd-pvc   Bound    pvc-173db48b-41bf-4826-9759-08d27838a882   1Gi        RWO            csi-rbd-sc     &lt;<span class="built_in">unset</span>&gt;                 9s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]<span class="comment"># rbd ls -p cephrbd</span></span><br><span class="line">cephrbdimg01</span><br><span class="line">cephrbdimg02</span><br><span class="line">cephrbdimg03</span><br><span class="line">csi-vol-dbc0b1f7-7d2c-4332-a789-8e2345b91158</span><br></pre></td></tr></table></figure>


<h3 id="6-3-3、创建Pod"><a href="#6-3-3、创建Pod" class="headerlink" title="6.3.3、创建Pod"></a>6.3.3、创建Pod</h3><p><strong>相关命令:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改 ./examples/rbd/pod.yaml</span></span><br><span class="line">vi ./examples/rbd/pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 ./examples/rbd/pod.yaml</span></span><br><span class="line">kubectl apply -f ./examples/rbd/pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get all,storageclass,pvc,pod -n default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证 pvc 是否已经在 pod 中挂载</span></span><br><span class="line">kubectl <span class="built_in">exec</span> csi-rbd-pod -- <span class="built_in">df</span> -h /var/lib/www/html</span><br></pre></td></tr></table></figure>

<p><strong>pod.yaml 文件示例:</strong> (参考文件 <a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/v3.14.1/examples/rbd/pod.yaml">.&#x2F;examples&#x2F;rbd&#x2F;pod.yaml</a>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-rbd-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web-server</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">docker.io/library/nginx:latest</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mypvc</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/lib/www/html</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mypvc</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">rbd-pvc</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>


<p><strong>相关操作记录:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bugwz@node03 ceph-csi]$ kubectl apply -f ./examples/rbd/pod.yaml</span><br><span class="line">pod/csi-rbd-pod created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl get all,storageclass,pvc,pod -n default</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbd-pod                                 1/1     Running   0          11s</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-lrmtv   0/7     Pending   0          49m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-ltb4z   0/7     Pending   0          49m</span><br><span class="line">pod/csi-rbdplugin-provisioner-967b7f495-xx5q7   7/7     Running   0          49m</span><br><span class="line">pod/csi-rbdplugin-qhchr                         3/3     Running   0          49m</span><br><span class="line"></span><br><span class="line">NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/csi-metrics-rbdplugin       ClusterIP   10.101.114.70   &lt;none&gt;        8080/TCP   49m</span><br><span class="line">service/csi-rbdplugin-provisioner   ClusterIP   10.111.61.159   &lt;none&gt;        8080/TCP   49m</span><br><span class="line">service/kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    50m</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">daemonset.apps/csi-rbdplugin   1         1         1       1            1           &lt;none&gt;          49m</span><br><span class="line"></span><br><span class="line">NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/csi-rbdplugin-provisioner   1/3     3            1           49m</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/csi-rbdplugin-provisioner-967b7f495   3         3         1       49m</span><br><span class="line"></span><br><span class="line">NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">storageclass.storage.k8s.io/csi-rbd-sc           rbd.csi.ceph.com           Delete          Immediate           <span class="literal">true</span>                   7m48s</span><br><span class="line">storageclass.storage.k8s.io/standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           <span class="literal">false</span>                  50m</span><br><span class="line"></span><br><span class="line">NAME                            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE</span><br><span class="line">persistentvolumeclaim/rbd-pvc   Bound    pvc-173db48b-41bf-4826-9759-08d27838a882   1Gi        RWO            csi-rbd-sc     &lt;<span class="built_in">unset</span>&gt;                 5m26s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[bugwz@node03 ceph-csi]$ kubectl <span class="built_in">exec</span> csi-rbd-pod -- <span class="built_in">df</span> -h /var/lib/www/html</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/rbd1       974M   24K  958M   1% /var/lib/www/html</span><br></pre></td></tr></table></figure>


<h1 id="七、参考资料"><a href="#七、参考资料" class="headerlink" title="七、参考资料"></a>七、参考资料</h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi">https://github.com/ceph/ceph-csi</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/">https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/lianngkyle/p/14772121.html">https://www.cnblogs.com/lianngkyle/p/14772121.html</a></li>
<li><a target="_blank" rel="noopener" href="https://dylanyang.top/post/2021/05/15/k8s%E4%BD%BF%E7%94%A8ceph-csi%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8cephfs/">https://dylanyang.top/post/2021/05/15/k8s%E4%BD%BF%E7%94%A8ceph-csi%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8cephfs/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/jiaxzeng/p/14880660.html">https://www.cnblogs.com/jiaxzeng/p/14880660.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.modb.pro/db/137721">https://www.modb.pro/db/137721</a></li>
<li><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tutorials/hello-minikube/">https://kubernetes.io/zh-cn/docs/tutorials/hello-minikube/</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://bugwz.com">bugwz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://bugwz.com/2024/03/05/ceph-csi/">https://bugwz.com/2024/03/05/ceph-csi/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://bugwz.com" target="_blank">咕咕</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Ceph/">Ceph</a></div><div class="post-share"><div class="social-share" data-image="/assets/images/bg/ceph.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2023/06/30/ceph-crush/" title="Ceph CRUSH 设计实现剖析"><img class="cover" src="/assets/images/bg/ceph.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Ceph CRUSH 设计实现剖析</div></div><div class="info-2"><div class="info-item-1">CRUSH（Controlled Replication Under Scalable Hashing）是 Ceph 存储系统中用于数据分布和复制的算法。关于 CRUSH 的论文解析参考: 译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data 。CRUSH map 是 Ceph 集群中一个关键的配置组件，它定义了数据如何在集群的物理硬件上分布。 CRUSH 算法使得 Ceph 能够在无需中心化或者分布式元数据管理器的情况下，高效、可靠地进行数据复制和恢复。 一、CRUSH map 解析CRUSH map 包含了集群的层次结构和各种规则，这些规则定义了数据应该如何在集群中分布。 CRUSH map 主要包含以下几个部分：  Tunables : 一组可用于调整 CRUSH 算法行为的参数。 Devices : 定义集群中所有可用的存储设备的列表。 Types : 定义存储层次结构中的不同层级类型。 Buckets : 组织和管理存储设备（如 OSDs ）的逻辑容器。 Rules :...</div></div></div></a><a class="pagination-related" href="/2024/08/01/gpfs/" title="GPFS 集群部署与运维记录"><img class="cover" src="/assets/images/bg/gpfs.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">GPFS 集群部署与运维记录</div></div><div class="info-2"><div class="info-item-1">一、GPFS 介绍IBM GPFS (General Parallel File System ,GPFS)是一款并行的文件系统，它保证在资源组内的所有节点可以并行访问整个文件系统，而且针对此文件系统的服务操作，可以同时安全地在此文件系统的多个节点上实现。GPFS 允许客户共享文件，而这些文件可能分布在不同节点的不同硬盘上，保证了数据的一致性和完整性。GPFS支持多种平台的部署，如Windows、Linux、AIX，每种环境部署方式相同，降低了软件部署的复杂度。 二、环境准备环境拓扑介绍:    节点名称 节点IP 节点角色    node01 10.10.0.1 Server，GUI(Dashboard)   node02 10.10.0.2 Server，GUI(Dashboard)，CES   node03 10.10.0.3 Server，CES   相关操作步骤如下:  配置 /etc/hosts : 用于节点间的 hostname 相互识别； 配置 ssh 免密登录 : 用于节点间的相互通信； 关闭防火墙和 selinux :...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2023/04/12/ceph-ansible/" title="ceph-ansible 集群部署运维指南"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-12</div><div class="info-item-2">ceph-ansible 集群部署运维指南</div></div><div class="info-2"><div class="info-item-1">本文详细介绍了使用 ceph-ansible 部署和运维 Ceph 集群的过程，包括各版本及其依赖的 Ansible 版本的对应关系、自定义模块与任务的结构、集群部署、运维操作及相关示例。特别强调了环境配置、节点连通性验证、MDS 和 OSD 组件的管理，以及安全和性能优化注意事项。 一、项目介绍以下分析基于 ceph-ansible stable-6.0 分支代码。 1.1、版本与对应关系目前 ceph-ansible 采用不同的代码分支来支持部署不同版本的 ceph 集群，且每个代码分支需要特定的 ansible 版本支持，具体的对应关系如下（以下对应关系更新于 2025&#x2F;05&#x2F;23 ）：    ceph-ansible 分支 支持的 ceph 版本 依赖的 ansible 核心版本 依赖的 ansible 发布版本包    stable-3.0 Jewel(V10), Luminous(V12) 2.4 -   stable-3.1 Luminous(V12), Mimic(V13) 2.4 -   stable-3.2 Luminous(V12),...</div></div></div></a><a class="pagination-related" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-12</div><div class="info-item-2">Ceph Crimson 集群搭建指南</div></div><div class="info-2"><div class="info-item-1">当前 ceph 集群搭建部署的方式主要有三种: ceph-ansible ，vstart.sh ， cephadm 。 其中 vstart.sh 脚本用于在开发环境中快速搭建测试集群； ceph-ansible 是一种部署 ceph 集群的老方式，支持在宿主机及容器部署的方式，目前社区已不推荐使用；cephadm 是当前最新的支持部署生产集群的方式，仅支持容器部署。接下来主要介绍通过 vstart.sh 和 cephadm 部署 crimson 集群的方式。以下测试基于 v19.2.1 版本进行。 一、vstart.sh 搭建集群vstart.sh 常用于在开发环境环境中快速搭建集群，且在部署集群前我们需要编译出对应的二进制包。由于编译环境可能会有各种依赖缺失，版本异常等问题，这里推荐使用 bugwz&#x2F;ceph-images 中提供的 CentOS Stream 9 的编译打包环境。同时后续的集群的搭建也可以在容器内部进行。 搭建集群操作步骤如下:  软件编译: 使用开发容器镜像，编译对应的 ceph 代码，产出对应的二进制运行文件； 集群部署: 在开发容器内部使用...</div></div></div></a><a class="pagination-related" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-01</div><div class="info-item-2">Ceph Crimson 设计实现深入解析</div></div><div class="info-2"><div class="info-item-1">Crimson 是 Crimson OSD 的代码名称，它是下一代用于多核心可扩展性的 OSD 。它通过快速网络和存储设备提高性能，采用包括 DPDK 和 SPDK 的顶级技术。BlueStore 继续支持 HDD 和 SSD。Crimson 旨在与早期版本的 OSD 守护进程与类 Ceph OSD 兼容。 Crimson 基于 SeaStar C++ 框架构建，是核心 Ceph 对象存储守护进程 OSD 组件的新实现，并替换了 Ceph OSD 。Crimson OSD 最小化延迟并增加 CPU 处理器用量。它使用高性能异步 IO 和新的线程架构，旨在最小化上下文切换和用于跨通信的操作间的线程通信。 以下分析基于 v19.2.1 进行分析。 一、架构对比Ceph OSD 是 Ceph 集群的一部分，负责通过网络提供对象访问、维护冗余和高可用性，并将对象持久化到本地存储设备。作为 Classic OSD 的重写版本，Crimson OSD 从客户端和其他 OSD 的角度兼容现有的 RADOS 协议，提供相同的接口和功能。Ceph OSD 的模块（例如 Messenger、OSD...</div></div></div></a><a class="pagination-related" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-25</div><div class="info-item-2">Ceph QoS 机制深入分析</div></div><div class="info-2"><div class="info-item-1">一、CephFS QoS社区的相关实现：  基于 tokenbucket 算法的目录 QoS : https://github.com/ceph/ceph/pull/29266 基于 dmclock 算法的 subvolume QoS : 来自日本的 line 公司提出的想法，https://github.com/ceph/ceph/pull/38506 ， https://github.com/ceph/ceph/pull/52147  1.1、基于 TokenBucket 算法的目录 QoS该实现并未合并到主分支。  相关材料：  社区的原始PR: https://github.com/ceph/ceph/pull/29266  实现特点：  基于 TokenBucketThrottle 类在客户端侧实现的 TokenBucket 类型的 QoS，用于约束每个独立的客户端的访问请求； QoS 的限制粒度为每个独立的客户端，没有全局的QoS限制； 用于限制目录级别的操作 QoS； 支持 IOPS 和 BPS 的 QoS 限制，且支持突发流量； 仅支持 FUSE...</div></div></div></a><a class="pagination-related" href="/2023/06/01/ceph-test/" title="Ceph 集群性能测试工具详解"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-01</div><div class="info-item-2">Ceph 集群性能测试工具详解</div></div><div class="info-2"><div class="info-item-1">本文详细介绍了包括 rados bench、rbd bench、dd 、fio 、vdbench 、mdtest 、iozone、cosbench、cbt 等测试工具对于 Ceph 集群的性能压测的使用。对于每个工具都提供了压测命令参数、示例命令等使用说明，实现了对 Ceph 块存储、文件存储、对象存储、rados 对象存储等存储类别的性能压测。文中重点阐述了各命令的使用格式、基本功能和参数选择，为用户在 Ceph 环境中进行性能评估提供了实用指南。 一、rados bench以下基于 v19.2.1 版本进行测试。 用途:  测试 ceph rados 对象存储性能；  1.1、测试配置参数命令格式: rados bench $seconds $type [args...]  $seconds : 压测运行时间； $type : 压测类型，可选值为 write&#x2F;seq&#x2F;rand （分别代表写&#x2F;连续读&#x2F;随机读）； -p : 指定压测的目标 pool ； -b : 只有当压测类型为 write 时可用，用于设置写入 block...</div></div></div></a><a class="pagination-related" href="/2023/06/30/ceph-crush/" title="Ceph CRUSH 设计实现剖析"><img class="cover" src="/assets/images/bg/ceph.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-30</div><div class="info-item-2">Ceph CRUSH 设计实现剖析</div></div><div class="info-2"><div class="info-item-1">CRUSH（Controlled Replication Under Scalable Hashing）是 Ceph 存储系统中用于数据分布和复制的算法。关于 CRUSH 的论文解析参考: 译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data 。CRUSH map 是 Ceph 集群中一个关键的配置组件，它定义了数据如何在集群的物理硬件上分布。 CRUSH 算法使得 Ceph 能够在无需中心化或者分布式元数据管理器的情况下，高效、可靠地进行数据复制和恢复。 一、CRUSH map 解析CRUSH map 包含了集群的层次结构和各种规则，这些规则定义了数据应该如何在集群中分布。 CRUSH map 主要包含以下几个部分：  Tunables : 一组可用于调整 CRUSH 算法行为的参数。 Devices : 定义集群中所有可用的存储设备的列表。 Types : 定义存储层次结构中的不同层级类型。 Buckets : 组织和管理存储设备（如 OSDs ）的逻辑容器。 Rules :...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">bugwz</div><div class="author-info-description">持续学习，持续进步</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">128</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">134</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/bugwz" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%BB%8B%E7%BB%8D"><span class="toc-text">一、介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1%E3%80%81Ceph-CSI-%E4%BB%8B%E7%BB%8D"><span class="toc-text">1.1、Ceph CSI 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2%E3%80%81%E8%B5%84%E6%BA%90%E6%8B%93%E6%89%91%E6%83%85%E5%86%B5"><span class="toc-text">1.2、资源拓扑情况</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E9%85%8D%E7%BD%AE-Ceph-%E7%8E%AF%E5%A2%83"><span class="toc-text">二、配置 Ceph 环境</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE%E7%8E%AF%E5%A2%83"><span class="toc-text">2.1、初始化数据访问环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E3%80%81%E6%96%B0%E5%A2%9E%E8%AE%BF%E9%97%AE%E5%AF%86%E9%92%A5"><span class="toc-text">2.2、新增访问密钥</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3%E3%80%81%E6%8C%82%E8%BD%BD-Ceph-%E6%9C%8D%E5%8A%A1"><span class="toc-text">2.3、挂载 Ceph 服务</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%90%AD%E5%BB%BA-K8S-%E9%9B%86%E7%BE%A4"><span class="toc-text">三、搭建 K8S 集群</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E3%80%81%E5%AE%89%E8%A3%85%E5%9F%BA%E7%A1%80%E5%B7%A5%E5%85%B7"><span class="toc-text">3.1、安装基础工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2%E3%80%81%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4"><span class="toc-text">3.2、搭建集群</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E9%83%A8%E7%BD%B2-CSI-%E6%9C%8D%E5%8A%A1"><span class="toc-text">四、部署 CSI 服务</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1%E3%80%81%E9%83%A8%E7%BD%B2-CephFS-CSI-%E6%9C%8D%E5%8A%A1"><span class="toc-text">4.1、部署 CephFS CSI 服务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2%E3%80%81%E9%83%A8%E7%BD%B2-CephRBD-CSI-%E6%9C%8D%E5%8A%A1"><span class="toc-text">4.2、部署 CephRBD CSI 服务</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81CephFS-%E5%AF%B9%E6%8E%A5-CSI"><span class="toc-text">五、CephFS 对接 CSI</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1%E3%80%81%E5%88%9B%E5%BB%BASecret"><span class="toc-text">5.1、创建Secret</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2%E3%80%81%E9%9D%99%E6%80%81%E9%85%8D%E7%BD%AE"><span class="toc-text">5.2、静态配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-1%E3%80%81%E5%88%9B%E5%BB%BA%E5%AD%90%E5%8D%B7"><span class="toc-text">5.2.1、创建子卷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-2%E3%80%81%E5%88%9B%E5%BB%BAPV"><span class="toc-text">5.2.2、创建PV</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-3%E3%80%81%E5%88%9B%E5%BB%BAPVC"><span class="toc-text">5.2.3、创建PVC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-4%E3%80%81%E5%88%9B%E5%BB%BAPod"><span class="toc-text">5.2.4、创建Pod</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3%E3%80%81%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AE"><span class="toc-text">5.3、动态配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-1%E3%80%81%E5%88%9B%E5%BB%BA%E5%AD%90%E5%8D%B7%E7%BB%84"><span class="toc-text">5.3.1、创建子卷组</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-2%E3%80%81%E5%88%9B%E5%BB%BAStorageClass"><span class="toc-text">5.3.2、创建StorageClass</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-3%E3%80%81%E5%88%9B%E5%BB%BAPVC"><span class="toc-text">5.3.3、创建PVC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-4%E3%80%81%E5%88%9B%E5%BB%BAPod"><span class="toc-text">5.3.4、创建Pod</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81CephRBD-%E5%AF%B9%E6%8E%A5-CSI"><span class="toc-text">六、CephRBD 对接 CSI</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1%E3%80%81%E5%88%9B%E5%BB%BASecret"><span class="toc-text">6.1、创建Secret</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2%E3%80%81%E9%9D%99%E6%80%81%E9%85%8D%E7%BD%AE"><span class="toc-text">6.2、静态配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-1%E3%80%81%E5%88%9B%E5%BB%BAPV"><span class="toc-text">6.2.1、创建PV</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2%E3%80%81%E5%88%9B%E5%BB%BAPVC"><span class="toc-text">6.2.2、创建PVC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-3%E3%80%81%E5%88%9B%E5%BB%BAPod"><span class="toc-text">6.2.3、创建Pod</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3%E3%80%81%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AE"><span class="toc-text">6.3、动态配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-1%E3%80%81%E5%88%9B%E5%BB%BAStorageClass"><span class="toc-text">6.3.1、创建StorageClass</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-2%E3%80%81%E5%88%9B%E5%BB%BAPVC"><span class="toc-text">6.3.2、创建PVC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-3%E3%80%81%E5%88%9B%E5%BB%BAPod"><span class="toc-text">6.3.3、创建Pod</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-text">七、参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 设计实现深入解析"/></a><div class="content"><a class="title" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析">Ceph Crimson 设计实现深入解析</a><time datetime="2025-05-31T16:00:00.000Z" title="发表于 2025-06-01 00:00:00">2025-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 集群搭建指南"/></a><div class="content"><a class="title" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南">Ceph Crimson 集群搭建指南</a><time datetime="2025-01-11T16:00:00.000Z" title="发表于 2025-01-12 00:00:00">2025-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph QoS 机制深入分析"/></a><div class="content"><a class="title" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析">Ceph QoS 机制深入分析</a><time datetime="2024-10-24T16:00:00.000Z" title="发表于 2024-10-25 00:00:00">2024-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/01/gpfs-csi/" title="GPFS CSI 对接 K8S 指南"><img src="/assets/images/bg/gpfs.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GPFS CSI 对接 K8S 指南"/></a><div class="content"><a class="title" href="/2024/09/01/gpfs-csi/" title="GPFS CSI 对接 K8S 指南">GPFS CSI 对接 K8S 指南</a><time datetime="2024-08-31T16:00:00.000Z" title="发表于 2024-09-01 00:00:00">2024-09-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/01/gpfs/" title="GPFS 集群部署与运维记录"><img src="/assets/images/bg/gpfs.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GPFS 集群部署与运维记录"/></a><div class="content"><a class="title" href="/2024/08/01/gpfs/" title="GPFS 集群部署与运维记录">GPFS 集群部署与运维记录</a><time datetime="2024-07-31T16:00:00.000Z" title="发表于 2024-08-01 00:00:00">2024-08-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By bugwz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: '6af3be16b94cec39bcf6',
      clientSecret: '13a5202ff773ffcea6300b6c8ff25f455566737c',
      repo: 'bugwz.github.io',
      owner: 'bugwz',
      admin: ['bugwz'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || 'fb3e2202b44086f555cc572c648492ec'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="docsearch-wrap"><div id="docsearch" style="display:none"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css/dist/style.min.css"/><script src="https://cdn.jsdelivr.net/npm/@docsearch/js/dist/umd/index.min.js"></script><script>(() => {
  docsearch(Object.assign({
    appId: 'PFB3WGSSCO',
    apiKey: '3e9cd446e41d93f2f130b91698b699f7',
    indexName: 'bugwz',
    container: '#docsearch',
    placeholder: '请输入要搜索的内容',
  }, {"maxResultsPerGroup":10}))

  const handleClick = () => {
    document.querySelector('.DocSearch-Button').click()
  }

  const searchClickFn = () => {
    btf.addEventListenerPjax(document.querySelector('#search-button > .search'), 'click', handleClick)
  }

  searchClickFn()
  window.addEventListener('pjax:complete', searchClickFn)
})()</script></div></div></body></html>