<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="译 - Gorilla: A Fast, Scalable, In-Memory Time Series Database"><meta name="keywords" content="Paper,Gorilla,TSDB"><meta name="author" content="bugwz"><meta name="copyright" content="bugwz"><title>译 - Gorilla: A Fast, Scalable, In-Memory Time Series Database | 咕咕</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?e0f19b930da2077e8b159cbe0c8eb5e0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-100348048-1', 'auto');
ga('send', 'pageview');</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容: ${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.3.0'
} </script><meta name="generator" content="Hexo 6.3.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="toggle-sidebar-info text-center"><span data-toggle="文章详情">站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">阅读进度</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%8B%E7%BB%8D"><span class="toc-text">1、介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E8%83%8C%E6%99%AF%E5%92%8C%E8%A6%81%E6%B1%82"><span class="toc-text">2、背景和要求</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1%E3%80%81%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%EF%BC%88ODS%EF%BC%89"><span class="toc-text">2.1、操作数据存储（ODS）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-1%E3%80%81%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E8%AF%BB%E5%8F%96%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98"><span class="toc-text">2.1.1、监控系统读取性能问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2%E3%80%81Gorilla%E7%9A%84%E9%9C%80%E6%B1%82"><span class="toc-text">2.2、Gorilla的需求</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E4%B8%8E-TSDB-%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">3、与 TSDB 系统的比较</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E3%80%81OpenTSDB"><span class="toc-text">3.1、OpenTSDB</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2%E3%80%81Whisper-Graphite"><span class="toc-text">3.2、Whisper (Graphite)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3%E3%80%81InfluxDB"><span class="toc-text">3.3、InfluxDB</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81Gorilla%E6%9E%B6%E6%9E%84"><span class="toc-text">4、Gorilla架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E3%80%81%E6%97%B6%E5%BA%8F%E5%8E%8B%E7%BC%A9"><span class="toc-text">4.1、时序压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1%E3%80%81-%E6%97%B6%E9%97%B4%E6%88%B3%E5%8E%8B%E7%BC%A9"><span class="toc-text">4.1.1、 时间戳压缩</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2%E3%80%81%E5%80%BC%E5%8E%8B%E7%BC%A9"><span class="toc-text">4.1.2、值压缩</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E3%80%81%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-text">4.2、内存数据结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E3%80%81%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-text">4.3、磁盘数据结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E3%80%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86"><span class="toc-text">4.4、故障处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81Gorilla%E7%9A%84%E6%96%B0%E5%B7%A5%E5%85%B7"><span class="toc-text">5、Gorilla的新工具</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1%E3%80%81%E5%85%B3%E8%81%94%E5%BC%95%E6%93%8E"><span class="toc-text">5.1、关联引擎</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2%E3%80%81%E5%9B%BE%E8%A1%A8"><span class="toc-text">5.2、图表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3%E3%80%81%E8%81%9A%E5%90%88"><span class="toc-text">5.3、聚合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E7%BB%8F%E9%AA%8C"><span class="toc-text">6、经验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1%E3%80%81%E5%AE%B9%E9%94%99"><span class="toc-text">6.1、容错</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2%E3%80%81%E7%BD%91%E7%AB%99%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5"><span class="toc-text">6.2、网站故障排查</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3%E3%80%81%E7%BB%8F%E9%AA%8C%E6%95%99%E8%AE%AD"><span class="toc-text">6.3、经验教训</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E6%9C%AA%E6%9D%A5%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="toc-text">7、未来的工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">8、总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9%E3%80%81%E8%87%B4%E8%B0%A2"><span class="toc-text">9、致谢</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10%E3%80%81%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-text">10、参考文献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-text">参考链接</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/assets/images/avatar.jpg"></div><div class="author-info__name text-center">bugwz</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">108</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">106</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友情链接</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://blog.nobug.in/">BugSniper</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ibytes.cn/">i字节</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://www.hozen.site/">李浩然的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.bugwz.com/paper.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">咕咕</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">首页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a></span></div><div id="post-info"><div id="post-title">译 - Gorilla: A Fast, Scalable, In-Memory Time Series Database</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-09-24</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p><a target="_blank" rel="noopener" href="http://www.vldb.org/pvldb/vol8/p1816-teller.pdf?spm=a2c6h.12873639.article-detail.7.69e9446b9wFFAw&file=p1816-teller.pdf">《Gorilla: A Fast, Scalable, In-Memory Time Series Database》</a> 这篇论文讲述了 <code>Fackbook</code> 在存储时序数据模型时的一些实践，重点讲述了他们内部的一款内存型的时序数据库 <code>Gorilla</code>。论文中通过使用 <code>Delta-Of-Delta</code> 和 <code>XOR</code> 方式分别对时序数据的时间戳以及浮点数据进行压缩编码，极大的节省了时序数据的存储开销，这也成为了业界时序数据库主流的数据编码压缩方式。这篇论文是时序数据库领域必读的一篇文章。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Large-scale internet services aim to remain highly available and responsive in the presence of unexpected failures. Providing this service often requires monitoring and analyzing tens of millions of measurements per second across a large number of systems, and one particularly effective solution is to store and query such measurements in a time series database (TSDB).</p>
<p>大型的互联网服务通常能够在出现意外故障时仍能保持高可用性和响应能力。 为了提供这种服务，通常需要时刻监控和分析系统中大量数据，一种特别高效的解决方案就是使用时序数据库 （TSDB） 对这些数据进行存储和查询。</p>
<p>A key challenge in the design of TSDBs is how to strike the right balance between efficiency, scalability, and reliability. In this paper we introduce Gorilla, Facebook’s inmemory TSDB. Our insight is that users of monitoring systems do not place much emphasis on individual data points but rather on aggregate analysis, and recent data points are of much higher value than older points to quickly detect and diagnose the root cause of an ongoing problem. Gorilla optimizes for remaining highly available for writes and reads, even in the face of failures, at the expense of possibly dropping small amounts of data on the write path. To improve query efficiency, we aggressively leverage compression techniques such as delta-of-delta timestamps and XOR’d floating point values to reduce Gorilla’s storage footprint by 10x.</p>
<p>时序数据库（TSDB）设计中的一个关键挑战是如何权衡效率、可扩展性以及可靠性。 在本文中，我们介绍了一款 Facebook 的内存型时序数据库（TSDB） - <code>Gorilla</code>。 我们认为对监控系统来说，相比于某一个数据点，用户更加重视于数据的聚合分析，并且在快速检测和诊断一个正在发生的问题时，新数据的价值通常会远远大于老数据。Gorilla 针对于高可用的读写进行了一些优化，当出现故障时可以通过牺牲少量的写数据来保障服务的整体可用性。为了提高查询效率，我们专门使用了一些压缩技术，例如使用 <code>Delta-Of-Delta</code> 来编码时间戳，使用 <code>XOR</code> 来编码浮点值，通过这种方式，我们将 Gorilla 的存储空间减少 10 倍左右。</p>
<p>This allows us to store Gorilla’s data in memory, reducing query latency by 73x and improving query throughput by 14x when compared to a traditional database (HBase) backed time series data. This performance improvement has unlocked new monitoring and debugging tools, such as time series correlation search and more dense visualization tools. Gorilla also gracefully handles failures from a single-node to entire regions with little to no operational overhead.</p>
<p>这使得我们能够将 Gorilla 的数据存储在内存中，相比于传统的时序数据库（HBase），查询延迟减少了 73 倍，查询吞吐量提高了 14 倍。 性能的提升解锁了新的监控和调试工具，例如时间序列关联搜索，更密集的可视化工具。 Gorilla 还可以优雅地应对单个节点到整个区域的故障，并且不会有额外的运维开销。</p>
<h2 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h2><p>Large-scale internet services aim to remain highly-available and responsive for their users even in the presence of unexpected failures. As these services have grown to support a global audience, they have scaled beyond a few systems running on hundreds of machines to thousands of individual systems running on many thousands of machines, often across multiple geo-replicated datacenters.</p>
<p>大型的互联网服务的目标是保持高可用，即使在故障的情况下也应该能够响应用户的请求。随着服务的发展，为了支持庞大的全球客户，需要将之前在数百台机器上运行的几个系统扩展到在数千台机器上运行的数千个单独的系统，它们通常还要跨越不同地域的数据中心。</p>
<p><img src="/assets/images/gorilla-cn-1.png" alt="图 1" loading="lazy"></p>
<p>Figure 1: High level overview of the ODS monitoring and alerting system, showing Gorilla as a writethrough cache of the most recent 26 hours of time series data.</p>
<p>图 1：ODS 监控和警报系统的高级概述（预览），展示了 Gorilla 作为直写缓存（五种缓存策略中的其中一种）的最近 26 小时的时序数据。</p>
<p>An important requirement to operating these large scale services is to accurately monitor the health and performance of the underlying system and quickly identify and diagnose problems as they arise. Facebook uses a time series database (TSDB) to store system measuring data points and provides quick query functionalities on top. We next specify some of the constraints that we need to satisy for monitoring and operating Facebook and then describe Gorilla, our new inmemory TSDB that can store tens of millions of datapoints (e.g., CPU load, error rate, latency etc.) every second and respond queries over this data within milliseconds.</p>
<p>操作这些大型服务的一个重要需求就是要能够准确的监控底层系统的运行状况和性能，并在出现问题时能够快速的识别和诊断。 Facebook 使用时序数据库 （TSDB） 来存储系统的测量数据，并在上层提供了快速查询的功能。 后来我们在监控和运维 Facebook 时遇到了一些限制（约束），于是我们设计了 Gorilla，这是一个新的内存型时序数据库（TSDB），它每秒可以存储数千万个数据点（例如，CPU 负载、错误率、延迟等），并且能够实现毫秒级的数据查询。</p>
<p><strong>Writes dominate.</strong> Our primary requirement for a TSDB is that it should always be available to take writes. As we have hundreds of systems exposing multiple data items, the write rate might easily exceed tens of millions of data points each second. In constrast, the read rate is usually a couple orders of magnitude lower as it is primarily from automated systems watching ’important’ time series, data visualization systems presenting dashboards for human consumption, or from human operators wishing to diagnose an observed problem.</p>
<p><strong>大量（主导地位）的写入。</strong>我们对于时序数据库（TSDB）的主要需求是它应该始终可以接受写操作。由于我们有数百个暴露了众多的数据项的系统，因此可能很容易就会超过每秒数千万的写入速率。 相比之下，读取的速率通常要低好几个数量级，读取主要来自于观测 “重要” 时序数据的自动化系统，供人使用的数据可视化仪表盘，诊断线上问题时的人为操作。</p>
<p><strong>State transitions.</strong> We wish to identify issues that emerge from a new software release, an unexpected side effect of a configuration change, a network cut and other issues that result in a significant state transition. Thus, we wish for our TSDB to support fine-grained aggregations over short-time windows. The ability to display state transitions within tens of seconds is particularly prized as it allows automation to quickly remediate problems before they become wide spread.</p>
<p><strong>状态转换。</strong>我们希望找出新软件发布中出现的问题，配置更改导致的副作用，网络中断 和 其他导致重大的状态转变的问题。 因此，我们希望我们的时序数据库 （TSDB） 能够支持在很短的时间窗口内实现细粒度的聚合计算。这种在几十秒内能够迅速检测到系统状态发生变化的能力是非常有价值的，因为（基于）它就可以在故障扩散之前进行自动化的修复。</p>
<p><strong>High availability.</strong> Even if a network partition or other failure leads to disconnection between different datacenters, systems operating within any given datacenter ought to be able to write data to local TSDB machines and be able to retrieve this data on demand.</p>
<p><strong>高可用性。</strong>即使网络分区或其他故障导致不同数据中心之间断开连接，任何数据中心都应该能够将数据写入本地的时序数据库（TSDB）机器中，并能够按需查询这些数据。</p>
<p><strong>Fault tolerance.</strong> We wish to replicate all writes to multiple regions so we can survive the loss of any given datacenter or geographic region due to a disaster.</p>
<p><strong>容错。</strong>我们希望将所有的写操作复制到多个区域中，这样我们就可以容忍任何数据中心或不同地域的节点发生故障（灾难）。</p>
<p>Gorilla is Facebook’s new TSDB that satisfies these constraints. Gorilla functions as a write-through cache of the most recent data entering the monitoring system. We aim to ensure that most queries run within 10’s of milliseconds.</p>
<p>Gorilla 是 Facebook 研发的新型时序数据库 （TSDB），它满足了这些约束（限制）。 Gorilla 使用直写缓存（Write-Through Cache）的方式来记录写入监控系统的最新数据。 我们的目标是确保大多数查询在 10 毫秒内完成。</p>
<p>The insight in Gorilla’s design is that users of monitoring systems do not place much emphasis on individual data points but rather on aggregate analysis. Additionally, these systems do not store any user data so traditional ACID guarantees are not a core requirement for TSDBs. However, a high percentage of writes must succeed at all times, even in the face of disasters that might render entire datacenters unreachable. Additionally, recent data points are of higher value than older points given the intuition that knowing if a particular system or service is broken right now is more valuable to an operations engineer than knowing if it was broken an hour ago. Gorilla optimizes for remaining highly available for writes and reads, even in the face of failures, at the expense of possibly dropping small amounts of data on the write path.</p>
<p>Gorilla 独特的设计点在于，用户在使用监控系统时通常不会重视单独的一个数据点，而是更在意整体的数据聚合分析。此外，这些系统不会存储任何用户数据，因此传统的 ACID 特性并不是时序数据库（TSDB） 的核心需求。但是，即使在面对可能导致整个数据中心都无法访问的灾难时，大部分的写操作也必须始终执行成功。此外，最近的数据点要比旧的数据点更有价值，通常对于运维工程师来说，了解一个特定的系统或者服务现在是否存在故障比知道之前它一个小时前是否存在故障更有价值。Gorilla 对于读写的可用性也做了一些优化，即使在出现故障时，也只会在仅仅丢失少量的数据的同时来保证整体的可用性。</p>
<p>The challenge then arises from high data insertion rate, total data quantity, real-time aggregation, and reliability requirements. We addressed each of these in turn. To address the first couple requirements, we analyzed the Operational Data Store (ODS) TSDB, an older monitoring system that was widely used at Facebook. We noticed that at least 85% of all queries to ODS was for data collected in the past 26 hours. Further analysis allowed us to determine that we might be able to serve our users best if we could replace a disk-based database with an in-memory database. Further, by treating this in-memory database as a cache of the persistent disk-based store, we could achieve the insertion speed of an in-memory system with the persistence of a disk based database.</p>
<p>挑战主要来自于高性能的写入、数据总量、实时聚合和可靠性的要求。我们依次地解决了这些问题。为了解决前两个需求，我们分析了操作数据存储 （ODS）时序数据库（TSDB），这是一个在 Facebook 上广泛使用的比较老的监控系统。我们注意到，对 ODS 的所有查询中，至少有 85% 的查询是访问过去 26 小时内收集的数据。进一步的分析使我们能够确定，如果我们能够用一个内存数据库代替基于磁盘的数据库，我们也许能够为用户提供最好的服务。此外，通过将这个内存数据库视为基于磁盘的持久存储的缓存，我们可以在基于磁盘的数据库持久存储的情况下实现内存系统的插入速度。</p>
<p>As of Spring 2015, Facebook’s monitoring systems generate more than 2 billion unique time series of counters, with about 12 million data points added per second. This represents over 1 trillion points per day. At 16 bytes per point, the resulting 16TB of RAM would be too resource intensive for practical deployment. We addressed this by repurposing an existing XOR based floating point compression scheme to work in a streaming manner that allows us to compress time series to an average of 1.37 bytes per point, a 12x reduction in size.</p>
<p>截至到 2015 年春季，Facebook 的监控系统产生了超过 20 亿个唯一的时间序列计数器，每秒增加大约1200万个数据点。这意味着每天会增加超过 1 万亿个数据点。假设每个数据点占用 16字节，每天就需要 16TB 的内存空间，这对于实际的部署而言是巨大的资源消耗。我们通过重用现有的基于异或（XOR）的浮点数压缩方案来解决这个问题，该方案以流的方式工作，允许我们将时间序列压缩到平均每个点 1.37 字节，大小减少了 12 倍。</p>
<p>We addressed the reliability requirements by running multiple instances of Gorilla in different datacenter regions and streaming data to each without attempting to guarantee consistency. Read queries are directed at the closest available Gorilla instance. Note that this design leverages our observation that individual data points can be lost without compromising data aggregation unless there’s significant discrepancy between the Gorilla instances.</p>
<p>我们通过在不同的数据中心区域运行 Gorilla 的多个实例并向每个实例传输数据来满足可靠性需求，但不会试图去保证一致性。读查询会直接路由到最近的可用 Gorilla 实例上。请注意，这种设计基于我们的所见所闻，即单个数据点的丢失并不会影响数据聚合，除非 Gorilla 实例之间存在显著差异。</p>
<p>Gorilla is currently running in production at Facebook and is used daily by engineers for real-time firefighting and debugging in conjunction with other monitoring and analysis systems like Hive [27] and Scuba [3] to detect and diagnose problems.</p>
<p>Gorilla 目前部署在 Facebook 的生产环境中，工程师们把它当做日常的实时数据工具，并协同其它监控和分析系统（例如Hive、Scuba）一起检测和诊断问题。</p>
<h2 id="2、背景和要求"><a href="#2、背景和要求" class="headerlink" title="2、背景和要求"></a>2、背景和要求</h2><h3 id="2-1、操作数据存储（ODS）"><a href="#2-1、操作数据存储（ODS）" class="headerlink" title="2.1、操作数据存储（ODS）"></a>2.1、操作数据存储（ODS）</h3><p>Operating and managing Facebook’s large infrastructure comprised of hundreds of systems distributed across multiple data centers would be very difficult without a monitoring system that can track their health and performance. The Operational Data Store (ODS) is an important portion of the monitoring system at Facebook. ODS comprises of a time series database (TSDB), a query service, and a detection and alerting system. ODS’s TSDB is built atop the HBase storage system as described in [26]. Figure 1 represents a high-level view of how ODS is organized. Time series data from services running on Facebook hosts is collected by the ODS write service and written to HBase.</p>
<p>如果没有可以跟踪其健康状况和性能的监控系统，运营和管理分布在多个数据中心的数百个系统的 Facebook 大型基础设施将会变得非常困难。操作数据存储（ODS）是 Facebook 监控系统的重要组成部分。ODS 由时间序列数据库 （TSDB）、查询服务和检测警报系统组成。 ODS 的 TSDB 构建在 HBase 存储系统之上，如 [26] 中所述。 图 1 展示了 ODS 组织方式的高级视图。 来自 Facebook 主机上运行的服务的时间序列数据由 ODS 写入服务收集并写入 HBase。</p>
<p>There are two consumers of ODS time series data. The first consumers are engineers who rely on a charting system that generates graphs and other visual representations of time series data from ODS for interactive analysis. The second consumer is our automated alerting system that read counters off ODS, compares them to preset thresholds for health, performance and diagnostic metrics and fires alarms to oncall engineers and automated remediation systems.</p>
<p>ODS 时间序列数据有两个消费者。 第一个消费者是工程师，他们依赖图表系统从 ODS 生成图形和其他时间序列数据的可视化表示，以便于进行交互式分析。 第二个消费者是我们的自动警报系统，它可以读取 ODS 的计数器，将它们与预设的健康、性能和诊断指标阈值进行比较，并向值班（oncall）工程师和自动修复系统发出警报。</p>
<h4 id="2-1-1、监控系统读取性能问题"><a href="#2-1-1、监控系统读取性能问题" class="headerlink" title="2.1.1、监控系统读取性能问题"></a>2.1.1、监控系统读取性能问题</h4><p>In early 2013, Facebook’s monitoring team realized that its HBase time series storage system couldn’t scale handle future read loads. While the average read latency was acceptable for interactive charts, the 90th percentile query time had increased to multiple seconds blocking our automation. Additionally, users were self-censoring their usage as interactive analysis of even medium-sized queries of a few thousand time series took tens of seconds to execute. Larger queries executing over sparse datasets would timeout as the HBase data store was tuned to prioritize writes. While our HBase-based TSDB was inefficient, we quickly rejected wholesale replacement of the storage system as ODS’s HBase store held about 2 PB of data [5]. Facebook’s data warehouse solution, Hive, was also unsuitable due to its already orders of magnitude higher query latency comparing to ODS, and query latency and efficiency were our main concerns [27].</p>
<p>2013 年初，Facebook 的监控团队意识到其 HBase 时间序列存储系统无法进行扩展来支撑未来的读取负载。 虽然交互式图表的平均读取延迟是可以接受的，但 90% 的查询时间已经到达到数秒，这阻碍了我们的自动化。 此外，用户对自己的使用情况会进行自我审查，即使是对几千个时间序列的中型查询的交互式分析也需要数十秒才能执行完成。 由于 HBase 数据存储已调整为写优先，在稀疏数据集上执行的较大查询将超时。虽然我们基于 HBase 的 TSDB 效率低下，但我们很快就拒绝了存储系统的大规模更换，因为 ODS 的 HBase 存储拥有大约 2 PB 的数据 [5]。 Facebook 的数据仓库解决方案 Hive 也不适合，因为它的查询延迟比 ODS 高出了几个数量级，而查询延迟和效率又是我们主要关心的问题 [27]。</p>
<p>We next turned our attention to in-memory caching. ODS already used a simple read-through cache but it was primarily targeted at charting systems where multiple dashboards shared the same time series. A particularly difficult scenario was when dashboards queried for the most recent data point, missed in the cache, and then issued requests directly to the HBase data store. We also considered a separate Memcache [20] based write-through cache but rejected it as appending new data to an existing time series would require a read&#x2F;write cycle, causing extremely high traffic to the memcache server. We needed a more efficient solution.</p>
<p>接下来我们将注意力转向了内存缓存。 ODS 已经使用了一个简单的直读缓存，但它主要针对多个仪表板共享相同时间序列的图表系统。 一个特别困难的场景是当仪表板查询最近的数据点时，如果缓存中不存在，它将直接向 HBase 数据存储发出请求。 我们还考虑了一个单独的基于 Memcache [20] 的直写缓存，但最后被否决了，因为将新数据追加到现有的时间序列将需要一个读&#x2F;写周期，从而导致 Memcache 服务器的流量非常大。 我们需要一个更有效的解决方案。</p>
<h3 id="2-2、Gorilla的需求"><a href="#2-2、Gorilla的需求" class="headerlink" title="2.2、Gorilla的需求"></a>2.2、Gorilla的需求</h3><p>With these considerations, we determined the following requirements for a new service:</p>
<ul>
<li>2 billion unique time series identified by a string key.</li>
<li>700 million data points (time stamp and value) added per minute.</li>
<li>Store data for 26 hours.</li>
<li>More than 40,000 queries per second at peak.</li>
<li>Reads succeed in under one millisecond.</li>
<li>Support time series with 15 second granularity (4 points per minute per time series).</li>
<li>Two in-memory, not co-located replicas (for disaster recovery capacity).</li>
<li>Always serve reads even when a single server crashes.</li>
<li>Ability to quickly scan over all in memory data.</li>
<li>Support at least 2x growth per year.</li>
</ul>
<p>考虑到这些因素，我们确定了新服务的以下要求（需要满足）：</p>
<ul>
<li>20 亿个由字符串键标识的唯一时间序列；</li>
<li>每分钟添加 7 亿个数据点（时间戳和值）；</li>
<li>存储数据 26 小时；</li>
<li>峰值时每秒超过 40,000 个查询；</li>
<li>在一毫秒内读取成功；</li>
<li>支持 15 秒粒度的时间序列（每个时间序列每分钟 4 个点）；</li>
<li>两个内存中的非共存副本（用于灾难恢复能力）；</li>
<li>即使单个服务器崩溃，也始终提供读取服务；</li>
<li>能够快速扫描所有内存数据；</li>
<li>支持每年至少 2 倍的增长；</li>
</ul>
<p>After a brief comparison with other TSDB systems in Section 3, we detail the implementation of Gorilla in Section 4, first discussing its new time stamp and data value compression schemes in Section 4.1. We then describe how Gorilla remains highly available despite single node failures and region-wide disasters in Section 4.4. We describe how Gorilla has enabled new tools in Section 5. We close out by describing our experience developing and deploying Gorilla in Section 6.</p>
<p>在第 3 节与其他 TSDB 系统进行了简要比较之后，我们在第 4 节详细介绍了 Gorilla 的实现，首先在第 4.1 节讨论了其新的时间戳和数据值压缩方案。 然后，我们将在 4.4 节中描述 Gorilla 如何在单节点故障和区域范围内发生灾难的情况下保持高可用性。 我们将在第 5 节中描述 Gorilla 如何启用新工具。最后我们在第 6 节中描述我们开发和部署 Gorilla 的经验。</p>
<h2 id="3、与-TSDB-系统的比较"><a href="#3、与-TSDB-系统的比较" class="headerlink" title="3、与 TSDB 系统的比较"></a>3、与 TSDB 系统的比较</h2><p>There are a number of publications detailing data mining techniques to search, classify, and cluster enormous amounts of time series data efficiently [8, 23, 24]. These systems demonstrate many of the uses of examining time series data, from clustering and classifying [8, 23] to anomaly detection [10, 16] to indexing the time series [9, 12, 24]. However, there are fewer examples detailing systems able to gather and store massive amounts of time series data in real-time. Gorilla’s design, focusing on reliable real-time monitoring of production systems, makes stand out compared to other TSDBs. Gorilla occupies an interesting design space, where being available for reads and writes in the face of failures prioritized over availability of any older data.</p>
<p>有很多出版物详细介绍了通过数据挖掘技术来进行搜索、分类和高效聚合大量的时间序列数据 [8, 23, 24]。这些系统描述了研究时间序列数据的许多用途，从聚类和分类 [8, 23] 到异常检测 [10, 16] 到索引时间序列 [9, 12, 24]。但是，很少有能够进行实时收集和存储大量时间序列数据的系统的详细示例。 Gorilla 的设计专注于对生产系统进行可靠的实时监控，在与其他 TSDB 的比较中脱颖而出。Gorilla 有一个有趣的设计：当出现故障时仍然可以进行读取和写入，该优先级高于任何旧数据的可用性。</p>
<p>Since Gorilla was designed from the beginning to store all data in memory, its in-memory structure is also different from existing TSDBs. However, if one views Gorilla as an intermediate store for in-memory storage of time series data in front of another on-disk TSDB, then Gorilla could be used as a write through cache for any TSDB (with relatively simple modifications). Gorilla’s focus on speed of ingest and horizontal scaling is similar to existing solutions.</p>
<p>由于 Gorilla 从一开始的设计初衷就是将数据存储在内存中，因此它的内存结构不同于现有的 TSDB。 但是，如果将 Gorilla 看作一个中间存储，用来在基于磁盘的 TSDB 的上层的内存中存储时间序列数据，那么 Gorilla 可以通过相对简单的修改来用作任何 TSDB 的直写缓存（Write-Through Cache）。Gorilla 对摄取速度和水平扩展的关注与现有解决的方案类似。</p>
<h3 id="3-1、OpenTSDB"><a href="#3-1、OpenTSDB" class="headerlink" title="3.1、OpenTSDB"></a>3.1、OpenTSDB</h3><p>OpenTSDB is based on HBase [28], and very closely resembles the ODS HBase storage layer we use for long term data. Both systems rely on similar table structures, and have come to similar conclusions for optimization and horizontal scalability [26, 28]. However, we had found that supporting the volume of queries necessary to build advanced monitoring tools required faster queries than a disk based store can support.</p>
<p>OpenTSDB 基于 HBase [28]，它和用于存储长期数据的 ODS HBase 存储层非常相似。 两个系统都拥有相似的表结构，并且在优化和水平扩展性方面都有着相似的结论 [26, 28]。 但是，我们发现为了支持构建高级监控工具的大量查询，我们需要比基于磁盘的存储所能支持的更快的查询。</p>
<p>Unlike OpenTSDB, the ODS HBase layer does do time roll up aggregation for older data to save space. This results in older, archived data having lower time granularity compared to more recent data in ODS, while OpenTSDB will keep the full resolution data forever. We have found that cheaper long time period queries and space savings are worth the loss of precision.</p>
<p>不同于 OpenTSDB ，ODS HBase 存储层会定期的对老数据进行聚合以节省空间。这导致 ODS 中的老数据相比于新数据的时间间隔粒度更大，而 OpenTSDB 永远保留完整分辨率的数据。 我们发现从成本较低的长时间片查询以及空间的节省上来说，数据精度的丢失是可以接受的。</p>
<p>OpenTSDB also has a richer data model for identifying time series. Each time series is identified by a set of arbitrary key-value pairs, called tags [28]. Gorilla identifies time series with a single string key and relies on higher level tools to extract and identify time series meta data.</p>
<p>OpenTSDB 还有更丰富的数据模型来识别时间序列。 每个时间序列由一组任意键值对标识，称为标签 [28]。 Gorilla 使用单个字符串键识别时间序列，并依赖更高级别的工具来提取和识别时间序列元数据。</p>
<h3 id="3-2、Whisper-Graphite"><a href="#3-2、Whisper-Graphite" class="headerlink" title="3.2、Whisper (Graphite)"></a>3.2、Whisper (Graphite)</h3><p>Graphite stores time series data on local disk in the Whisper format, a Round Robin Database (RRD) style database [1]. This file format expects time series data to be timestamped at regular intervals, and does not support jitter in the time series. While Gorilla does work more efficiently if data are timestamped at regular intervals, it can handle arbitrary and changing intervals. With Whisper, each time series is stored in a separate file, and new samples overwrite old ones after a certain amount of time [1]. Gorilla works in a similar fashion, only holding the most recent day of data in memory. However, with its focus on on-disk storage, query latency using Graphite&#x2F;Whisper is not fast enough to match the requirements for Gorilla.</p>
<p>Graphite 将时间序列数据以 Whisper 格式存储在本地磁盘上，这是一种循环数据库 (RRD) 风格的数据库 [1]。 这种文件格式要求时间序列数据是按照固定的时间间隔产生的，不支持间隔跳动的时间序列。 如果数据时间戳的时间间隔固定，Gorilla 确实可以更有效地工作，但它也支持处理任意和不断变化的时间间隔。 使用 Whisper，每个时间序列都存储在一个单独的文件中，并且新样本数据会在一定时间后覆盖旧样本数据 [1]。 Gorilla 以类似的方式工作，只在内存中保存最近一天的数据。 然而，由于 Graphite&#x2F;Whisper 专注于磁盘存储，因此它的查询延迟无法满足 Gorilla 的要求。</p>
<h3 id="3-3、InfluxDB"><a href="#3-3、InfluxDB" class="headerlink" title="3.3、InfluxDB"></a>3.3、InfluxDB</h3><p>InfluxDB is a new open-source time series database, with an even richer data model than OpenTSDB. Each event in a time series can have a full set of meta data. While this flexibility does allow for rich data, it necessarily results in larger disk usage than schemes that only store time series within the database [2].</p>
<p>InfluxDB 是一个全新的开源时序数据库，其数据模型比 OpenTSDB 更加丰富。 时间序列中的每个事件都可以拥有一整套元数据。 虽然这种灵活性允许其使用丰富的数据，但是相比于仅在数据库中存储时间序列的方案来说，它必然会导致更大的磁盘使用量 [2]。</p>
<p>InfluxDB also contains the code to build it as a distributed storage cluster, allowing users to scale horizontally without the overhead of managing an HBase&#x2F;Hadoop cluster [2]. At Facebook, we already have dedicated teams to support our HBase installations, so using it for ODS did not involve a large extra investment in resources. Like other systems, InfluxDB keeps data on-disk, leading to slower queries than if data are kept in memory.</p>
<p>InfluxDB 还包含一些支持构建为分布式存储集群的代码，它允许用户水平扩展而无需管理 HBase&#x2F;Hadoop 集群 [2]。 在 Facebook，我们已经有专门的团队来支持 HBase 的安装，因此将其用于 ODS 并不需要大量额外的资源投资。 与其他系统一样，InfluxDB 将数据保存在磁盘上，导致查询速度相比将数据保存在内存中要慢。</p>
<h2 id="4、Gorilla架构"><a href="#4、Gorilla架构" class="headerlink" title="4、Gorilla架构"></a>4、Gorilla架构</h2><p>Gorilla is an in-memory TSDB that functions as a writethrough cache for monitoring data written to an HBase data store. The monitoring data stored in Gorilla is a simple 3- tuple of a string key, a 64 bit time stamp integer and a double precision floating point value. Gorilla incorporates a new time series compression algorithm that allows us to compress each by series down from 16 bytes to an average of 1.37 bytes, a 12x reduction in size. Further, we have arranged Gorilla’s in-memory data structures to allow fast and efficient scans of all data while maintaining constant time lookup of individual time series.</p>
<p>Gorilla 是一个基于内存的 TSDB，在监控数据写入 HBase 数据存储时，起到直写缓存（Write-Through Cache）的作用。 Gorilla 中存储的监控数据是一个简单的三元组：字符串键、64 位时间戳整数和双精度浮点值。 Gorilla 采用了一种新的时间序列压缩算法，允许我们将每个序列从 16 个字节压缩到平均 1.37 个字节，大小减少了 12 倍。 此外，我们设计了 Gorilla 的内存数据结构，在保持对单个时间序列查找时间恒定的同时，也能够快速高效的进行全数据扫描。</p>
<p><img src="/assets/images/gorilla-cn-2.png" alt="图 2" loading="lazy"></p>
<p>Figure 2: Visualizing the entire compression algorithm. For this example, 48 bytes of values and time stamps<br>are compressed to just under 21 bytes&#x2F;167 bits.</p>
<p>图 2：可视化整个压缩算法。 对于此示例，48 字节的值和时间戳被压缩到略低于 21 字节（大约167 位）。</p>
<p>The key specified in the monitoring data is used to uniquely identify a time series. By sharding all monitoring data based on these unique string keys, each time series dataset can be mapped to a single Gorilla host. Thus, we can scale Gorilla by simply adding new hosts and tuning the sharding function to map new time series data to the expanded set of hosts. When Gorilla was launched to production 18 months ago, our dataset of all time series data inserted in the past 26 hours fit into 1.3TB of RAM evenly distributed across 20 machines. Since then, we have had to double the size of the clusters twice due to data growth, and are now running on 80 machines within each Gorilla cluster. This process was simple due to the share-nothing architecture and focus on horizontal scalability.</p>
<p>监控数据中指定的键用于唯一标识一个时间序列。 通过使用这些唯一的字符串键来对所有的监控数据进行分片，每个时间序列数据集都可以映射到单个 Gorilla 主机。因此，我们可以通过简单地添加新主机并调整分片算法来将新的时间序列数据映射到新的主机上，从而实现扩展 Gorilla 的目的。 当 Gorilla 在 18 个月前投入生产时，我们在过去 26 小时内插入的所有时间序列数据的数据集均匀分布在 20 台机器上的 1.3TB 内存中。 在那之后，由于数据的增长，我们不得不将集群规模翻倍，现在每个 Gorilla 集群运行在 80 台机器上。 由于无共享（Share-Nothing）架构和专注于水平的可扩展性，这个扩展的过程相当简单。</p>
<p>Gorilla tolerates single node failures, network cuts, and entire datacenter failures by writing each time series value to two hosts in separate geographic regions. On detecting a failure, all read queries are failed over to the alternate region ensuring that users do not experience any disruption.</p>
<p>Gorilla 通过将每个时间序列值写入不同地域的两台主机中来容忍单节点故障，网络中断，甚至于整个数据中心故障。 检测到故障时，所有读取操作都将故障转移到备用区域，确保用户不会感知任何中断。</p>
<h3 id="4-1、时序压缩"><a href="#4-1、时序压缩" class="headerlink" title="4.1、时序压缩"></a>4.1、时序压缩</h3><p>In evaluating the feasibility of building an in-memory time series database, we considered several existing compression schemes to reduce the storage overhead. We identified techniques that applied solely to integer data which didn’t meet our requirement of storing double precision floating point values. Other techniques operated on a complete dataset but did not support compression over a stream of data as was stored in Gorilla [7, 13]. We also identified lossy time series approximation techniques used in data mining to make the problem set more easily fit within memory [15, 11], but Gorilla is focused on keeping the full resolution representation of data.</p>
<p>在评估构建内存时间序列数据库的可行性时，我们考虑了几种现有的压缩方案来降低存储开销。 我们确定了仅适用于整数数据的技术，这些技术不符合我们存储双精度浮点值的要求。 其他技术在完整数据集上运行，但不支持对存储在 Gorilla [7, 13] 中的数据流进行压缩。 我们还确定了数据挖掘中使用的有损的时间序列近似技术（Lossy Time Series Approximation Techniques），尝试使其更适合用内存来存储 [15, 11]，但 Gorilla 专注于保持数据的完整性。</p>
<p>Our work was inspired by a compression scheme for floating point data derived in scientific computation. This scheme leveraged XOR comparison with previous values to generate a delta encoding [25, 17].</p>
<p>我们受到了科学计算中浮点数据压缩方案的启发。 该方案利用与先前值的 XOR 比较来生成差值编码 [25, 17]。</p>
<p>Gorilla compresses data points within a time series with no additional compression used across time series. Each data point is a pair of 64 bit values representing the time stamp and value at that time. Timestamps and values are compressed separately using information about previous values. The overall compression scheme is visualized in Figure 2, showing how time stamps and values are interleaved in the compressed block.</p>
<p>Gorilla 只对一个时间序列中的数据点进行压缩，不会有跨时间序列压缩。每个数据点是一对 64 位值，表示当时的时间戳和值。时间戳和值根据先前的值分别进行压缩。整体的压缩方案如图 2 所示，图中展示了时间戳和值是如何在压缩块中交错分布的。</p>
<p>Figure 2.a illustrates the time series data as a stream consisting of pairs of measurements (values) and time stamps. Gorilla compresses this data stream into blocks, partitioned by time. After a simple header with an aligned time stamp (starting at 2 am, in this example) and storing the first value in a less compressed format, Figure 2.b shows that timestamps are compressed using delta-of-delta compression, described in more detail in Section 4.1.1. As shown in Figure 2.b the time stamp delta of delta is −2. This is stored with a two bit header (‘10’), and the value is stored in seven bits, for a total size of just 9 bits. Figure 2.c shows floating-point values are compressed using XOR compression, described in more detail in Section 4.1.2. By XORing the floating point value with the previous value, we find that there is only a single meaningful bit in the XOR. This is then encoded with a two bit header (‘11’), encoding that there are eleven leading zeros, a single meaningful bit, and the actual value (‘1’). This is stored in fourteen total bits.</p>
<p>图 2.a 表明时间序列数据是由成对的时间戳和测量值组成的数据流。 Gorilla 按照时间分区将数据流压缩到数据块中。这里先定义了一个由基线时间构成的 Header （图例中从 2 点开始），然后将第一个值进行了简单的压缩并存储，图 2.b 显示了时间戳的压缩方式为 delta-of-delta ，这个在第 4.1.1 节由更详细的描述。如图 2.b 所示，时间戳 delta-of-delta 的值为 -2 ，我们使用 2 位来存储 Header （’10’），并且之后使用 7 位来存储该值，总大小只有 9 位。图 2.c 显示了浮点值的压缩方式为 XOR，这个在第 4.1.2 节由更详细的描述。通过将浮点值与前一个值进行异或（XOR）计算，我们发现最后只有 1 个有意义的位，然后使用 2 位来编码 Header（’11’），编码中有 11 个前导零（Leading Zero），1 个有意义的位和实际值（’1’），总大小为 14 位。</p>
<p><img src="/assets/images/gorilla-cn-3.png" alt="图 3" loading="lazy"></p>
<p>Figure 3: Distribution of time stamp compression across different ranged buckets. Taken from a sample of 440, 000 real time stamps in Gorilla.</p>
<p>图 3：不同范围存储桶的时间戳压缩分布。 来自于 Gorilla 中 440, 000 个实时时间戳的样本数据。</p>
<h4 id="4-1-1、-时间戳压缩"><a href="#4-1-1、-时间戳压缩" class="headerlink" title="4.1.1、 时间戳压缩"></a>4.1.1、 时间戳压缩</h4><p>We analyzed the time series data stored in ODS so we could optimize the compression scheme implemented in Gorilla. We noticed that the vast majority of ODS data points arrived at a fixed interval. For instance, it is common for a time series to log a single point every 60 seconds. Occasionally, the point may have a time stamp that is 1 second early or late, but the window is usually constrained.</p>
<p>我们分析了 ODS 中存储的时间序列数据，因此我们尝试优化 Gorilla 中实现的数据压缩方案。我们观察到绝大部分的 ODS 数据的时间间隔都是固定的。例如，时间序列通常每 60 秒记录一个点，有时这个点可能会提前或者推迟 1 秒，但是时间窗口通常都是固定的。</p>
<p>Rather than storing timestamps in their entirety, we store an efficient delta of deltas. If the delta between time stamps for subsequent data points in a time series are 60, 60, 59 and 61 respectively, the delta of deltas is computed by subtracting the current time stamp value from the previous one which gives us 0, -1 and 2. An example of how this works is shown in Figure 2.</p>
<p>我们不是存储完整的时间戳，而是存储一个有效的差值的差值（ delta of deltas ）。 如果时间序列中后续数据点的时间戳之间的 delta 分别为 60、60、59 和 61，则 delta 的 delta 是通过从前一个时间戳值中减去当前时间戳值来计算的，得到 0、-1 和 2 . 图 2 展示了其工作原理的一个示例。</p>
<p>We next encode the delta of deltas using variable length encoding with the following algorithm:</p>
<ol>
<li><p>The block header stores the starting time stamp, t−1, which is aligned to a two hour window; the first time stamp, t0, in the block is stored as a delta from t−1 in 14 bits.</p>
</li>
<li><p>For subsequent time stamps, tn:</p>
<ul>
<li><p>Calculate the delta of delta: D &#x3D; (tn − t(n−1)) − (t(n−1) − t(n−2))</p>
</li>
<li><p>If D is zero, then store a single ‘0’ bit</p>
</li>
<li><p>If D is between [-63, 64], store ‘10’ followed by the value (7 bits)</p>
</li>
<li><p>If D is between [-255, 256], store ‘110’ followed by the value (9 bits)</p>
</li>
<li><p>If D is between [-2047, 2048], store ‘1110’ followed by the value (12 bits)</p>
</li>
<li><p>Otherwise store ‘1111’ followed by D using 32 bits</p>
</li>
</ul>
</li>
</ol>
<p>接下来，我们使用以下算法使用可变长度编码对 <strong>差值的差值</strong>（delta of delta） 进行编码：</p>
<ol>
<li>数据块的头部存储起始时间戳（ t-1 ），对齐窗口为 2 小时；数据块中的第一个时间戳（ t0 ）存储为 14 位中（ t-1 ）的增量。</li>
<li>对于后续的时间戳（ tn ）：<ul>
<li>计算差值的差值：D &#x3D; (tn − t(n−1)) − (t(n−1) − t(n−2)) ；</li>
<li>如果 D 为 0 ，则使用 1 个比特位来存储 ‘0’ ；</li>
<li>如果 D 的范围位于 [-63, 64] ，则先使用 2 个比特位存储 ‘10’ ，然后再使用 7 个比特位存储 D 值；</li>
<li>如果 D 的范围位于 [-255, 256] ，则先使用 3 个比特位存储 ‘110’ ，然后再使用 9 个比特位存储 D 值；</li>
<li>如果 D 的范围位于 [-2047, 2048] ，则先使用 4 个比特位存储 ‘1110’ ，然后再使用 12 个比特位存储 D 值；</li>
<li>其他情况下，则先使用 4 个比特位存储 ‘1111’ ，然后再使用 32 个比特位存储 D 值；</li>
</ul>
</li>
</ol>
<p>The limits for the different ranges were selected by sampling a set of real time series from the production system and selecting the ones that gave the best compression ratio. A time series might have data points missing but the existing points likely arrived at fixed intervals. For example if there’s one missing data point the deltas could be 60, 60, 121 and 59. The deltas of deltas would be 0, 61 and -62. Both 61 and -62 fit inside the smallest range and fewer bits can be used to encode these values. The next smallest range [-255, 256] is useful because a lot of the data points come in every 4 minutes and a single data point missing still uses that range.</p>
<p>这些不同的取值范围是从真实的生产环境的时间序列中采样出来的，每个值都能选择合适的范围以达到最好的压缩比。一个时间序列可能会丢失部分数据点，但是它现存的数据很可能都是以固定的时间间隔产生的。例如，如果缺少了一个数据点，则增量可能是 60，60，121，59，那么差值的差值（ delta of deltas ）将是 0，61 和 -62。其中 61 和 -62 都处于最小的的范围内，并且可以使用较小的位来编码这些值。下一个编码取值范围 [-255, 256] 也很有用，因为还有很多数据点是每 4 分钟出现一次，当缺少了单个数据点时仍然可以使用这个取值范围。</p>
<p><img src="/assets/images/gorilla-cn-4.png" alt="图 4" loading="lazy"></p>
<p>Figure 4: Visualizing how XOR with the previous value often has leading and trailing zeros, and for many series, non-zero elements are clustered.</p>
<p>图 4：展示了与前一个值的 XOR 通常如何具有前导零和尾随零，并且对于许多时间序列，非零元素通常是聚集在一起的。</p>
<p>Figure 3 show the results of time stamp compression in Gorilla. We have found that about 96% of all time stamps can be compressed to a single bit.</p>
<p>图 3 显示了 Gorilla 中时间戳压缩的结果。 我们发现大约 96% 的时间戳可以压缩到一个位。</p>
<h4 id="4-1-2、值压缩"><a href="#4-1-2、值压缩" class="headerlink" title="4.1.2、值压缩"></a>4.1.2、值压缩</h4><p>In addition to the time stamp compression, Gorilla also compresses data values. Gorilla restricts the value element in its tuple to a double floating point type. We use a compression scheme similar to existing floating point compression algorithms, like the ones described in [17] and [25].</p>
<p>除了压缩时间戳，Gorilla 还会压缩数据值。 Gorilla 将其元组中的数据值限制为双浮点类型。 我们使用类似于现有浮点压缩算法的压缩方案，类似于 [17] 和 [25] 中描述的那些。</p>
<p>From analyzing our ODS data, we discovered that the value in most time series does not change significantly when compared to its neighboring data points. Further, many data sources only store integers into ODS. This allowed us to tune the expensive prediction scheme in [25] to a simpler implementation that merely compares the current value to the previous value. If values are close together the sign, exponent, and first few bits of the mantissa will be identical. We leverage this to compute a simple XOR of the current and previous values rather than employing a delta encoding scheme.</p>
<p>通过分析 ODS 的数据，我们发现大多数时间序列中的值与其相邻数据点的值相比没有显著的变化。 此外，许多数据源只会将整数存储到 ODS 中。 这使我们可以将文末参考文献 [25] 中的复杂方案调整为更简单的实现，该实现仅将当前值与先前值进行比较。 如果值接近，那么尾数的符号、指数和前几位将完全相同。 因此对当前值和前一个值做一个简单的异或（XOR）运算，而不是像时间戳那样采用差值编码方案。</p>
<p>We then encode these XOR’d values with the following variable length encoding scheme:</p>
<ol>
<li><p>The first value is stored with no compression</p>
</li>
<li><p>If XOR with the previous is zero (same value), store single ‘0’ bit</p>
</li>
<li><p>When XOR is non-zero, calculate the number of leading and trailing zeros in the XOR, store bit ‘1’ followed by either a) or b):</p>
<ul>
<li><p>(Control bit ‘0’) If the block of meaningful bits falls within the block of previous meaningful bits, i.e., there are at least as many leading zeros and as many trailing zeros as with the previous value use that information for the block position and just store the meaningful XORed value.</p>
</li>
<li><p>(Control bit ‘1’) Store the length of the number of leading zeros in the next 5 bits, then store the length of the meaningful XORed value in the next 6 bits. Finally store the meaningful bits of the XORed value.</p>
</li>
</ul>
</li>
</ol>
<p>我们用下面的规则对异或（XOR）后的值进行可变长编码：</p>
<ol>
<li><p>第一个值不进行压缩存储；</p>
</li>
<li><p>如果与前一个值的异或（XOR）结果为零（相同值），则使用 1 个比特位存储 ‘0’ ；</p>
</li>
<li><p>当 XOR 不为零时，计算 XOR 中前导零和尾随零的数量，则使用 1 个比特位存储 ‘1’ ，接下来的值为下面两种之一：</p>
<ul>
<li>（控制位 ‘0’ ）如果有意义的位（即中间非 0 部分）的数据块被前一个数据块包含，即，至少有与先前值一样多的前导零和尾随零，那么就可以直接在数据块中使用这些信息，并且仅需要存储非 0 的 XOR 值；</li>
<li>（控制位 ‘1’ ）将前导零数量的长度存储在接下来的 5 位中，然后将有意义的 XOR 值的长度存储在接下来的 6 位中。 最后存储 XOR 值的有意义的位；</li>
</ul>
</li>
</ol>
<p><img src="/assets/images/gorilla-cn-5.png" alt="图 5" loading="lazy"></p>
<p>Figure 5: Distribution of values compressed across different XOR buckets. Taken from a sample of 1.6 million real values in Gorilla.</p>
<p>图 5：不同 XOR 存储桶压缩的值分布。 取自 Gorilla 中 160 万个真实值的样本。</p>
<p>The overall compression scheme is visualized in Figure 2 which depicts how our XOR encoding can store the values in a time series efficiently.</p>
<p>整体压缩方案如图 2 所示，它描述了我们的 XOR 编码如何有效地将值存储在时间序列中。</p>
<p>Figure 5 shows the distribution of actual values in Gorilla. Roughly 51% of all values are compressed to a single bit since the current and previous values are identical. About 30% of the values are compressed with the control bits ‘10’ (case b), with an average compressed size of 26.6 bits. The remaining 19% are compressed with control bits ‘11’, with an average size of 36.9 bits, due to the extra 13 bits of overhead required to encode the length of leading zero bits and meaningful bits.</p>
<p>图 5 显示了 Gorilla 中实际值的分布。 由于当前值和以前的值相同，因此大约 51% 的值被压缩后仅使用 1 个比特位。 大约 30% 的值使用控制位 ‘10’（情况 b）进行压缩，平均压缩大小为 26.6 位。 剩余的 19% 使用控制位 ‘11’ 进行压缩，平均大小为 36.9 位，位数多是因为编码前导零位和有意义位的长度需要额外的 13 位开销。</p>
<p>This compression algorithm uses both the previous floating point value and the previous XORed value. This results in an additional compression factor because a sequence of XORed values often have a very similar number of leading and trailing zeros, as visualized in Figure 4. Integer values compress especially well because the location of the one bits after the XOR operation is often the same for the whole time series, meaning most values have the same number of trailing zeros.</p>
<p>这种压缩算法同时使用了前序值和前序XOR值。如图 4 所示，由于 XOR 值的序列通常具有非常相似数量的前导零和尾随零，因此最终的结果会有较好的压缩率。这种算法对于整型的压缩效果尤其好，这是因为经过 XOR 运算后的中间段位的位置一般在整个时间序列中对齐的，意味着大多数XOR值有相同个数的尾随零。</p>
<p>One trade-off that is inherent in our encoding scheme is the time span over which the compression algorithm operates. Using the same encoding scheme over larger time periods allows us to achieve better compression ratios. However, queries that wish to read data over a short time range might need to expend additional computational resources on decoding data. Figure 6 shows the average compression ratio for the time series stored in ODS as we change the block size. One can see that blocks that extend longer than two hours provide diminishing returns for compressed size. A two-hour block allows us to achieve a compression ratio of 1.37 bytes per data point.</p>
<p>我们的编码方案有一个折衷是压缩算法运行的时间跨度。 在更长的时间段内使用相同的编码方案可以让我们获得更好的压缩比。但是，这会导致在短时间内读取数据的操作可能需要在解码数据上花费额外的计算资源。图 6 显示了存储在 ODS 中的时间序列在不同数据块大小下的平均压缩率。可以看出，块大小超过两个小时之后，数据的压缩收益率是逐渐减少的。两小时的数据块使得我们能够实现每个数据点占用大约 1.37 个字节。</p>
<p><img src="/assets/images/gorilla-cn-6.png" alt="图 6" loading="lazy"></p>
<p>Figure 6: Average bytes used for each ODS data point as the compression bucket is varied from 0 (no compression) to 240 minutes. Bucket size larger than two hours do not give significant additional compression for our dataset. This is across the entire production Gorilla data set (approximately 2 billion time series).</p>
<p>图 6：每个 ODS 数据点所占用的平均字节数，因为压缩桶（数据块）从 0（无压缩）到 240 分钟不等。 大于两个小时的桶大小不会为我们的数据集提供明显的压缩效果。 这是整个生产 Gorilla 数据集（大约 20 亿个时间序列）。</p>
<h3 id="4-2、内存数据结构"><a href="#4-2、内存数据结构" class="headerlink" title="4.2、内存数据结构"></a>4.2、内存数据结构</h3><p>The primary data structure in Gorilla’s implementation is a Timeseries Map (TSmap). Figure 7 provides an overview of this data structure. TSmap consists of a vector of C++ standard library shared-pointers to time series and a caseinsensitive, case-preserving map from time series names to the same. The vector allows for efficient paged scans through all the data, while the map enables constant time lookups of particular time series. Constant time lookup is necessary to achieve the design requirement for fast reads while still allowing for efficient data scans.</p>
<p>Gorilla 实现中的主要数据结构是时间序列 Map （TSmap）。 图 7 提供了该数据结构的概述。 时间序列图包含一个 C++ 标准库中的 vector ，它是一个指向时间序列的指针；还包含一个 map ，其中 key 位时间序列的名称，不区分大小写并且保留原有大小写，值是和 vector 中一样的指针。vector 可以实现全数据分页查询，而 map 可以支撑指定时间序列的定长时间段查询，要满足快速查询的需求必须要具备恒定时间的查询能力，同时也要满足有效的数据扫描。</p>
<p>The use of C++ shared-pointers enables scans to copy the vector (or pages thereof) in a few microseconds, avoiding lengthy critical sections that would impact the flow of incoming data. On deletion of a time series, a vector entry is tombstoned, and the index is placed in a free pool which is re-used when new time series are created. Tombstoneing a section of memory marks it as ’dead’, and ready to be reused, without actually freeing it to the underlying system.</p>
<p>使用 C++ 共享指针可以在扫描时能够仅用几微秒内就复制整个 vector（或者其中的几页），避免冗长数据对传入的数据流产生影响。删除时间序列时，它的 vector 被标记为 “墓碑状态” ，它的索引会被放到一个空闲迟中，当创建新的时间序列时会被复用。”墓碑状态” 实际上是将一段内存标记为 “死亡” ，并准备好被复用，实际上并未将其释放到底层系统。</p>
<p>Concurrency is attained with a single read-write spin lock protecting map and vector access and 1-byte spin lock on each time series. Since each individual time series has a relatively low write throughput, the spin lock has very low contention between reads and writes.</p>
<p>每个时间序列上有一个读写自旋锁用于保护对 map 和 vector 的并发访问。由于单个时间序列具有相对较低的写入吞吐量，因此自旋锁在读取和写入间的竞争非常低。</p>
<p>As illustrated in Figure 7, the mapping of shard identifier (shardId) to TSmap, named ShardMap, is maintained with a vector of pointers to the TSmaps. Mapping of a time series name to a shard is done using the same caseinsensitive hash in the TSmap, mapping it to an ID between [0,NumberOfShards). Since the total number of shards in the system is constant and numbering in the low thousands, the additional overhead of storing null pointers in the ShardMap is negligible. Like the TSmaps, concurrent access to the ShardMap is managed with a read-write spin lock.</p>
<p>如图 7 所示，分片唯一标识 （shardId） 与 TSmap 的映射存储在 ShardMap 中，它是一个存储了 TSmaps 指针的 vector。时间序列名称到分片的映射使用了与 TSmap 中一样的对大小写不敏感的哈希算法，哈希后的值位于 [0,NumberOfShards)  之间。由于系统中分片的数量固定，并且总量只有几千个，因此在 ShardMap 中存储空指针的额外开销可以忽略不计。 与 TSmap 一样，对 ShardMap 的并发访问也是由读写自旋锁管理。</p>
<p>Since the data are already partitioned by shard, individual maps remain sufficiently small (about 1 million entries), the C++ standard library unordered-map has sufficient performance, and there have been no issues with lock contention.</p>
<p>由于数据已经被 shard 分区，单个 map 仍然可以足够小（大约 100 万个条目），C++ 标准库 unordered-map 具有足够好的性能，并且不会发生锁争用的问题。</p>
<p><img src="/assets/images/gorilla-cn-7.png" alt="图 7" loading="lazy"></p>
<p>Figure 7: Gorilla in-memory data structure. On a query, first a) the TSmap pointer is examined. If b) the pointer is null, it means this Gorilla host does not own the shard. If non-null, then c) the TSmap is read-locked, and the pointer to the time series structure (TS) is found in the unordered map and copied out. At this point, both RW locks can be unlocked. Next, d) the TS spinlock is locked, and data for the query time range can be directly copied out of the TS.</p>
<p>图 7：Gorilla 内存数据结构。</p>
<ul>
<li>a) 在查询时，首先检查 TSmap 指针。 </li>
<li>b) 如果指针为空，则表示此 Gorilla 主机不拥有该分片。</li>
<li>c) 如果指针非空，则  TSmap 被读锁定，并且在无序映射中找到指向时间序列结构 (TS) 的指针并复制出来。 此时，两个 RW 锁都可以解锁。 </li>
<li>d) 接下来，TS 自旋锁被锁定，查询时间范围的数据可以直接从 TS 中复制出来。</li>
</ul>
<p>A time series data structure is composed of a sequence of closed blocks for data older than two hours and a single open data block that holds the most recent data. The open data block is an append-only string, to which new, compressed time stamps and values are appended. Since each block holds two hours of compressed data, the open block is closed once it is full. Once a block is closed, it is never changed until it is deleted out of memory. Upon closing, a block is copied to memory allocated from large slabs to reduce fragmentation. While the open block is often reallocated as it changes sizes, we find that the copy process reduces the overall fragmentation within Gorilla.</p>
<p>时间序列的数据结构有两个组成部分：一部分是两小时以上数据的封闭块，一部分是保存最新数据的开放块。开放块是一个只追加的字符串，新的时间戳和值压缩后会追加到这个字符串上。由于每个块置存储两个小时的压缩数据，因此开发块一旦满了之后就会封闭，一个块被关闭后它就永远不会被改变，知道将它从内存中删除。关闭块时，会根据使用的 slab 总大小分配出一个新的数据块存储数据，虽然开放的数据块在改变大小时也会重新分配，但是我们发现通过这种方式能够减少 Gorilla 的整体内存碎片。</p>
<p>Data is read out by copying the data blocks that could contain data for the queried time range directly into the output remote procedure call structure. The entire data block is returned to the client, leaving the decompression step to be done outside Gorilla.</p>
<p>通过将可能包含查询时间范围内数据的数据块直接复制到远程调用的数据结构中来读取数据。 将整个数据块返回给客户端，使得解压的过程可以在 Gorilla 外完成。</p>
<h3 id="4-3、磁盘数据结构"><a href="#4-3、磁盘数据结构" class="headerlink" title="4.3、磁盘数据结构"></a>4.3、磁盘数据结构</h3><p>One of our goals for Gorilla is to survive single host failures. Gorilla achieves persistence by storing data in GlusterFS, a POSIX-compliant, distributed file system [4] with 3x replication. HDFS or other distributed file systems would have sufficed just as easily. We also considered single host databases such as MySQL and RocksDB but decided against these because our persistency use case did not require a database query language.</p>
<p>Gorilla 的目标之一是能够应对单机故障。Gorilla 通过将数据存储在 GlusterFS 中来实现持久性，GlusterFS 是一种符合 POSIX 的支持三副本复制的分布式文件系统 [4]。HDFS 或者其他的分布式文件系统也同样可以应对单机故障。我们还考虑了 MySQL 和 RocksDB 等单机数据库，不过最终还是决定不使用这类数据库，因为我们的持久化场景中不会使用数据库查询语音。</p>
<p>A Gorilla host will own multiple shards of data, and it maintains a single directory per shard. Each directory contains four types of files: Key lists, append-only logs, complete block files, and checkpoint files.</p>
<p>Gorilla 主机将拥有多个数据分片，并且每个分片维护一个目录。 每个目录包含四种类型的文件：key 列表、 append-only 日志、完整块文件和 checkponit 文件。</p>
<p>The key list is simply a map of the time series string key to an integer identifier. This integer identifier is the index into the in-memory vector. New keys are append to the current key list, and Gorilla periodically scans all the keys for each shard in order to re-write the file.</p>
<p>Key 列表只是时间序列字符串键名到整数标识符的映射。这个整数标识符是内存中 vector 的索引下标。新的密钥会被追加到当前密钥列表中，Gorilla 会定期扫描每个分片的所有密钥，以便重新写入文件。</p>
<p>As data points are streamed to Gorilla, they are stored in a log file. The time stamps and values are compressed using the format described in Section 4.1. However, there is only one append-only log per shard, so values within a shard are interleaved across time series. This difference from the in memory encoding means that each compressed time stampvalue pair is also marked with it’s 32-bit integer ID, adding significant storage overhead to the per-shard log file.</p>
<p>当数据点流式传输到 Gorilla 时，它们会被存储在日志文件中。 时间戳和值会被按照第 4.1 节中描述的格式进行压缩。 但是，每个分片只有一个 append-only 日志，因此数据会交叉跨越多个时间序列。和内存编码不同的是，每个时间戳和值还要加上 32 位的整型 ID 进行标记，所以相比之下，每个分片上的日志文件会增加明显的存储开销。</p>
<p>Gorilla does not offer ACID guarantees and as such, the log file is not a write-ahead-log. Data is buffered up to 64kB, usually comprising one or two seconds worth of data, before being flushed. While the buffer is flushed on a clean shutdown, a crash might result in the loss of small amounts of data. We found this trade-off to be worth the data loss, as it allowed higher data rate to be pushed to disk and higher availability for writes compared with a traditional write-ahead log.</p>
<p>Gorilla 不提供 ACID 保证，因此，日志文件不是 WAL 日志。 在被刷新之前，数据缓冲高达 64kB，这通常会包含一到两秒的数据。 虽然在正常退出系统时缓冲区中的数据会被刷到磁盘，但是当发生异常崩溃时可能会导致少部分的数据丢失。相比于传统的 WAL 日志的收益，由于这种方式能够实现以更高的速率将数据写入磁盘并提供更高的写入可用性，因此我们认为这个取舍是值得的。</p>
<p>Every two hours, Gorilla copies the compressed block data to disk, as this format is much smaller than the log files. There is one complete block file for every two hours worth of data. It has two sections: a set of consecutive 64kB slabs of data blocks, copied directly as they appear in memory, and a list of &lt;time series ID, data block pointer&gt; pairs. Once a block file is complete, Gorilla touches a checkpoint file and deletes the corresponding logs. The checkpoint file is used to mark when a complete block file is flushed to disk. If a block file was not successfully flushed to disk when it on a process crash, when the new process starts up, the checkpoint file will not exist, so the new process knows it cannot trust the block file and will instead read from the log file only.</p>
<p>每隔两个小时，Gorilla 就会将压缩块数据复制到磁盘，因为这种格式的数据比日志文件小得多。 每两小时的数据就有一个完整的块文件。 它有两个部分：一组连续的 64kB 数据块，他们直接从内存中复制而来，以及一系列由 &lt;时间序列ID，数据块指针&gt; 组成的键值对。一旦某个块文件完全刷到磁盘，Gorilla 就会记录一个 checkpoint 文件并删除相应的日志。checkpoint 文件用来标记一个完整的数据块什么时候被刷到磁盘。如果在遇到进程崩溃时块文件没有被成功刷到磁盘，那么在新的进程启动时对应的 checkpoint 文件是不存在的，因此这个时候每次启动新的进程时除了读取块文件之外，还会从日志文件中读取 checkpoint 之后的数据。</p>
<h3 id="4-4、故障处理"><a href="#4-4、故障处理" class="headerlink" title="4.4、故障处理"></a>4.4、故障处理</h3><p>For fault tolerance, we chose to prioritize tolerating single node, temporary failures with zero observable downtime and large scale, and localized failures (such as a network cut to an entire region). We did this because single node failures happen quite often, and large scale, localized failures become a concern at Facebook’s scale to allow the ability to operate under natural (or human-caused) disasters. There is the added benefit that one can model rolling software upgrades as a set of controlled, single node failures, so optimizing for this case means hassle-free and frequent code pushes. For all other failures, we chose trade-offs that, when they do cause data-loss, will prioritize the availability of more recent data over older data. This is because we can rely on the existing HBase TSDB system to handle historical data queries, and automated systems that detect level changes in time series are still useful with partial data, as long as has the most recent data.</p>
<p>Gorilla ensures that it remains highly available to data center faults or network partitions by maintaining two completely independent instances in separate data center regions. On a write, data is streamed to each Gorilla instance, with no attempt to guarantee consistency. This makes largescale failures easy to handle. When an entire region fails, queries are directed at the other until the first has been back up for 26 hours. This is important to handle large scale disaster events, whether actual or simulated [21]. For example, when the Gorilla instance in region A completely fails, both reads and writes to that region will also fail. The read failures will be transparently retried to the Gorilla instance in the healthy region B. If the event lasts long enough (more than one minute), data will be dropped from region A, and requests will not be retried. When this happens, all reads can be turned off from region A until the cluster has been healthy for at least 26 hours. This remediation can be performed either manually or automated.</p>
<p>Within each region, a Paxos-based [6, 14] system called ShardManager assigns shards to nodes. When a node fails, ShardManager distributes its shards among other nodes in the cluster. During shard movement, write clients buffer their incoming data. The buffers are sized to hold 1 minute of data, and points older than a minute are discarded to make room for newer data. We have found that this time period is sufficient to allow shard reassignment in most situations, but for extended outages, it prioritizes the most recent data, as more recent data is intuitively more useful for driving automated detection systems. When a Gorilla host α in region A crashes or goes down for any reason, writes are buffered for at least 1 minute as the Gorilla cluster tries to resurrect the host. If the rest of the cluster is healthy, shard movement happens in thirty seconds or less, resulting in no data loss. If the movement does not occur fast enough reads can be pointed to the Gorilla instance in region B, either in a manual or automated process.</p>
<p>When shards are added to a host, it will read all the data from GlusterFS. These shards may have been owned by the same host before the restart or another. A host can read and process all the data it needs to be fully functional in about 5 minutes from GlusterFS. Because of the number of shards in the system and the total data stored, each shard represents about 16GB of on-disk storage. This can be read from GlusterFS in just a few minutes, as the files are spread across several physical hosts. While the host is reading the data, it will accept new incoming data points and put them into a queue to be processed at the earliest possible time. When shards are reassigned, clients immediately drain their buffers by writing to the new node. Going back to the Gorilla host α in region A crashing example: when α crashes, the shards are reassigned to host β in the same Gorilla instance. As soon as host β is assigned the shards, it begins accepting streaming writes, so no data loss occurs for in-flight data. If Gorilla host α is going down in a more controlled manner, it flushes all data to disk before exiting, so no data is lost for software upgrades.</p>
<p>In our example, if host α crashes before successfully flushing its buffers to disk, that data will be lost. In practice, this happens very rarely, and only a few seconds of data is actually lost. We make this trade-off to accept a higher throughput of writes and to allow accepting more recent writes sooner after an outage. Also, we monitor this situation, and are able to point reads at the more healthy region.</p>
<p>Note that after a node failure, shards will be partially unavailable for reads until the new nodes hosting these shards read the data from disk. Queries will return partial data (blocks are read most recent to least recent) and will mark the results as partial.</p>
<p>When the read client library receives a partial result from its query to the Gorilla instance in region A, it will retry fetching the affected time series from region B and keep those results if they are not partial. If both region A and region B return partial results, both partial results are returned to the caller with a flag set that some error caused incomplete data. The caller can then decide if it has enough information to continue processing the request or if it should fail outright. We make this choice because Gorilla is most often used by automated systems to detect level changes in time series. These systems can function well with only partial data, as long as it is the most recent data.</p>
<p>Automatic forwarding of reads from an unhealthy host to a healthy one means that users are protected from restarts and software upgrades. We find that upgrading the version of software causes zero data drops, and all reads continue to be served successfully with no manual intervention. This also allows Gorilla to transparently serve reads across server failures ranging from a single node to an entire region [21].</p>
<p>Finally, we still use our HBase TSDB for long-term storage of data. If all in-memory copies of data are lost, our engineers can still query the more durable storage system to do their analysis and drive ad-hoc queries, and Gorilla can still drive real-time detection of level changes, once it is restarted and accepting new writes.</p>
<h2 id="5、Gorilla的新工具"><a href="#5、Gorilla的新工具" class="headerlink" title="5、Gorilla的新工具"></a>5、Gorilla的新工具</h2><p>Gorilla’s low latency query processing has enabled the creation of new analysis tools.</p>
<h3 id="5-1、关联引擎"><a href="#5-1、关联引擎" class="headerlink" title="5.1、关联引擎"></a>5.1、关联引擎</h3><p>The first is a time series correlation engine that runs within Gorilla. Correlation search allows users to perform interactive, brute-force search on many time series, currently limited to 1 million at a time.</p>
<p><img src="/assets/images/gorilla-cn-8.png" alt="图 8" loading="lazy"></p>
<p>Figure 8: Total query latency breakdown with different TSDB solutions for ODS. Comparing to HBase, Gorilla has provided between 73x and 350x improvement, depending on the query size. This plot also includes preliminary results of two other options: Gorilla using flash to store data older than 26 hours, and HBase with ODS cache.</p>
<p>The correlation engine calculates the Pearson ProductMoment Correlation Coefficient (PPMCC) which compares a test time series to a large set of time series [22]. We find that PPMCC’s ability to find correlation between similarly shaped time series, regardless of scale, greatly helps automate root-cause analysis and answer the question “What happened around the time my service broke?”. We found that this approach gives satisfactory answers to our question and was simpler to implement than similarly focused approaches described in the literature[10, 18, 16].</p>
<p>To compute PPMCC, the test time series is distributed to each Gorilla host along with all of the time series keys. Then, each host independently calculates the top N correlated time series, ordered by the absolute value of the PPMCC compared to the needle, and returning the time series values. In the future, we hope that Gorilla enables more advanced data mining techniques on our monitoring time series data, such as those described in the literature for clustering and anomaly detection [10, 11, 16].</p>
<h3 id="5-2、图表"><a href="#5-2、图表" class="headerlink" title="5.2、图表"></a>5.2、图表</h3><p>Low latency queries have also enabled higher query volume tools. As an example, engineers unrelated to the monitoring team have created a new data visualization which will show large sets of horizon charts, which themselves are reductions across many time series. This visualization allows users to quickly visually scan across large sets of data to see outliers and time-correlated anomalies.</p>
<h3 id="5-3、聚合"><a href="#5-3、聚合" class="headerlink" title="5.3、聚合"></a>5.3、聚合</h3><p>Recently, we moved the roll up background process from a set of map-reduce jobs to running directly against Gorilla. Recall that ODS performs time-based aggregations (or roll up) compression on old data, which is a lossy compression that reduces data granularity [26], similar to the format used by Whisper [1]. Before Gorilla, map-reduce jobs were run against the HBase cluster, which would read all data for the past hour and output values for a new, lower granularity table. Now, a background process periodically scans all completed buckets every two hours, generating the new values for the lower granularity table. Because scanning all data in Gorilla is very efficient, this move has reduced load on the HBase cluster, as we no longer need to write all the high granularity data to disk and do expensive full table scans on HBase.</p>
<p><img src="/assets/images/gorilla-cn-9.png" alt="图 9" loading="lazy"></p>
<p>Figure 9: Growth of the total query volume since Gorilla’s introduction to ease data exploration and develop new analysis tools.</p>
<h2 id="6、经验"><a href="#6、经验" class="headerlink" title="6、经验"></a>6、经验</h2><h3 id="6-1、容错"><a href="#6-1、容错" class="headerlink" title="6.1、容错"></a>6.1、容错</h3><p>We next describe several planned and unplanned events that occurred over the past 6 months that affected some portion of Facebook’s site availability. We restrict ourselves to discussing the impact of these events on Gorilla as other issues are beyond the scope of this paper.</p>
<p><strong>Network cuts.</strong> 3 unplanned events resembling network cuts&#x2F;outages to some portion of machines. The cuts were automatically detected and Gorilla automatically shifted reads to the unaffected coast without any service disruption.</p>
<p><strong>Disaster readiness.</strong> 1 planned major fire drill simulating total network cut to one storage back end. As above, Gorilla switched reads to the unaffected coast. Once the downed region was restored, Gorilla in the down region was manually remediated to pull in logs from the firedrill time frame so dashboards served out of the down region would display the expected data to end users.</p>
<p><strong>Configuration changes and code pushes.</strong> There were 6 configuration changes and 6 code releases that required restarting Gorilla in a given region.</p>
<p><strong>Bug.</strong> A release with a major bug was pushed to a single coast. Gorilla immediately shifted load to the other region and continued serving uses until the bug was fixed. There was minimal correctness issues in the served data.</p>
<p><strong>Single node failures.</strong> There were 5 single machine failures (unassociated with the major bug), causing no lost data and no remediation needed.</p>
<p>There were zero events in Gorilla in the last 6 months that caused anomaly detection and alerting issues. Since Gorilla launched, there has only been 1 event that disrupted realtime monitoring. In all cases, the long-term storage was able to act as a backup for all monitoring-related queries.</p>
<p><img src="/assets/images/gorilla-cn-10.png" alt="图 10" loading="lazy"></p>
<p>Figure 10: When searching for the root cause for a site-wide error rate increase, Gorilla’s time series correlation found anomalous events that were correlated in time, namely a drop in memory used when copying a newly released binary.</p>
<h3 id="6-2、网站故障排查"><a href="#6-2、网站故障排查" class="headerlink" title="6.2、网站故障排查"></a>6.2、网站故障排查</h3><p>For an example of how Facebook uses time series data to drive our monitoring, one can look at a recent issue that was detected quickly and fixed due to monitoring data, first described externally at SREcon15 [19].</p>
<p>A mysterious problem resulted in a spike in the site wide error rate. This error rate was visible in Gorilla a few minutes after the error rate spike and raised an alert which notified the appropriate team a few minutes later [19]. Then, the hard work began. As one set of engineers mitigated the issue, others began the hunt for a root cause. Using tools built on Gorilla, including a new time series correlation search described in Section 5, they were able to find that the routine process of copying the release binary to Facebook’s web servers caused an anomalous drop in memory used across the site, as illustrated in Figure 10. The detection of the problem, various debugging efforts and root cause analysis, depended on time series analysis tools enabled by Gorilla’s high performance query engine.</p>
<p>Since launching about 18 months ago, Gorilla has helped Facebook engineers identify and debug several such production issues. By reducing the 90th percentile Gorilla query time to 10ms, Gorilla has also improved developer productivity. Further by serving 85% of all monitoring data from Gorilla, very few queries must hit the HBase TSDB [26], resulting in a lower load on the HBase cluster.</p>
<h3 id="6-3、经验教训"><a href="#6-3、经验教训" class="headerlink" title="6.3、经验教训"></a>6.3、经验教训</h3><p><strong>Prioritize recent data over historical data.</strong> Gorilla occupies an interesting optimization and design niche. While it must be very reliable, it does not require ACID data guarantees. In fact, we have found that it is more important for the most recent data to be available than any previous data point. This led to interesting design trade-offs, such as making a Gorilla host available for reads before older data is read off disk.</p>
<p><strong>Read latency matters.</strong> The efficient use of compression and in-memory data structures has allowed for extremely fast reads and resulted in a significant usage increase. While ODS served 450 queries per second when Gorilla launched, Gorilla soon overtook it and currently handles more than 5,000 steady state queries per second, peaking at one point to 40,000 peak queries per second, as seen in Figure 9. Low latency reads have encouraged our users to build advanced data analysis tools on top of Gorilla as described in Section 5.</p>
<p><strong>High availability trumps resource efficiency.</strong> Fault tolerance was an important design goal for Gorilla. It needed to be able to withstand single host failures with no interruption in data availability. Additionally, the service must be able to withstand disaster events that may impact an entire region. For this reason, we keep two redundant copies of data in memory despite the efficiency hit. </p>
<p>We found that building a reliable, fault tolerant system was the most time consuming part of the project. While the team prototyped a high performance, compressed, inmemory TSDB in a very short period of time, it took several more months of hard work to make it fault tolerant. However, the advantages of fault tolerance were visible when the system successfully survived both real and simulated failures [21]. We also benefited from a system that we can safely restart, upgrade, and add new nodes to whenever we need to. This has allowed us to scale Gorilla effectively with low operational overhead while providing a highly reliable service to our customers.</p>
<h2 id="7、未来的工作"><a href="#7、未来的工作" class="headerlink" title="7、未来的工作"></a>7、未来的工作</h2><p>We wish to extend Gorilla in several ways. One effort is to add a second, larger data store between in-memory Gorilla and HBase based on flash storage. This store has been built to hold the compressed two hour chunks but for a longer period than 26 hours. We have found that flash storage allows us to store about two weeks of full resolution, Gorilla compressed data. This will extend the amount of time full resolution data is available to engineers to debug problems. Preliminary performance results are included in Figure 8.</p>
<p>Before building Gorilla, ODS relied on the HBase backing store to be a real-time data store: very shortly after data was sent to ODS for storage, it needed to be available to read operations placing a significant burden on HBase’s disk I&#x2F;O. Now that Gorilla is acting as a write-through cache for the most recent data, we have at least a 26 hour window after data is sent to ODS before they will be read from HBase. We are exploiting this property by rewriting our write path to wait longer before writing to HBase. This optimization should be much more efficient on HBase, but the effort is too new to report results.</p>
<h2 id="8、总结"><a href="#8、总结" class="headerlink" title="8、总结"></a>8、总结</h2><p>Gorilla is a new in-memory times series database that we have developed and deployed at Facebook. Gorilla functions as a write through cache for the past 26 hours of monitoring data gathered across all of Facebook’s systems. In this paper, we have described a new compression scheme that allows us to efficiently store monitoring data comprising of over 700 million points per minute. Further, Gorilla has allowed us to reduce our production query latency by over 70x when compared to our previous on-disk TSDB. Gorilla has enabled new monitoring tools including alerts, automated remediation and an online anomaly checker. Gorilla has been in deployment for the past 18 months and has successfully doubled in size twice in this period without much operational effort demonstrating the scalability of our solution. We have also verified Gorilla’s fault tolerance capabilities via several large scale simulated failures as well as actual disaster situations—Gorilla remained highly available for both writes and reads through these events aiding site recovery.</p>
<h2 id="9、致谢"><a href="#9、致谢" class="headerlink" title="9、致谢"></a>9、致谢</h2><p>Lots of thanks to Janet Wiener, Vinod Venkataraman and the others who reviewed early drafts of this paper to find typos and incorrect information.</p>
<p>Huge thanks to Sanjeev Kumar and Nathan Bronson, who had great insights into framing the paper to make it read better.</p>
<p>Thank you to Mike Nugent, who had the brilliant idea to use PPMCC to find root causes and effects caused by interesting time series, and hacked a prototype together so quickly.</p>
<p>Of course, thanks to the current ODS team (Alex Bakhturin, Scott Franklin, Ostap Korkuna, Wojciech Lopata, Jason Obenberger, and Oleksandr Voietsa), and ODS alumnus (Tuomas Pelkonen and Charles Thayer) who have made monitoring Facebook’s infrastructure so much fun over the last few years. You guys are awesome!</p>
<h2 id="10、参考文献"><a href="#10、参考文献" class="headerlink" title="10、参考文献"></a>10、参考文献</h2><p>[1] Graphite - Scalable Realtime Graphing. <a target="_blank" rel="noopener" href="http://graphite.wikidot.com/">http://graphite.wikidot.com/</a>. Accessed March 20, 2015.<br>[2] Influxdb.com: InfluxDB - Open Source Time Series, Metrics, and Analytics Database. <a target="_blank" rel="noopener" href="http://influxdb.com/">http://influxdb.com/</a>. Accessed March 20, 2015.<br>[3] L. Abraham, J. Allen, O. Barykin, V. R. Borkar, B. Chopra, C. Gerea, D. Merl, J. Metzler, D. Reiss, S. Subramanian, J. L. Wiener, and O. Zed. Scuba: Diving into Data at Facebook. PVLDB, 6(11):1057–1067, 2013.<br>[4] E. B. Boyer, M. C. Broomfield, and T. A. Perrotti. GlusterFS One Storage Server to Rule Them All. Technical report, Los Alamos National Laboratory (LANL), 2012.<br>[5] N. Bronson, T. Lento, and J. L. Wiener. Open Data Challenges at Facebook. In Workshops Proceedings of the 31st International Conference on Data Engineering Workshops, ICDE Seoul, Korea. IEEE, 2015.<br>[6] T. D. Chandra, R. Griesemer, and J. Redstone. Paxos Made Live: An Engineering Perspective. In Proceedings of the twenty-sixth annual ACM symposium on Principles of distributed computing, pages 398–407. ACM, 2007.<br>[7] H. Chen, J. Li, and P. Mohapatra. RACE: Time Series Compression with Rate Adaptivity and Error Bound for Sensor Networks. In Mobile Ad-hoc and Sensor Systems, 2004 IEEE International Conference on, pages 124–133. IEEE, 2004.<br>[8] B. Hu, Y. Chen, and E. J. Keogh. Time Series Classification under More Realistic Assumptions. In SDM, pages 578–586, 2013.<br>[9] E. Keogh, K. Chakrabarti, M. Pazzani, and S. Mehrotra. Locally Adaptive Dimensionality Reduction for Indexing Large Time Series Databases. ACM SIGMOD Record, 30(2):151–162, 2001.<br>[10] E. Keogh, S. Lonardi, and B.-c. Chiu. Finding Surprising Patterns in a Time Series Database in Linear Time and Space. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 550–556. ACM, 2002.<br>[11] E. Keogh, S. Lonardi, and C. A. Ratanamahatana. Towards Parameter-Free Data Mining. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 206–215. ACM, 2004.<br>[12] E. Keogh and C. A. Ratanamahatana. Exact Indexing of Dynamic Time Warping. Knowledge and information systems, 7(3):358–386, 2005.<br>[13] I. Lazaridis and S. Mehrotra. Capturing Sensor-Generated Time Series with Quality Guarantees. In Data Engineering, 2003. Proceedings. 19th International Conference on, pages 429–440. IEEE, 2003.<br>[14] Leslie Lamport. Paxos Made Simple. SIGACT News, 32(4):51–58, December 2001.<br>[15] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. A Symbolic Representation of Time Series, with Implications for Streaming Algorithms. In Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery, pages 2–11. ACM, 2003.<br>[16] J. Lin, E. Keogh, S. Lonardi, J. P. Lankford, and D. M. Nystrom. Visually Mining and Monitoring Massive Time Series. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 460–469. ACM, 2004.<br>[17] P. Lindstrom and M. Isenburg. Fast and Efficient Compression of Floating-Point Data. Visualization and Computer Graphics, IEEE Transactions on, 12(5):1245–1250, 2006.<br>[18] A. Mueen, S. Nath, and J. Liu. Fast Approximate Correlation for Massive Time-Series Data. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, pages 171–182. ACM, 2010.<br>[19] R. Nishtala. Learning from Mistakes and Outages. Presented at SREcon, Santa Clara, CA, March 2015.<br>[20] R. Nishtala, H. Fugal, S. Grimm, M. Kwiatkowski, H. Lee, H. C. Li, R. McElroy, M. Paleczny, D. Peek, P. Saab, et al. Scaling Memcache at Facebook. In nsdi, volume 13, pages 385–398, 2013.<br>[21] J. Parikh. Keynote speech. Presented at @Scale Conference, San Francisco, CA, September 2014.<br>[22] K. Pearson. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58(347-352):240–242, 1895.<br>[23] F. Petitjean, G. Forestier, G. Webb, A. Nicholson, Y. Chen, and E. Keogh. Dynamic Time Warping Averaging of Time Series Allows Faster and More Accurate Classification. In IEEE International Conference on Data Mining, 2014.<br>[24] T. Rakthanmanon, B. Campana, A. Mueen, G. Batista, B. Westover, Q. Zhu, J. Zakaria, and E. Keogh. Searching and Mining Trillions of Time Series Subsequences Under Dynamic Time Warping. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 262–270. ACM, 2012.<br>[25] P. Ratanaworabhan, J. Ke, and M. Burtscher. Fast Lossless Compression of Scientific Floating-Point Data. In DCC, pages 133–142. IEEE Computer Society, 2006.<br>[26] L. Tang, V. Venkataraman, and C. Thayer. Facebook’s Large Scale Monitoring System Built on HBase. Presented at Strata Conference, New York, 2012.<br>[27] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu, P. Wyckoff, and R. Murthy. Hive: A Warehousing Solution Over a Map-Reduce Framework. PVLDB, 2(2):1626–1629, 2009.<br>[28] T. W. Wlodarczyk. Overview of Time Series Storage and Processing in a Cloud Environment. In Proceedings of the 2012 IEEE 4th International Conference on Cloud Computing Technology and Science (CloudCom), pages 625–628. IEEE Computer Society, 2012.</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.huoban.com/news/post/7708.html">https://www.huoban.com/news/post/7708.html</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/174535">https://developer.aliyun.com/article/174535</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shuzhiduo.com/A/8Bz8NyoX5x/">https://www.shuzhiduo.com/A/8Bz8NyoX5x/</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.acolyer.org/2016/05/03/gorilla-a-fast-scalable-in-memory-time-series-database/">https://blog.acolyer.org/2016/05/03/gorilla-a-fast-scalable-in-memory-time-series-database/</a></li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">作者: </span><span class="post-copyright-info"><a href="mailto:undefined">bugwz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">链接: </span><span class="post-copyright-info"><a href="https://bugwz.com/2022/09/24/gorilla-cn/">https://bugwz.com/2022/09/24/gorilla-cn/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://bugwz.com">咕咕</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Paper/">Paper</a><a class="post-meta__tags" href="/tags/Gorilla/">Gorilla</a><a class="post-meta__tags" href="/tags/TSDB/">TSDB</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/09/29/redismodule-redistimer/"><i class="fa fa-chevron-left">  </i><span>RedisModule剖析 - RedisTimer</span></a></div><div class="next-post pull-right"><a href="/2022/07/01/redismodule-redistimeseries/"><span>RedisModule剖析 - RedisTimeSeries</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '6af3be16b94cec39bcf6',
  clientSecret: '13a5202ff773ffcea6300b6c8ff25f455566737c',
  repo: 'bugwz.github.io',
  owner: 'bugwz',
  admin: 'bugwz',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(https://cdn.bugwz.com/paper.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2014 - 2022 By bugwz</div><div class="framework-info"><span></span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span></span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"></div></div></div></div><div class="search-mask"></div></body></html>