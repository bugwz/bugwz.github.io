<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>译 - The rsync algorithm | 咕咕</title><meta name="author" content="bugwz"><meta name="copyright" content="bugwz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="《The rsync algorithm》这篇发表于 1996 年的论文中介绍了一种名为 rsync 的增量同步算法，它能够快速地将两个文件夹中的内容同步。该算法利用了文件的局部性和差异性，通过计算文件的弱校验和和块校验和来确定文件的相似性，并进行增量同步。该算法具有高效性、可靠性和安全性等优点，在实际应用中被广泛使用。">
<meta property="og:type" content="article">
<meta property="og:title" content="译 - The rsync algorithm">
<meta property="og:url" content="https://bugwz.com/2019/10/20/the-rsync-algorithm/index.html">
<meta property="og:site_name" content="咕咕">
<meta property="og:description" content="《The rsync algorithm》这篇发表于 1996 年的论文中介绍了一种名为 rsync 的增量同步算法，它能够快速地将两个文件夹中的内容同步。该算法利用了文件的局部性和差异性，通过计算文件的弱校验和和块校验和来确定文件的相似性，并进行增量同步。该算法具有高效性、可靠性和安全性等优点，在实际应用中被广泛使用。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bugwz.com/assets/images/bg/paper.jpg">
<meta property="article:published_time" content="2019-10-19T16:00:00.000Z">
<meta property="article:modified_time" content="2025-06-08T05:48:49.967Z">
<meta property="article:author" content="bugwz">
<meta property="article:tag" content="论文">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="rsync">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bugwz.com/assets/images/bg/paper.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "译 - The rsync algorithm",
  "url": "https://bugwz.com/2019/10/20/the-rsync-algorithm/",
  "image": "https://bugwz.com/assets/images/bg/paper.jpg",
  "datePublished": "2019-10-19T16:00:00.000Z",
  "dateModified": "2025-06-08T05:48:49.967Z",
  "author": [
    {
      "@type": "Person",
      "name": "bugwz",
      "url": "https://bugwz.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/assets/images/bg/favicon.png"><link rel="canonical" href="https://bugwz.com/2019/10/20/the-rsync-algorithm/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '译 - The rsync algorithm',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">124</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">132</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/assets/images/bg/paper.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">咕咕</span></a><a class="nav-page-title" href="/"><span class="site-name">译 - The rsync algorithm</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">译 - The rsync algorithm</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-10-19T16:00:00.000Z" title="发表于 2019-10-20 00:00:00">2019-10-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-08T05:48:49.967Z" title="更新于 2025-06-08 13:48:49">2025-06-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/%E6%9D%82%E9%A1%B9/">杂项</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.1k</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div>

<p><a target="_blank" rel="noopener" href="https://www.andrew.cmu.edu/course/15-749/READINGS/required/cas/tridgell96.pdf">《The rsync algorithm》</a>这篇发表于 1996 年的论文中介绍了一种名为 rsync 的增量同步算法，它能够快速地将两个文件夹中的内容同步。该算法利用了文件的局部性和差异性，通过计算文件的弱校验和和块校验和来确定文件的相似性，并进行增量同步。该算法具有高效性、可靠性和安全性等优点，在实际应用中被广泛使用。</p>
</div>

<h2 id="0、摘要"><a href="#0、摘要" class="headerlink" title="0、摘要"></a>0、摘要</h2><p>This report presents an algorithm for updating a file on one machine to be identical to a file on another machine. We assume that the two machines are connected by a low-bandwidth high-latency bi-directional communications link. The algorithm identifies parts of the source file which are identical to some part of the destination file, and only sends those parts which cannot be matched in this way. Effectively, the algorithm computes a set of differences without having both files on the same machine. The algorithm works best when the files are similar, but will also function correctly and reasonably efficiently when the files are quite different.</p>
<p>该报告中提出了一种算法，用于更新一台机器上的文件使其与另一台机器上的文件相同。 我们假设两台机器之间通过低带宽、高延迟的网络进行双向连接。 该算法能够识别出源文件与目标文件中相同的部分，并且只发送那些不一致的部分。 实际上，该算法可以计算出一个差异的数据集，而无需两个文件在同一个机器上。该算法在文件相似的情况下效果很好，在文件完全不同的情况下，它也能正确并合理的高效工作。</p>
<h2 id="1、问题"><a href="#1、问题" class="headerlink" title="1、问题"></a>1、问题</h2><p>Imagine you have two files, A and B, and you wish to update B to be the same as A. The obvious method is to copy A onto B.</p>
<p>假设您有两个文件，A 和 B，您希望将 B 的内容更新的和 A 一样。很明显最简单（显而易见）的方法是将 A 复制到 B。</p>
<p>Now imagine that the two files are on machines connected by a slow com- munications link, for example a dial up IP link. If A is large, copying A onto B will be slow. To make it faster you could compress A before sending it, but that will usually only gain a factor of 2 to 4.</p>
<p>现在假设这两个文件之间的通信链路十分缓慢，例如使用一个拨号上网的路由器。 如果 A 文件很大，那么将 A 文件复制到 B 的过程会十分缓慢。 为了使其速度更快，我们可以在发送之前压缩 A，但这通常只会增加 2 到 4 倍的传输效率。</p>
<p>Now assume that A and B are quite similar, perhaps both derived from the same original file. To really speed things up you would need to take advantage of this similarity. A common method is to send just the differences between A and B down the link and then use this list of differences to reconstruct the file.</p>
<p>现在假设 A 和 B 非常相似，可能都来自于同一个原始文件。 要想真正的提高传输速度，我们可以利用这种相似性。 一种常见的方法是仅传输 A 和 B 之间的差异，然后使用此差异列表来重建文件。</p>
<p>The problem is that the normal methods for creating a set of differences between two files rely on being able to read both files. Thus they require that both files are available beforehand at one end of the link. If they are not both available on the same machine, these algorithms cannot be used (once you had copied the file over, you wouldn’t need the differences). This is the problem that rsync addresses.</p>
<p>问题在于创建两个文件差异数据集的常规方法需要能够读取这两个文件。 因此，它们要求在传输开始前这两个文件在链接一端是存在的。如果这两个文件在同一个机器上不存在，则无法使用这些算法（一旦将文件复制过来，就不需要差异信息了）。这就是 rsync 解决的问题。</p>
<p>The rsync algorithm efficiently computes which parts of a source file match some part of an existing destination file. These parts need not be sent across the link; all that is needed is a reference to the part of the destination file. Only parts of the source file which are not matched in this way need to be sent verbatim. The receiver can then construct a copy of the source file using the references to parts of the existing destination file and the verbatim material.</p>
<p>rsync 算法能够高效地计算源文件与目标文件中匹配的部分。 这部分数据不需要通过链接发送；所需要的只是引用目标文件的部分数据。 只需要发送源文件中不匹配的分布数据。然后，接收者可以使用对现有目标文件部分内容的引用和逐字记录的材料（传输的差异数据）来构建源文件的副本。</p>
<p>Trivially, the data sent to the receiver can be compressed using any of a range of common compression algorithms, for further speed improvements.</p>
<p>通常，可以使用众多常用压缩算法中的任何一种来压缩待发送到接收器的数据，来进一步提高速度。</p>
<h2 id="2、rsync算法"><a href="#2、rsync算法" class="headerlink" title="2、rsync算法"></a>2、rsync算法</h2><p>Suppose we have two general purpose computers a and b. Computer a has access to a file A and b has access to file B, where A and B are “similar”. There is a slow communications link between a and b.</p>
<p>假设我们有两台通用计算机 a 和 b。 计算机 a 可以访问文件 A，b 可以访问文件 B，其中 A 和 B 是 “相似的” 。 a 和 b 之间的通信链路很慢。</p>
<p>The rsync algorithm consists of the following steps:</p>
<p>rsync算法包括以下步骤：</p>
<ol>
<li>b splits the file B into a series of non-overlapping fixed-sized blocks of size S bytes [1] . The last block may be shorter than S bytes.</li>
<li>For each of these blocks b calculates two checksums: a weak “rolling” 32-bit checksum (described below) and a strong 128-bit MD4 checksum.</li>
<li>b sends these checksums to a.</li>
<li>a searches through A to find all blocks of length S bytes (at any offset, not just multiples of S) that have the same weak and strong checksum as one of the blocks of B. This can be done in a single pass very quickly using a special property of the rolling checksum described below.</li>
<li>a sends b a sequence of instructions for constructing a copy of A. Each instruction is either a reference to a block of B, or literal data. Literal data is sent only for those sections of A which did not match any of the blocks of B.</li>
</ol>
<br />


<ol>
<li>b 将文件 B 拆分为一系列大小为 S 字节 [1] 的非重叠的固定大小的块。 最后一个块的大小可能小于 S 字节。</li>
<li>对于这些块中的每一个，b 会计算两个校验和：弱 “滚动” 32 位校验和（如下所述）和强 128 位 MD4 校验和。</li>
<li>b 将这些校验和发送给 a。</li>
<li>a 搜索 A 以找到所有长度为 S 字节的块（在任何偏移量，而不仅仅是 S 的倍数），这些块具有与 B 的块之一相同的弱校验和和强校验和。使用下面介绍的滚动校验和的特殊属性可以非常快速地一次完成此操作。</li>
<li>a 向 b 发送一系列指令，用于构造 A 的副本。每条指令要么是对 B 块的引用，要么是文字数据。 只有当 A 和 B 的不匹配的部分数据块才会发送文字数据。</li>
</ol>
<p>The end result is that b gets a copy of A, but only the pieces of A that are not found in B (plus a small amount of data for checksums and block indexes) are sent over the link. The algorithm also only requires one round trip, which minimises the impact of the link latency.</p>
<p>最终结果是 b 获得了 A 的副本，但是仅通过链路发送了 B 中找不到的 A 中的片段（以及很少的用于校验和和块索引的数据）。该算法只需要一次往返，从而能够最大限度的减少链路延迟的影响。</p>
<p>The most important details of the algorithm are the rolling checksum and the associated multi-alternate search mechanism which allows the all-offsets checksum search to proceed very quickly. These will be discussed in greater detail below.</p>
<p>该算法最重要的细节是滚动校验和以及相关联的多变量搜索机制，它使得能够非常快速的进行偏移量校验和搜索。 这些将在下面更详细地讨论。</p>
<h2 id="3、滚动校验和"><a href="#3、滚动校验和" class="headerlink" title="3、滚动校验和"></a>3、滚动校验和</h2><p>The weak rolling checksum used in the rsync algorithm needs to have the property that it is very cheap to calculate the checksum of a buffer X(2)..X(n+1) given the checksum of buffer X(1)..X(n) and the values of the bytes X(1) and X(n+1).</p>
<p>rsync 算法中使用的弱滚动校验和必须具有以下特性：在给定缓冲区 X(1) .. X(n) 的校验和的情况下，计算缓冲区 X(2) .. X(n +1) 的校验和以及字节 X(1) 和 X(n + 1) 非常方便 。</p>
<p>The weak checksum algorithm we used in our implementation was inspired by Mark Adler’s adler-32 checksum. Our checksum is defined by: </p>
<p>我们在实现中使用的弱校验和的算法灵感来自 Mark Adler 的 adler-32 校验和。 我们的校验和定义为：</p>
<div>

<p><img src="/assets/images/rsync-algorithm-1.png" loading="lazy"></p>
</div>

<div>

<p><img src="/assets/images/rsync-algorithm-2.png" loading="lazy"></p>
</div>

<div>

<p><img src="/assets/images/rsync-algorithm-3.png" loading="lazy"></p>
</div>


<p>where s(k, l) is the rolling checksum of the bytes X(k)..X(l). For simplicity and speed, we use M &#x3D; 2^16 .</p>
<p>其中 s(k, l) 是字节 X(k) .. X(l) 的滚动校验和。 为了简单和速度，我们使用 M &#x3D; 2^16 。</p>
<p>The important property of this checksum is that successive values can be computed very efficiently using the recurrence relation</p>
<p>该校验和的重要特性是可以使用递归关系非常高效地计算连续值</p>
<div>

<p><img src="/assets/images/rsync-algorithm-4.png" loading="lazy"></p>
</div>

<div>

<p><img src="/assets/images/rsync-algorithm-5.png" loading="lazy"></p>
</div>


<p>Thus the checksum can be calculated for blocks of length S at all possible offsets within a file in a  “rolling” fashion, with very little computation at each point.</p>
<p>因此，可以使用 “滚动” 的方式为文件内所有可能偏移量计算长度为 S 的块校验和，每个点的计算量非常少。</p>
<p>Despite its simplicity, this checksum was found to be quite adequate as a rst level check for a match of two file blocks. We have found in practice that the probability of this checksum matching when the blocks are not equal is quite low. This is important because the much more expensive strong checksum must be calculated for each block where the weak checksum matches.</p>
<p>尽管它很简单，但将这个校验和作为两个文件块匹配的第一级检查是足够的了。 我们在实践中发现，当块不相等时，这个校验和匹配的概率很低。 这很重要，否则的话就需要为弱校验和匹配的每个块计算更昂贵的强校验和。</p>
<h2 id="4、校验和搜索"><a href="#4、校验和搜索" class="headerlink" title="4、校验和搜索"></a>4、校验和搜索</h2><p>Once a has received the list of checksums of the blocks of B, it must search A for any blocks at any offset that match the checksum of some block of B. The basic strategy is to compute the 32-bit rolling checksum for a block of length S starting at each byte of A in turn, and for each checksum, search the list for a match. To do this our implementation uses a simple 3 level searching scheme.</p>
<p>一旦 a 收到来自 B 的块的校验和列表，它必须在 A 中搜索任何偏移量与 B 的某个块的校验和匹配的块。基本策略是从 A 的每个字节处开始计计算长度为 S 的块的 32 位滚动校验和，并且为每个校验和再列表中搜索匹配项。为此，我们的实现使用了一个简单的三级搜索方案。</p>
<p>The first level uses a 16-bit hash of the 32-bit rolling checksum and a 2^16 entry hash table. The list of checksum values (i.e., the checksums from the blocks of B) is sorted according to the 16-bit hash of the 32-bit rolling checksum. Each entry in the hash table points to the first element of the list for that hash value, or contains a null value if no element of the list has that hash value.</p>
<p>第一级使用 32 位滚动校验和的 16 位哈希和 2^16 个条目的哈希表。 根据 32 位滚动校验和的 16 位哈希对校验和值列表（即来自 B 块的校验和）进行排序。哈希表中的每个条目都指向该哈希值的列表的第一个元素，或者如果列表中没有元素具有该哈希值，则包含一个空值。</p>
<p>At each offset in the file the 32-bit rolling checksum and its 16-bit hash are calculated. If the hash table entry for that hash value is not a null value, the second level check is invoked.</p>
<p>在文件中的每个偏移处计算 32 位滚动校验和及其 16 位哈希。 如果该哈希值的哈希表条目不是空值，则调用第二级检查。</p>
<p>The third level check involves calculating the strong checksum for the current offset in the file and comparing it with the strong checksum value in the current list entry. If the two strong checksums match, we assume that we have found a block of A which matches a block of B. In fact the blocks could be different, but the probability of this is microscopic, and in practice this is a reasonable assumption.</p>
<p>第三级检查涉及计算文件中当前偏移量的强校验和，并将其与当前列表条目中的强校验和值进行比较。 如果两个强校验和匹配，我们假设我们找到了一个匹配 B 块的 A 块。事实上，块可能不同，但这种可能性很小，实际上这是一个合理的假设。</p>
<p>When a match is found, a sends b the data in A between the current file offset and the end of the previous match, followed by the index of the block in B that matched. This data is sent immediately a match is found, which allows us to overlap the communication with further computation.</p>
<p>当找到匹配项时，a 向 b 发送 A 中当前文件偏移量和上一个匹配项结尾之间的数据，然后是 B 中匹配的块的索引。 找到匹配项后立即发送此数据，这使我们可以将通信与进一步的计算重叠。</p>
<p>If no match is found at a given offset in the file, the rolling checksum is updated to the next offset and the search proceeds. If a match is found, the search is restarted at the end of the matched block. This strategy saves a considerable amount of computation for the common case where the two files are nearly identical. In addition, it would be a simple matter to encode the block indexes as runs, for the common case where a portion of A matches a series of blocks of B in order.</p>
<p>如果在文件中的给定偏移量处未找到匹配项，则滚动校验和将更新为下一个偏移量并继续搜索。 如果找到匹配项，则在匹配块的末尾重新开始搜索。 对于两个文件几乎相同的常见情况，此策略可以节省大量计算。 此外，对于 A 的一部分按顺序匹配 B 的一系列块的常见情况，将块索引编码为运行是一件简单的事情。</p>
<h2 id="5、流水线"><a href="#5、流水线" class="headerlink" title="5、流水线"></a>5、流水线</h2><p>The above sections describe the process for constructing a copy of one file on a remote system. If we have a several files to copy, we can gain a considerable latency advantage by pipelining the process.</p>
<p>以上部分描述了在远程系统上构建一个文件副本的过程。 如果我们有多个文件要复制，我们可以通过流水线化过程获得相当大的延迟优势。</p>
<p>This involves b initiating two independent processes. One of the processes generates and sends the checksums to a while the other receives the difference information from a and reconstructs the files.</p>
<p>这涉及 b 启动两个独立的进程。 其中一个进程生成校验和并将其发送给 a，而另一个进程从 a 接收差异信息并重建文件。</p>
<p>If the communications link is buffered then these two processes can proceed independently and the link should be kept fully utilised in both directions for most of the time.</p>
<p>如果通信链路被缓冲，那么这两个过程可以独立进行，并且在大多数时间里，链路应该在两个方向上都得到充分利用。</p>
<h2 id="6、结果"><a href="#6、结果" class="headerlink" title="6、结果"></a>6、结果</h2><p>To test the algorithm, tar files were created of the Linux kernel sources for two versions of the kernel. The two kernel versions were 1.99.10 and 2.0.0. These tar files are approximately 24MB in size and are separated by 5 released patch levels.</p>
<p>为了测试该算法，为两个版本的内核创建了 Linux 内核源代码的 tar 文件。 两个内核版本分别是 1.99.10 和 2.0.0。 这些 tar 文件大小约为 24MB，由 5 个发布的补丁级别分隔。</p>
<p>Out of the 2441 files in the 1.99.10 release, 291 files had changed in the 2.0.0 release, 19 files had been removed and 25 files had been added.</p>
<p>在 1.99.10 版本的 2441 个文件中，2.0.0 版本更改了 291 个文件，删除了 19 个文件，添加了 25 个文件。</p>
<p>A “diff” of the two tar files using the standard GNU diff utility produced over 32 thousand lines of output totalling 2.1 MB.</p>
<p>使用标准 GNU diff 实用程序对两个 tar 文件进行“diff”产生了 32,000 多行输出，总计 2.1 MB。</p>
<p>The following table shows the results for rsync between the two files with a varying block size. [2]</p>
<p>下表显示了具有不同块大小的两个文件之间的 rsync 结果。[2]</p>
<div>

<p><img src="/assets/images/rsync-algorithm-6.png" loading="lazy"></p>
</div>


<p>In each case, the CPU time taken was less than the time it takes to run “diff” on the two files. [3]</p>
<p>在每种情况下，占用的 CPU 时间都少于在两个文件上运行 “diff” 所花费的时间。 [3]</p>
<p>The columns in the table are as follows:</p>
<p>表中各列如下：</p>
<ul>
<li>block size : The size in bytes of the checksummed blocks.</li>
<li>matches : The number of times a block of B was found in A.</li>
<li>tag hits : The number of times the 16 bit hash of the rolling checksum matched a hash of one of the checksums from B.</li>
<li>false alarms : The number of times the 32 bit rolling checksum matched but the strong checksum didn’t.</li>
<li>data : The amount of file data transferred verbatim, in bytes.</li>
<li>written : The total number of bytes written by a including protocol overheads. This is almost all file data.</li>
<li>read : The total number of bytes read by a including protocol overheads. This is almost all checksum information.</li>
</ul>
<br />

<ul>
<li>块大小 ： 校验和块的大小（以字节为单位）。</li>
<li>匹配数 ： 在 A 中找到 B 块的次数。</li>
<li>标签点击数 ： 滚动校验和的 16 位散列与来自 B 的校验和之一的散列匹配的次数。</li>
<li>误报 ：32 位滚动校验和匹配但强校验和不匹配的次数。</li>
<li>数据 ： 逐字传输的文件数据量，以字节为单位。</li>
<li>写 ： 包括协议开销在内的写入的总字节数。 这几乎是所有文件数据。</li>
<li>读 ： 包括协议开销在内的读取的总字节数。 这几乎就是所有的校验和信息。</li>
</ul>
<p>The results demonstrate that for block sizes above 300 bytes, only a small fraction (around 5%) of the file was transferred. The amount transferred was also considerably less than the size of the diff file that would have been transferred if the diff&#x2F;patch method of updating a remote file was used.</p>
<p>结果表明，对于超过 300 字节的块大小，只有一小部分（大约 5%）的文件被传输。 如果使用更新远程文件的 diff&#x2F;patch 方法，传输的数量也大大小于 diff 文件的大小。</p>
<p>The checksums themselves took up a considerable amount of space, although much less than the size of the data transferred in each case. Each pair of checksums consumes 20 bytes: 4 bytes for the rolling checksum plus 16 bytes for the 128-bit MD4 checksum.</p>
<p>校验和本身占用了大量空间，尽管远小于每种情况下传输的数据大小。 每对校验和占用 20 个字节：4 个字节用于滚动校验和，另外 16 个字节用于 128 位 MD4 校验和。</p>
<p>The number of false alarms was less than 1&#x3D;1000 of the number of true matches, indicating that the 32 bit rolling checksum is quite good at screening out false matches.</p>
<p>误报数小于真实匹配数的1&#x3D;1000，说明 32 位滚动校验和非常适合筛选错误匹配。</p>
<p>The number of tag hits indicates that the second level of the checksum search algorithm was invoked about once every 50 characters. This is quite high because the total number of blocks in the file is a large fraction of the size of the tag hash table. For smaller files we would expect the tag hit rate to be much closer to the number of matches. For extremely large files, we should probably increase the size of the hash table.</p>
<p>标记命中数表示第二级校验和搜索算法大约每 50 个字符调用一次。 这是相当高的，因为文件中的块总数是标签哈希表大小的很大一部分。 对于较小的文件，我们希望标签命中率更接近匹配数。 对于非常大的文件，我们可能应该增加哈希表的大小。</p>
<p>The next table shows similar results for a much smaller set of files. In this case the files were not packed into a tar file first. Rather, rsync was invoked with an option to recursively descend the directory tree. The files used were from two source releases of another software package called Samba. The total source code size is 1.7 MB and the diff between the two releases is 4155 lines long totalling 120 kB.</p>
<p>下表显示了一组更小的文件的类似结果。 在这种情况下，文件没有先打包到 tar 文件中。 相反，调用 rsync 时带有递归下降目录树的选项。 使用的文件来自另一个名为 Samba 的软件包的两个源版本。 源代码总大小为 1.7 MB，两个版本之间的差异为 4155 行，总计 120 kB。</p>
<div>

<p><img src="/assets/images/rsync-algorithm-7.png" loading="lazy"></p>
</div>




<h2 id="7、可用性"><a href="#7、可用性" class="headerlink" title="7、可用性"></a>7、可用性</h2><p>An implementation of rsync which provides a convenient interface similar to the common UNIX command rcp has been written and is available for download from <a href="ftp://samba.anu.edu.au/pub/rsync">ftp://samba.anu.edu.au/pub/rsync</a>.</p>
<p>rsync 的实现提供了一个类似于通用 UNIX 命令 rcp 的方便接口，已经编写完成，可以从 <a href="ftp://samba.anu.edu.au/pub/rsync">ftp://samba.anu.edu.au/pub/rsync</a> 下载。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://bugwz.com">bugwz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://bugwz.com/2019/10/20/the-rsync-algorithm/">https://bugwz.com/2019/10/20/the-rsync-algorithm/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://bugwz.com" target="_blank">咕咕</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法</a><a class="post-meta__tags" href="/tags/rsync/">rsync</a></div><div class="post-share"><div class="social-share" data-image="/assets/images/bg/paper.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2019/10/20/command-rsync/" title="rsync指令的使用与算法解析 - 每周指令"><img class="cover" src="/assets/images/bg/tunnel.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">rsync指令的使用与算法解析 - 每周指令</div></div><div class="info-2"><div class="info-item-1">rsync命令是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件。rsync使用所谓的rsync算法来使本地和远程两个主机之间的文件达到同步，这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。 rsync是一个功能非常强大的工具，其命令也有很多功能特色选项，我们下面就对它的选项一一进行分析说明。 一、参数解析-v, --verbose 详细模式输出。-q, --quiet 精简输出模式。-c, --checksum 打开校验开关，强制对文件传输进行校验。-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。-r, --recursive 对子目录以递归模式处理。-R, --relative 使用相对路径信息。-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。--backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX...</div></div></div></a><a class="pagination-related" href="/2019/10/30/nginx-current-limiting/" title="Ngxin的限流方式"><img class="cover" src="/assets/images/bg/nginx.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Ngxin的限流方式</div></div><div class="info-2"><div class="info-item-1">一、简介Nginx的限流的实现，可以保证高并发场景下的服务的可用性，控制网络以及CPU&#x2F;内存负载，极端场景下还可以减小暴力破解对系统的危害性。Nginx本身自带了几个限流模块 ：  对客户端的限流模块：  ngx_http_limit_conn_module：按照连接数限流，限制单个IP的并发连接数； ngx_http_limit_req_module：按照请求速率限流，使用漏桶的方式限制请求的处理速率；   对服务端的限流模块：  ngx_http_upstream_module：用于定义可以由proxy_pass， fastcgi_pass， uwsgi_pass， scgi_pass， memcached_pass和 grpc_pass指令引用的服务器组；    二、限流模块2.1、ngx_http_limit_conn_module用于设置单IP最大允许的连接数，当超过该连接数，服务器将返回错误信息（默认错误码为503）。 http &#123;    limit_conn_zone $binary_remote_addr zone=one:10m;   ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2020/05/23/bloom-filter-summary-cache-paper/" title="译 - Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2020-05-23</div><div class="info-item-2">译 - Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol</div></div><div class="info-2"><div class="info-item-1">《Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol》翻译过来是 《摘要缓存：可扩展的广域 Web 缓存共享协议》，这篇文章中提出了布隆过滤器的设计背景以及实现原理，详细介绍了在误判率以及存储空间之间的权衡，之后很多系统中实现的布隆过滤器基本都是参考了这篇文论的实现。 摘要Web Proxy之间的共享缓存是减少Web流量并缓解网络瓶颈的一项重要技术。然而，由于现有协议的开销，它并未得到广泛部署。在本文中，我们演示了缓存共享的好处，衡量了现有协议的开销，并提出了一种称为”摘要缓存’’的新协议。在这个新协议中，每个Proxy都保留了一个包含所有Proxy的缓存摘要目录，并在任何查询之前都要检查在这些摘要之中是否存在潜在的匹配项。有两个因素利于我们协议的低开销：摘要的定期更新以及十分简朴的目录信息，每个条目只有8bits。通过使用跟踪驱动的仿真和原型实现，我们证明了与现有的协议（例如 Internet...</div></div></div></a><a class="pagination-related" href="/2022/03/01/scalable-bloom-filters/" title="译 - Scalable Bloom Filters"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-01</div><div class="info-item-2">译 - Scalable Bloom Filters</div></div><div class="info-2"><div class="info-item-1">  《Scalable Bloom Filters》 这篇论文讲述了一种布隆过滤器的变体实现方式，通过将预设的误判率分配给多个子布隆过滤器来约束整体的一个误判率情况，并且可以通过新增子布隆过滤器来实现对存储元素数量的调节，以满足初始容量无法准确估计的情况，论文中详细介绍了在不同的误判率变化率以及布隆过滤器容量变化率的情况下，存储空间等的使用情况。目前了解到的，RedisBloom 和 TairBloom 都参考了这篇论文实现了各自的布隆过滤器。     摘要Bloom Filters provide space-efficient storage of sets at the cost of a probability of false positives on membership queries. The size of the filter must be defined a priori based on the number of elements to store and the desired false positive probability, being...</div></div></div></a><a class="pagination-related" href="/2022/11/16/bitcoin/" title="译 - Bitcoin: A Peer-to-Peer Electronic Cash System"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-16</div><div class="info-item-2">译 - Bitcoin: A Peer-to-Peer Electronic Cash System</div></div><div class="info-2"><div class="info-item-1">  《Bitcoin: A Peer-to-Peer Electronic Cash System》 翻译过来是《 比特币：一种点对点的电子现金系统》 ，这篇文章是比特币的发明人中本聪于 2008 年发表的比特币白皮书。这篇文章介绍了比特币的设计背景，讲述了比特币的工作原理，是加密货币，区块链领域必读的一篇文章，其中讲述了很多巧妙的构思。作者翻译水平有限，翻译的语句可能会有一些出入，建议有能力的读者还是去阅读一下原文。   0、摘要A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is...</div></div></div></a><a class="pagination-related" href="/2023/06/20/crush/" title="译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-20</div><div class="info-item-2">译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</div></div><div class="info-2"><div class="info-item-1">  译作: 可控的、可扩展的、分布式的副本数据放置算法，论文原文 。 该论文于 2006 年 11 月发布于 SC2006 。 CRUSH 是一种用于大规模分布式存储系统的数据分布算法，它通过伪随机函数将数据对象映射到存储设备上，无需依赖中央目录。CRUSH 算法设计考虑了系统的动态性，支持在添加或移除存储设备时高效地重组数据，并最小化不必要的数据移动。此外，CRUSH 支持多种数据复制和可靠性机制，并允许根据用户定义的策略进行数据分布，这些策略能够在故障域之间有效地分离副本，增强数据安全性。 CRUSH 的核心是其层级集群图，该图描述了存储集群的物理和逻辑结构，并通过一系列规则来确定数据的放置位置。CRUSH 算法通过将数据均匀分布在加权设备上，保持存储和设备带宽资源的平衡利用。算法还考虑了设备的故障和过载情况，能够在设备发生故障或过载时重新分配数据，避免数据丢失并优化系统性能。 CRUSH 的映射性能高效，计算复杂度为 O(logn) ，适用于管理大规模（多 PB...</div></div></div></a><a class="pagination-related" href="/2019/10/14/dynamo/" title="转&#x2F;译-Dynamo:Amazon的高可用键值存储"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2019-10-14</div><div class="info-item-2">转&#x2F;译-Dynamo:Amazon的高可用键值存储</div></div><div class="info-2"><div class="info-item-1">本文翻译自 2007 年 Amazon 的分布式存储经典论文：《Dynamo: Amazon’s Highly Available Key-value Store》)，直译为 《Dynamo：Amazon 的高可用键值存储》，这里对排版做了一些调整，以更适合 web 阅读。 Dynamo 是 Amazon 的高可用分布式键值存储（key&#x2F;value storage）系统。这篇论文发表 的时候（2007）它还只是一个内部服务，现在（改名为 DynamoDB）已经发展成 AWS 最核心 的存储产品（服务）之一，与 S3 等并列。据了解，国内某一线大厂的公有云键值 存储服务，也是参考这篇文章设计和实现的。 现在提到键值存储，大家首先想到的可能是 Redis，那么 Dynamo 和 Redis 是不是竞品， 只是一个开源一个是商业的？不是的，二者针对的场景不同，这里非常粗地列举几方面：  使用场景：Dynamo 定位是永远可写（always writable）的持久文件系统，Redis 主要用作（易失）缓存或内存数据库 存储方式：Dynamo 是磁盘，Redis...</div></div></div></a><a class="pagination-related" href="/2022/09/24/gorilla-cn/" title="译 - Gorilla: A Fast, Scalable, In-Memory Time Series Database"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-24</div><div class="info-item-2">译 - Gorilla: A Fast, Scalable, In-Memory Time Series Database</div></div><div class="info-2"><div class="info-item-1">  《Gorilla: A Fast, Scalable, In-Memory Time Series Database》 这篇论文讲述了 Facebook 在存储时序数据模型时的一些实践，重点讲述了他们内部的一款内存型的时序数据库 Gorilla。论文中通过使用 Delta-Of-Delta 和 XOR 方式分别对时序数据的时间戳以及浮点数据进行压缩编码，极大的节省了时序数据的存储开销，这也成为了业界时序数据库主流的数据编码压缩方式。这篇论文是时序数据库领域必读的一篇文章。        摘要Large-scale internet services aim to remain highly available and responsive in the presence of unexpected failures. Providing this service often requires monitoring and analyzing tens of millions of measurements per second across a large number...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/assets/images/bg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">bugwz</div><div class="author-info-description">持续学习，持续进步</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">124</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">132</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/bugwz" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0%E3%80%81%E6%91%98%E8%A6%81"><span class="toc-text">0、摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E9%97%AE%E9%A2%98"><span class="toc-text">1、问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81rsync%E7%AE%97%E6%B3%95"><span class="toc-text">2、rsync算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E6%BB%9A%E5%8A%A8%E6%A0%A1%E9%AA%8C%E5%92%8C"><span class="toc-text">3、滚动校验和</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E6%A0%A1%E9%AA%8C%E5%92%8C%E6%90%9C%E7%B4%A2"><span class="toc-text">4、校验和搜索</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E6%B5%81%E6%B0%B4%E7%BA%BF"><span class="toc-text">5、流水线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E7%BB%93%E6%9E%9C"><span class="toc-text">6、结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E5%8F%AF%E7%94%A8%E6%80%A7"><span class="toc-text">7、可用性</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph Crimson 集群搭建指南"/></a><div class="content"><a class="title" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群搭建指南">Ceph Crimson 集群搭建指南</a><time datetime="2025-01-11T16:00:00.000Z" title="发表于 2025-01-12 00:00:00">2025-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph QoS 机制深入分析"/></a><div class="content"><a class="title" href="/2024/10/25/ceph-qos/" title="Ceph QoS 机制深入分析">Ceph QoS 机制深入分析</a><time datetime="2024-10-24T16:00:00.000Z" title="发表于 2024-10-25 00:00:00">2024-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/30/ceph-crush/" title="Ceph CRUSH 实现细节分析"><img src="/assets/images/bg/ceph.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ceph CRUSH 实现细节分析"/></a><div class="content"><a class="title" href="/2023/06/30/ceph-crush/" title="Ceph CRUSH 实现细节分析">Ceph CRUSH 实现细节分析</a><time datetime="2023-06-29T16:00:00.000Z" title="发表于 2023-06-30 00:00:00">2023-06-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/20/crush/" title="译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data"><img src="/assets/images/bg/paper.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data"/></a><div class="content"><a class="title" href="/2023/06/20/crush/" title="译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data">译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</a><time datetime="2023-06-19T16:00:00.000Z" title="发表于 2023-06-20 00:00:00">2023-06-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/10/rush/" title="译 - Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution"><img src="/assets/images/bg/paper.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="译 - Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution"/></a><div class="content"><a class="title" href="/2023/06/10/rush/" title="译 - Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution">译 - Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution</a><time datetime="2023-06-09T16:00:00.000Z" title="发表于 2023-06-10 00:00:00">2023-06-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By bugwz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="paperlayout-btn" type="button" title="论文布局"><i class="fas fa-language"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: '6af3be16b94cec39bcf6',
      clientSecret: '13a5202ff773ffcea6300b6c8ff25f455566737c',
      repo: 'bugwz.github.io',
      owner: 'bugwz',
      admin: ['bugwz'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || '205f56409b3e458cd830ba1db58a9114'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="docsearch-wrap"><div id="docsearch" style="display:none"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css/dist/style.min.css"/><script src="https://cdn.jsdelivr.net/npm/@docsearch/js/dist/umd/index.min.js"></script><script>(() => {
  docsearch(Object.assign({
    appId: 'PFB3WGSSCO',
    apiKey: '3e9cd446e41d93f2f130b91698b699f7',
    indexName: 'bugwz',
    container: '#docsearch',
    placeholder: '请输入要搜索的内容',
  }, {"maxResultsPerGroup":10}))

  const handleClick = () => {
    document.querySelector('.DocSearch-Button').click()
  }

  const searchClickFn = () => {
    btf.addEventListenerPjax(document.querySelector('#search-button > .search'), 'click', handleClick)
  }

  searchClickFn()
  window.addEventListener('pjax:complete', searchClickFn)
})()</script></div></div></body></html>