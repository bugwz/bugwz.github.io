<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>译 - Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol | 咕咕</title><meta name="author" content="bugwz"><meta name="copyright" content="bugwz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="《Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol》翻译过来是 《摘要缓存：可扩展的广域 Web 缓存共享协议》，这篇文章中提出了布隆过滤器的设计背景以及实现原理，详细介绍了在误判率以及存储空间之间的权衡，之后很多系统中实现的布隆过滤器基本都是参考了这篇文论的实现。"><meta property="og:type" content="article"><meta property="og:title" content="译 - Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol"><meta property="og:url" content="https://bugwz.com/2020/05/23/bloom-filter-summary-cache-paper/index.html"><meta property="og:site_name" content="咕咕"><meta property="og:description" content="《Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol》翻译过来是 《摘要缓存：可扩展的广域 Web 缓存共享协议》，这篇文章中提出了布隆过滤器的设计背景以及实现原理，详细介绍了在误判率以及存储空间之间的权衡，之后很多系统中实现的布隆过滤器基本都是参考了这篇文论的实现。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://bugwz.com/assets/images/bg/paper.jpg"><meta property="article:published_time" content="2020-05-22T16:00:00.000Z"><meta property="article:modified_time" content="2025-12-16T13:38:47.370Z"><meta property="article:author" content="bugwz"><meta property="article:tag" content="布隆过滤器"><meta property="article:tag" content="算法"><meta property="article:tag" content="论文"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bugwz.com/assets/images/bg/paper.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "译 - Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol",
  "url": "https://bugwz.com/2020/05/23/bloom-filter-summary-cache-paper/",
  "image": "https://bugwz.com/assets/images/bg/paper.jpg",
  "datePublished": "2020-05-22T16:00:00.000Z",
  "dateModified": "2025-12-16T13:38:47.370Z",
  "author": [
    {
      "@type": "Person",
      "name": "bugwz",
      "url": "https://bugwz.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/assets/images/bg/favicon.png"><link rel="canonical" href="https://bugwz.com/2020/05/23/bloom-filter-summary-cache-paper/index.html"><link rel="preconnect"/><link rel="preload" as="style" href="/css/index.css" onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel="stylesheet" href="/css/index.css"></noscript><link rel="stylesheet" href="/pluginsSrc/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><noscript><link rel="stylesheet" href="/pluginsSrc/@fortawesome/fontawesome-free/css/all.min.css"></noscript><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:500,highlightFullpage:!1,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"/pluginsSrc/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"译 - Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/self/github-dark.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 8.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/assets/images/bg/avatar-256.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"/ loading='lazy'></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">134</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">135</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span>友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/assets/images/bg/paper.jpg)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">咕咕</span></a><a class="nav-page-title" href="/"><span class="site-name">译 - Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><span>友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">译 - Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-05-22T16:00:00.000Z" title="发表于 2020-05-23 00:00:00">2020-05-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-16T13:38:47.370Z" title="更新于 2025-12-16 21:38:47">2025-12-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/%E6%9D%82%E9%A1%B9/">杂项</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">18k</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p><a target="_blank" rel="noopener" href="http://pages.cs.wisc.edu/~jussara/papers/00ton.pdf">《Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol》</a>翻译过来是 《摘要缓存：可扩展的广域 Web 缓存共享协议》，这篇文章中提出了布隆过滤器的设计背景以及实现原理，详细介绍了在误判率以及存储空间之间的权衡，之后很多系统中实现的布隆过滤器基本都是参考了这篇文论的实现。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Web Proxy之间的共享缓存是减少Web流量并缓解网络瓶颈的一项重要技术。然而，由于现有协议的开销，它并未得到广泛部署。在本文中，我们演示了缓存共享的好处，衡量了现有协议的开销，并提出了一种称为”摘要缓存’’的新协议。在这个新协议中，每个Proxy都保留了一个包含所有Proxy的缓存摘要目录，并在任何查询之前都要检查在这些摘要之中是否存在潜在的匹配项。有两个因素利于我们协议的低开销：摘要的定期更新以及十分简朴的目录信息，每个条目只有<strong>8bits</strong>。通过使用跟踪驱动的仿真和原型实现，我们证明了与现有的协议（例如 Internet 的缓存协议ICP）相比，”摘要缓存”将<strong>缓存间协议消息的数量减少了25到60</strong>，带宽消耗<strong>减少了超过50%</strong>，<strong>消除了75%到95%的CPU处理协议开销</strong>，同时<strong>保持了与ICP几乎相同的缓存命中率</strong>。因此”摘要缓存”可以扩展到大量的Proxy。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>随着万维网的巨大增长给互联网带来的持续压力，缓存已经被认为是减少带宽消耗的最重要的技术之一[29]。特别是Web代理中的缓存已经被证明十分有效[14,33]。为了获得缓存的全部好处，常见瓶颈环节后面的代理环境应该相互配合并未彼此的未命中服务，从而进一步减少通过瓶颈的流量。我们称这个过程为 “Web缓存共享”。</p><p>Web共享缓存的概念最初是在Harvest项目中被提出的[26,12]。Harvest小组设计了Internet缓存协议（ICP）[18]，该协议支持从相邻的缓存中发现和检索文档。如今，很多机构和很多国家都建立了代理缓存的层次结构，这些层次结构通过ICP进行合作已减少Internet流量[25,30,41,5,14]。</p><p>然而，ICP协议的开销目前阻碍了Web共享缓存的广泛部署。ICP在代理发生缓存未命中的时候会讲查询消息多播到其他的代理来尝试在其他代理中命中该消息。因此随着代理数量的增加，通信和CPU的处理开销都将成倍的增加。</p><p>目前已经提出了很多替代协议来解决该问题，例如，一种在代理之间划分URL空间的缓存陈列路由协议[46]。但是，此类解决方案通常不适用于广域网缓存共享，其特点是代理之间的网络带宽有限以及代理与其用户之间的网络距离不均匀（例如，每个代理可能更靠近一个用户组而不是其他用户组）。</p><p>在本文中，我们解决了用于关于Web共享缓存的可伸缩协议的问题。我们首先通过分析Web访问跟踪的采集信息来检查Web共享缓存的好处。我们表明，在代理之间共享缓存的内存会显著减少Web服务器的通信量，并可简单的缓存共享（无需在代理的缓存之间进行协调）就足以获得完全协调的缓存的大部分好处。我们还通过运行一组代理基准来量化ICP协议的开销。结果表明，即使协作代理的数量低至四个，ICP也会讲代理间的流量增加70到90倍，每个代理接收到的网络数据包的数量也增加了13%甚至更多，并且CPU的开销也超过了15%。​ 在没有代理间缓存命中（也称为远程缓存命中）的情况下，开销可以使平均用户延迟增加多达11%。</p><p>然后，我们提出了一种称为摘要缓存的新缓存共享协议。在此协议下，每个代理都保留每个其他代理的缓存目录的简明摘要。当发生高速缓存未命中时，代理首先探测所有的摘要信息，以查看该请求是否可能是其他代理中的高速缓存命中，然后仅将查询消息发送给其摘要显示出有希望的结果的那些代理。摘要信息不一定总是准确的。如果摘要中显示的请求并没有缓存命中（错误命中），则结果就是浪费了查询消息。如果摘要中另有说明（虚假未命中）时请求是缓存命中，则结果就是较高的未命中率。</p><p>我们研究了协议设计中的两个关键问题：摘要更新的频率和摘要的表现方式。使用跟踪驱动的模拟，我们显示摘要的更新可以延迟到新的固定百分比（例如<code>1％</code>）的缓存文档，并且命中率将成比例降低（对于<code>1％</code>的选择，降级为介于<code>0.02％</code>至<code>1.7％</code>之间，具体取决于轨迹）。</p><p>为了减少内存需求，我们将每个摘要存储为”布隆过滤器”。这是一种计算效率非常高的基于哈希的概率方案，它可以表示一组具有最低内存要求的键（在我们的情况下为高速缓存目录），同时成员查询的假阴性概率为0，误报率很低。跟踪驱动的模拟表明，对于典型的代理配置，对于仅在N个字节内表示的N个缓存文档，误报的百分比为<code>1％</code>至<code>2％</code>。实际上，可以通过增加误报率为代价而进一步减少存储器。（我们稍后将更详细地描述Bloom过滤器。）</p><p>基于这些结果，我们设计了摘要缓存增强的ICP协议，并在Squid代理中实现了原型。通过使用跟踪驱动的仿真以及基准测试和跟踪重放的实验，我们证明了新协议将<strong>代理间消息的数量减少了25到60倍以上，网络带宽的消耗（以传输的字节数表示） ）降低了50％以上，并减少了30％到95％的协议在CPU上开销</strong>。与没有缓存共享的情况相比，我们的实验表明，该协议只产生很少的网络流量，并且<strong>仅将CPU时间增加5％到12％</strong>，具体取决于远程缓存命中率。但是，该协议在大多数情况下都达到了类似于ICP协议的缓存命中率。</p><p>结果表明，摘要缓存增强型ICP协议可以扩展到大量代理。因此，它有可能显著增加Web缓存共享的部署并减少Internet上的Web流量。为此，我们正在将我们的实施公开发布[ 15 ]，并且正在将其转移到ICP用户社区。</p><h2 id="轨迹和模拟"><a href="#轨迹和模拟" class="headerlink" title="轨迹和模拟"></a>轨迹和模拟</h2><p><strong>表1</strong>：有关跟踪的统计信息。 命中率和字节命中率是在无限缓存下实现的。</p><table><thead><tr><th align="center">Traces</th><th align="center">DEC</th><th align="center">UCB</th><th align="center">UPisa</th><th align="center">Questnet</th><th align="center">NLANR</th></tr></thead><tbody><tr><td align="center">Time(时间)</td><td align="center">8&#x2F;29-9&#x2F;4, 1996</td><td align="center">9&#x2F;14-9&#x2F;19, 1996</td><td align="center">Jan-March, 1997</td><td align="center">1&#x2F;15-1&#x2F;21, 1998</td><td align="center">12&#x2F;22, 1997</td></tr><tr><td align="center">Requests(请求数)</td><td align="center">3543968</td><td align="center">1907762</td><td align="center">2833624</td><td align="center">2885285</td><td align="center">1766409</td></tr><tr><td align="center">Infinite Cache Size(无限缓存大小)</td><td align="center">2.88e+10</td><td align="center">1.80e+10</td><td align="center">2.07e+10</td><td align="center">2.33e+10</td><td align="center">1.37e+10</td></tr><tr><td align="center">Maximum Hit Ratio(最大命中率)</td><td align="center">0.49</td><td align="center">0.30</td><td align="center">0.40</td><td align="center">0.30</td><td align="center">0.36</td></tr><tr><td align="center">Maximum ByteHit Ratio(最大字节命中率)</td><td align="center">0.36</td><td align="center">0.14</td><td align="center">0.27</td><td align="center">0.15</td><td align="center">0.27</td></tr><tr><td align="center">Client Population(客户人数)</td><td align="center">10089</td><td align="center">5780</td><td align="center">2203</td><td align="center">12</td><td align="center">4</td></tr><tr><td align="center">Client Groups(客户群)</td><td align="center">16</td><td align="center">8</td><td align="center">8</td><td align="center">12</td><td align="center">4</td></tr></tbody></table><p>在我们的研究中，我们收集了五组HTTP请求的痕迹。<strong>表1</strong>中列出了每个跟踪中的请求数，客户端数以及其他统计信息 。特别是，<strong>表1</strong>列出了每个跟踪的”无限’’缓存大小，即跟踪中唯一文档的总大小（以字节为单位）（即”无限’’缓存的大小，不会导致缓存的替换）。</p><ul><li><p><strong>DEC</strong>跟踪[32]：Digital Equipment Corporation Web代理服务器跟踪，为大约<code>17,000个工作站</code>提供服务。跟踪持续25天（1996年8月29日至9月21日）。我们将跟踪分为三个1周和1个半周的跟踪。由于交换空间的限制，我们的模拟器只能模拟子迹线。在本文中，我们介绍了<code>1996年8月29日</code>至<code>9月4日</code>这一周的迹线结果。其他迹线的结果非常相似。</p></li><li><p><strong>UCB</strong>跟踪[24]：从UC Berkeley向其学生，教职员工提供的家庭IP服务收集的HTTP请求的跟踪。从1996年11月1日到11月19日，总迹线为期18天，并分为四个子迹线，每四或五天一次。我们在11月14日至11月19日的跟踪中显示结果。尽管该跟踪最初记录了2,468,890个请求，但其中许多响应数据大小为0或1，因此我们决定忽略这些请求。同样，我们在UCB集合中的其他迹线上进行了仿真，结果与此处介绍的相似。</p></li><li><p><strong>UPisa</strong>跟踪[43]：意大利比萨大学计算机科学系的用户在1997年1月至3月的三个月内对HTTP请求的跟踪。在跟踪中，我们仅模拟GET请求，并且仅网址不包含查询字符串的用户，因为大多数代理不缓存查询请求。</p></li><li><p><strong>Questnet</strong>追踪[47]：1998年1月15日至1月21日，父级代理在Questnet（澳大利亚的区域网络）中看到的HTTP请求日志为7天，这些代理服务于大约12个父级代理区域网络中的子代理。我们提取父代理看到的成功GET请求。因此，跟踪只是进入十个代理的用户请求的子集。不幸的是，用户对代理的完整请求集不可用。</p></li><li><p><strong>NLANR</strong>跟踪 [40]：由NLANR（应用网络研究国家实验室）向国家高速缓存层次结构中的四个主要父代理高速缓存发送HTTP请求的一日日志（1997年12月22日）。国家缓存层次结构中大约有八个代理，但是只有四个代理（“ bo”，“ pb”，“ sd”和“ uc”）处理来自.com，.net，.edu和其他主要领域。因此，我们决定仅模拟对四个代理的请求。</p></li></ul><p>在我们的缓存共享模拟中，我们将<code>DEC</code>，<code>UCB</code>和<code>UPisa</code>中的客户端分为几组，假设每个组都有自己的代理，并模拟代理之间的缓存共享。这大致对应于以下情况：</p><p>公司的每个分支机构或大学中的每个部门都有自己的代理缓存，并且这些缓存协作。我们将DEC，UCB和UPisa迹线中的组数分别设置为<code>16</code>、<code>8</code>和<code>8</code>。如果客户端的clientID与组大小相等，则将其放入组中。尽管该模拟并不完全符合实际情况，但我们相信它确实带来了有关缓存共享协议的见识。Questnet跟踪包含从区域网络中的一组子代理到父代理的HTTP请求。我们假设这些是进入子代理的请求（因为子代理将其缓存未命中发送给父代理），并且模拟了子代理之间的缓存共享。最后，NLANR跟踪包含去往四个主要代理的实际HTTP请求，我们模拟了它们之间的缓存共享。</p><p>在所有模拟中，我们将LRU用作缓存替换算法，但要注意的是，不能缓存大于250KB的文档。该策略类似于实际代理中使用的策略。我们不会根据年龄或生存时间来模拟到期的文档。而是，我们的大多数跟踪记录都包含每个请求的文档的上次修改时间，如果用户请求命中了上次修改时间已更改的文档，我们会将其视为缓存未命中。换句话说，我们假设缓存一致性机制是完美的。在实践中，有各种协议[12]，[34]，[28]为Web缓存的一致性。</p><p>我们的大多数模拟都假设缓存大小是”无限’’缓存大小的10％。研究表明，该”无限”的缓存大小的10％通常实现关于最大缓存命中率[90％ [49]，[8]，[35] ]。我们还执行了高速缓存大小为无限高速缓存大小5％的仿真，结果非常相似。</p><h2 id="缓存共享的好处"><a href="#缓存共享的好处" class="headerlink" title="缓存共享的好处"></a>缓存共享的好处</h2><p>最近的研究 [8],[23],[14]表明，在无限的缓存容量下，Web缓存命中率似乎与缓存所服务的用户数量成对数增长。 显然，来自不同用户的请求重叠减少了冷数据的miss，这通常是高速缓存未命中的重要部分，因为首次引用文档和文档修改都会对它们造成影响。</p><p><img src="/assets/images/summary-cache-figure-1.png" alt="图1：不同协作缓存方案下的缓存命中率（与字节命中率的结果相似，请注意，x轴为对数刻度）" loading="lazy" loading='lazy'></p><p>为了检查有限缓存大小下的缓存共享的好处，我们使用上一节中列出的跟踪信息模拟以下方案：</p><ul><li><strong>无缓存共享</strong>：代理通过不协作的方式来服务彼此的缓存未命中；</li><li><strong>简单的缓存共享</strong>：代理服务彼此的缓存未命中。代理从另一个代理中获取文档后，便会在本地缓存该文档。代理通过不协调的方式进行缓存替换。这是由ICP协议实现的共享；</li><li><strong>单副本缓存共享</strong>：代理服务彼此的缓存未命中，但是一个代理不缓存从另一个代理获取的文档。而是，另一个代理将文档标记为最近访问的文档，并增加了其缓存优先级。与<strong>简单的缓存共享</strong>相比，此方案消除了重复副本的存储并提高了可用缓存空间的利用率；</li><li><strong>全局缓存</strong>：代理共享缓存内容并协调替换，以便它们显示为一个统一的缓存，对用户而言具有全局LRU替换。这是协作缓存的完全协调形式。我们通过假设所有请求都转到一个缓存的大小来模拟该方案，该缓存的大小是所有代理缓存大小的总和；.</li></ul><p>为了回答两个问题，我们研究了这些方案：简单的缓存共享是否会显著减少Web服务器的流量，更紧密的协调方案是否会导致命中率显著提高。</p><p><strong>图1</strong> 显示了当每个跟踪的缓存大小分别设置为”无限缓存大小’’（完全避免替换所需的最小高速缓存大小）大小的 <code>0.5％</code>，<code>5％</code>，<code>10％</code> 和 <code>20％</code> 时考虑的不同方案下的命中率。字节命中率的结果非常相似，由于空间限制，我们将其省略。</p><p>通过查看 <strong>图1</strong>，我们了解到，首先所有缓存共享方案比没有缓存共享显著提高了命中率。结果充分证实了共享缓存的好处，即使使用了很小的缓存。</p><p>其次，<strong>单拷贝缓存共享</strong>和<strong>简单的缓存共享</strong>下的命中率通常与<strong>全局缓存</strong>下的命中率相同甚至更高。我们认为，原因是全局LRU有时表现不如逐组LRU。特别是，在<strong>全局缓存</strong>设置中，来自一个用户的快速连续请求突发可能会干扰许多用户的工作集。在<strong>单副本缓存共享</strong>或<strong>简单缓存共享</strong>中，每个缓存专用于特定的用户组，并且来自每个组的流量争夺单独的缓存空间。因此，缓存穿透只包含在特定组中。</p><p>第三，将<strong>单副本缓存共享</strong>与<strong>简单的缓存共享</strong>进行比较时，我们发现浪费空间仅会产生很小的影响。原因是有效缓存略小，命中率没有明显差异。为了证明这一点，我们还使用比原始缓存小 <code>10％</code> 的<strong>全局缓存</strong>运行模拟。从 <strong>图1</strong> 可以看出，差异很小。</p><p>因此，尽管它很简单，但 <code>ICP</code> 类型的<strong>简单的缓存共享</strong>却获得了更精细的协作缓存的大部分好处。** 简单的缓存共享**不会通过将内容从繁忙的缓存移动到较不繁忙的缓存来执行任何负载平衡，并且无法通过仅保留每个文档的一个副本来节省空间。 但是，如果正确地完成了每个代理的资源规划，则无需执行负载平衡并产生更紧密协调方案的开销。</p><p>最后，请注意该结果是根据第2节中所述的LRU替换算法获得的。不同的替换算法[8]可能给出不同的结果。 此外，单独的仿真已确认，在严重的负载不平衡的情况下，<strong>全局缓存</strong>将具有更好的缓存命中率，因此，重要的是分配每个代理的缓存大小，使其与用户群大小和预期使用成比例。</p><h2 id="ICP的开销"><a href="#ICP的开销" class="headerlink" title="ICP的开销"></a>ICP的开销</h2><p><strong>表2</strong>：在四个代理情况下的ICP开销。 SC-ICP协议在第6节中介绍，稍后将进行说明。 实验进行了3次，每次测量的方差在括号中列出。 开销信息的行列出了每次测量的无ICP百分比增加。 请注意，在合成实验中没有代理间缓存命中。</p><table><thead><tr><th>Exp 1</th><th>Hit Ratio</th><th>Client Latency</th><th>User CPU</th><th>System CPU</th><th>UDP Msgs</th><th>TCP Msgs</th><th>Total Packets</th></tr></thead><tbody><tr><td>no ICP</td><td>25%</td><td>2.75 (5%)</td><td>94.42 (5%)</td><td>133.65 (6%)</td><td>615 (28%)</td><td>334K (8%)</td><td>355K(7%)</td></tr><tr><td>ICP</td><td>25%</td><td>3.07 (0.7%)</td><td>116.87 (5%)</td><td>146.50 (5%)</td><td>54774 (0%)</td><td>328K (4%)</td><td>402K (3%)</td></tr><tr><td><strong>Overhead</strong></td><td></td><td>12%</td><td>24%</td><td>10%</td><td>9000%</td><td>2%</td><td>13%</td></tr><tr><td>SC-ICP</td><td>25%</td><td>2.85 (1%)</td><td>95.07 (6%)</td><td>134.61 (6%)</td><td>1079 (0%)</td><td>330K (5%)</td><td>351K (5%)</td></tr><tr><td><strong>Overhead</strong></td><td></td><td>4%</td><td>0.7%</td><td>0.7%</td><td>75%</td><td>-1%</td><td>-1%</td></tr><tr><td><strong>Exp 2</strong></td><td><strong>Hit Ratio</strong></td><td><strong>Client Latency</strong></td><td><strong>User CPU</strong></td><td><strong>System CPU</strong></td><td><strong>UDP Msgs</strong></td><td><strong>TCP Msgs</strong></td><td><strong>Total Packets</strong></td></tr><tr><td>no ICP</td><td>45%</td><td>2.21 (1%)</td><td>80.83 (2%)</td><td>111.10 (2%)</td><td>540 (3%)</td><td>272K (3%)</td><td>290K (3%)</td></tr><tr><td>ICP</td><td>45%</td><td>2.39 (1%)</td><td>97.36 (1%)</td><td>118.59 (1%)</td><td>39968 (0%)</td><td>257K (2%)</td><td>314K (1%)</td></tr><tr><td><strong>Overhead</strong></td><td></td><td>8%</td><td>20%</td><td>7%</td><td>7300%</td><td>-1%</td><td>8%</td></tr><tr><td>SC-ICP</td><td>45%</td><td>2.25 (1%)</td><td>82.03 (3%)</td><td>111.87 (3%)</td><td>799 (5%)</td><td>269K (5%)</td><td>287K (5%)</td></tr><tr><td><strong>Overhead</strong></td><td></td><td>2%</td><td>1%</td><td>1%</td><td>48%</td><td>-1%</td><td>-1%</td></tr></tbody></table><p>Internet缓存协议（ICP）[18]在鼓励世界各地的Web缓存共享实践方面非常成功。 它需要代理之间的松散协调，并且基于UDP构建以提高效率。 它是由Harvest研究小组[26]设计的，并得到了公共领域Squid [19]代理软件和当今一些商业产品的支持。 随着Squid代理在全球的部署，ICP被全球很多国家广泛使用，以减少跨大西洋和跨太平洋链接的流量。</p><p>尽管ICP取得了成功，但它并不是一个可扩展的协议。 问题在于ICP依赖查询来查找远程缓存命中。 每当一个代理遇到高速缓存未命中时，其他所有人都会收到查询消息并进行处理。 随着协作代理服务器数量的增加，开销很快就会让人望而却步。</p><p>为了衡量ICP的开销及其对代理性能的影响，我们使用了我们设计的代理基准进行了实验[1]。 （该基准已作为行业标准基准的候选者提交给SPEC，目前已在许多代理系统供应商中使用）。该基准测试由一系列客户端进程组成，这些客户端进程按照实际跟踪中观察到的模式（包含请求大小的分布和时间局限性）发出请求， 以及一组服务器的进程，这些服务器进程会延迟答复以模拟Internet中的延迟。</p><p>实验是在 <code>10</code> 个与 <code>100Mb/s</code> 以太网连接的 <code>Sun Sparc-20</code> 工作站上进行的。四个工作站充当四个代理系统，运行 <code>Squid 1.1.14</code> ，每个工作站具有75MB的缓存空间。缓存大小被人为的变小，因此在实验的短时间内就会发生缓存替换。另外四个工作站运行 <code>120</code> 个客户端进程，每个工作站上运行 <code>30</code> 个进程。每个工作站上的客户端进程都连接到代理之一。客户进程发出请求时没有思考时间，请求的文档大小遵循Pareto分布，其中 $ \alpha &#x3D; 1.1 $ 和 $k &#x3D; 3.0$ [9]。最后，两个工作站充当服务器，每个工作站监听 <code>15</code> 个不同的端口。Web服务器在处理HTTP请求时复制一个进程，该进程等待 <code>1秒钟</code>，然后发送答复以模拟网络延迟。</p><p>我们尝试使用两种不同的缓存命中率，分别为 <code>25％</code> 和 <code>45％</code>，因为ICP的开销随每个代理中的缓存未命中率而变化。在基准测试中，客户端按照[35,8]中观察到的时间局部性模式发出请求，并且可以调整请求流中的固有缓存命中率。在一个实验中，每个客户端进程发出 <code>200</code> 个请求，总共 <code>24000</code>个请求。</p><p>通过使用基准测试，我们比较了两种配置：<strong>无ICP（代理不协作）</strong> 和 <strong>ICP（代理通过ICP协作）</strong>。由于我们仅对负载开销感兴趣，因此客户端发出的请求不会重叠，并且代理之间没有远程缓存命中。对于ICP，这是最坏的情况，结果可以衡量协议的开销。我们在no-ICP和ICP实验中的随机数生成器中使用相同的种子，以确保可比较的结果，否则的话，繁重文档的大小分布和我们的低请求数会导致高方差。在不同种子环境下，无ICP和ICP之间的相对差异是相同的。 我们在这里提供一组实验的结果。</p><p>我们测量缓存中的命中率，客户端看到的平均延迟，Squid代理消耗的CPU时间（根据用户CPU时间和系统CPU时间以及网络流量）。我们使用netstat收集发送和接收的UDP数据报的数量，发送和接收的TCP数据包以及以太网接口处理的IP数据包的总数。第三个采集的信息大约是前两个采集信息的数字之和。 ICP查询和回复消息会产生UDP流量。 TCP通信包括代理与服务器之间以及代理与客户端之间的HTTP通信。 结果示于 <strong>表2</strong>。</p><p>结果表明，即使协作代理的数量低至四个，ICP也会产生可观的开销。UDP消息的数量增加了大概 <code>73</code> 到 <code>90</code>。由于UDP消息的增加，代理看到的总网络流量增加了 <code>8％</code> 至 <code>13％</code>。协议处理将用户CPU时间增加 <code>20％</code> 至 <code>24％</code> ，而UDP消息处理将系统CPU时间增加 <code>7％</code> 至 <code>10％</code> 。反映给客户端，HTTP请求的平均延迟增加了 <code>8％</code> 至 <code>11％</code> 。尽管实验是在高速局域网上进行的，但仍会发生性能下降。</p><p>结果凸显了Web缓存管理员面临的困境。 缓存共享有明显的好处，但是ICP的开销很高。此外，大多数时候，由于没有缓存文档，因此浪费了查询消息的处理时间。从本质上讲，在处理ICP上花费的精力与其他代理所经历的缓存未命中总数成正比，而不是与实际的远程缓存命中数成正比。</p><p>为了解决该问题，我们提出了一种新的可伸缩缓存共享协议：摘要缓存。</p><h2 id="摘要缓存"><a href="#摘要缓存" class="headerlink" title="摘要缓存"></a>摘要缓存</h2><p>在摘要缓存方案中，每个代理在每个其他代理中存储其缓存文档目录的摘要。当用户请求未在本地缓存中丢失时，本地代理会检查存储的摘要，以查看请求的文档是否可能存储在其他代理中。如果出现这种情况，则代理会将请求发送到相关代理以获取文档。 否则，代理将请求直接发送到Web服务器。</p><p>该方案可扩展性的关键是摘要不必是最新的或准确的。每次更改缓存目录时都不必更新摘要。 相反，更新可以按固定的时间间隔进行，也可以在摘要中未反映一定百分比的缓存文档时进行。摘要只需要包含在内（即，描述存储在缓存中的文档的超集），以避免影响总缓存命中率。 也就是说，可以容忍两种错误：</p><ul><li><p><strong>假的未命中</strong>：所请求的文档被缓存在其他代理服务器上，但是其摘要未反映该事实。 在这种情况下，不利用远程高速缓存命中，并且降低了高速缓存集合中的总命中率。</p></li><li><p><strong>假的命中</strong>：所请求的文档未在其他代理处缓存，但其摘要表明已缓存。 代理将向另一个代理发送查询消息，仅通知该文件未缓存在该代理中。 在这种情况下，浪费了查询消息。</p></li></ul><p>该错误会影响总缓存命中率或代理间流量，但不会影响缓存方案的正确性。例如，错误的命中不会导致送达错误的文档。 通常，我们会努力降低误报率，因为误报会增加Internet的流量，而缓存共享的目标是减少到Internet的流量。</p><p>摘要缓存和ICP中都会发生第三种错误，即远程过时命中。远程过时命中是指文档在另一个代理处缓存，但是缓存的副本是过时的。远程过时命中并不一定是浪费精力，因为可以使用增量压缩来传输新文档[39]。 但是，它确实有助于代理间的通信。</p><p>有两个因素限制了摘要缓存的可伸缩性：网络开销（代理间通信）和存储摘要所需的内存（出于性能原因，摘要应存储在DRAM中，而不是磁盘上）。网络开销取决于摘要更新的频率以及错误匹配和远程匹配的数量。内存需求取决于各个摘要的大小和协作代理的数量。由于内存随代理的数量线性增长，因此保持单个摘要较小很重要。下面，我们首先介绍更新频率，然后讨论各种摘要表示。</p><h3 id="更新延迟的影响"><a href="#更新延迟的影响" class="headerlink" title="更新延迟的影响"></a>更新延迟的影响</h3><p><img src="/assets/images/summary-cache-figure-2.png" alt="图2：摘要更新延迟对总缓存命中率的影响（ 缓存大小是&quot;无限&#39;&#39;缓存大小的10％）" loading="lazy" loading='lazy'></p><p>我们调查延迟摘要的更新，直到 “新”（即未反映在摘要中）缓存文档的百分比达到阈值为止。选择阈值标准是因为错误遗漏的数量（以及总命中率的下降）往往与未反映在摘要中的文档数量成正比。另一种方法是按固定的时间间隔更新摘要。可以通过将间隔转换为阈值来得出这种方法下的误漏率。也就是说，根据请求率和典型的高速缓存未命中率，可以计算出每个时间间隔内有多少新文档进入高速缓存及其在高速缓存文档中的百分比。</p><p>使用跟踪，我们可以模拟阈值分别为已缓存文档的 <code>0.1％</code>，<code>1％</code>，<code>2％</code>，<code>5％</code> 和 <code>10％</code>时总缓存命中率。目前，我们忽略摘要表示的问题，并假设摘要是缓存目录（即文档URL列表）的副本。结果如 <strong>图2</strong> 所示。图中第一行是未引入更新延迟时的命中率。第二行显示了命中率随着更新延迟的增加。这两行之间的差异是误漏率。底部的两条曲线显示了远程过时命中的比率和错误命中的比率（延迟的确引入了一些错误命中，因为从缓存中删除的文档可能仍存在于摘要中）。</p><p>结果表明，除了NLANR的跟踪数据外，总缓存命中率的下降几乎随更新阈值线性增长。在 <code>1％</code> 的阈值下，命中率相对降低为 <code>0.2％（UCB）</code>，<code>0.1％（UPisa）</code>，<code>0.3％（Questnet）</code> 和 <code>1.7％（DEC）</code>。 远程陈旧命中率几乎不受更新延迟的影响。虚假命中率非常小，因为摘要是缓存目录的精确副本，尽管它确实随阈值线性增加。</p><p>对于 <strong>NLANR跟踪</strong>，似乎某些客户端正在同时将两个完全相同的文档请求发送到代理”bo”和NLANR集合中的另一个代理。如果仅在NLANR中模拟其他三个代理，则结果与其他跟踪的结果类似。在包含”bo’’的情况下，我们还模拟了 <code>2个用户请求</code> 和 <code>10个用户请求</code> 的延迟，命中率分别从 <code>30.7％</code> 降至 <code>26.1％</code> 和 <code>20.2％</code> 。在 <code>0.1％</code> 的阈值处的命中率大约为 <code>18.4％</code>，大约等于200个用户请求。因此，我们认为命中率的急剧下降是由于NLANR迹线中的异常引起的。不幸的是，我们无法确定有问题的客户端，因为跨NLANR跟踪的客户端ID不一致[40]。</p><p>结果表明，实际上，摘要更新延迟阈值为 <code>1％</code> 到 <code>10％</code> 时会导致高速缓存命中率的可容忍度降低。对于这五个跟踪，阈值在两次更新之间转换成大约 <code>300</code> 到 <code>3000</code> 个用户请求，平均而言，大约 <strong>每5分钟到一个小时更新一次</strong>。因此，这些更新的带宽消耗可能非常低。</p><h3 id="摘要表示"><a href="#摘要表示" class="headerlink" title="摘要表示"></a>摘要表示</h3><p>影响可伸缩性的第二个问题是摘要的大小。摘要信息需要存储在主内存中，不仅因为内存查找要快得多，而且因为磁盘臂通常是代理缓存中的瓶颈[36]。尽管DRAM价格继续下降，但由于内存需求随代理数量线性增长，因此我们仍需要仔细设计。摘要信息还会使DRAM脱离热文档的内存高速缓存，从而影响代理性能。因此，重要的是要使摘要变小。幸运的是，摘要只需包含所有内容（即，描述存储在缓存中的文档的超集），即可避免影响缓存命中率。</p><p>我们首先研究两个简单的摘要表示形式：确切目录和服务器名称。在精确目录方法中，摘要基本上是缓存目录，每个URL均由其<strong>16字节MD5签名</strong>表示[38,22]。在服务器名称方法中，摘要是缓存中URL的服务器名称部分的列表。平均来看，不同URL与不同服务器名称的比率约为 <strong>10：1</strong>（从我们的跟踪记录中可以看到），因此服务器名称方法可以将内存减少10倍。</p><p>我们使用跟踪来模拟这些方法，发现它们都不令人满意。结果在图7中，以及在另一个摘要表示中的结果（在5.2节中详细讨论了图7）。精确目录方法消耗太多内存。实际上，代理通常具有<strong>8GB</strong>至<strong>20GB</strong>的缓存空间。如果我们假设<strong>16</strong>个代理服务器每个<strong>8GB</strong>，平均文件大小为<strong>8KB</strong>，则精确目录摘要将消耗**（16 -1）* 16 <em>（8GB &#x2F; 8KB）&#x3D;每个代理240MB主内存</em>*。服务器名称方法虽然消耗较少的内存，但会产生过多的错误命中，从而大大增加了网络消息。</p><p>理想的摘要表示形式的要求是小尺寸和低假命中率。 经过几次其他尝试，我们找到了一种称为布隆过滤器的古老技术的解决方案。</p><h3 id="布隆过滤器-数学"><a href="#布隆过滤器-数学" class="headerlink" title="布隆过滤器-数学"></a>布隆过滤器-数学</h3><p>布隆过滤器是一种可以用来在 <code>$n$</code> 个元素（也被叫做Keys）的集合 <code>$A = \&#123;&#123;a_1,a_2,...,a_n&#125;\&#125;$</code> 中查询成员的方法。</p><p>它由<code>Burton Bloom</code>在1970年发明，并被<code>Marais</code>和<code>Bharat</code>提出在Web的上下文中使用，作为一种机制用来识别哪些页面已经被存储在 CommonKnowledge 服务器中。</p><p><img src="/assets/images/summary-cache-figure-3.png" alt="图3：具有4个哈希函数的Bloom过滤器" loading="lazy" loading='lazy'></p><p>这个想法（如图3所示）就是分配一个拥有 $m$ 个比特位的向量 $v$，最初全部设置为0，然后选择 $k$ 个独立的哈希函数，$h_1,h_2,…,h_k$，每一个的范围都是 ${1,…,m}$，对于每个元素 $a \in A$ ，$v$ 中处于位置 $h_1(a),h_2(a),….,h_k(a)$ 的位置都设置为 $1$ （一个特定的位置可能会被多次设置为 $1$）。当查询 $b$ 的时候，我们会检查 $h_1(b),h_2(b),…,h_k(b)$ 位置上的比特值。如果他们中的任何一个值为 $0$ ， $b$ 就肯定不在集合 $A$ 中。</p><p>尽管存在一定的可能性我们会判断错误，$b$ 本身不在集合中，但我们推测 $b$ 在集合中，这就被称为误报。 这种情况在历史上也叫做 “假阴性”。我们应该通过参数 $k$ 和 $m$ ，以使误报率（因此导致错误命中的概率）处在可接受的范围之内 。</p><p>布隆过滤器的显著特征是：在 $m$ 与误报的概率之间存在明显的折衷。通过观察发现在将 $n$ 个key插入到大小为 $m$ 的表中之后，某一个比特位仍然为 $0$ 的概率正好是：$(1-\displaystyle \frac{1}{m})^{kn}$</p><p>因此，在这种情况下误报的概率为：$(1-(1-\displaystyle \frac{1}{m})^{kn})^k \approx (1 - e^{-\frac{kn}{m}})^k$</p><p>在右面的值最小的情况下，$k$ 的值为：$k&#x3D;ln2 * \displaystyle \frac{m}{n}$ ，在这种情况下：$(\displaystyle \frac{1}{2})^k &#x3D; (0.6185)^\frac{m}{n}$</p><p>由于 $k$ 必须是整数，并且实际上我们可以选择一个小于最佳值的值以减少计算开销。 一些示例值是：</p><p><strong>表3</strong> 到 <strong>表5</strong> 列出了 $\displaystyle \frac{m}{n}$ 和 $k$ 的常见组合的误报率情况。</p><p><strong>表3：</strong> 在各种 m&#x2F;n 和 $k$ 组合下的误判率。</p><table><thead><tr><th align="center">m&#x2F;n</th><th align="center">k</th><th align="center">k&#x3D;1</th><th align="center">k&#x3D;2</th><th align="center">k&#x3D;3</th><th align="center">k&#x3D;4</th><th align="center">k&#x3D;5</th><th align="center">k&#x3D;6</th><th align="center">k&#x3D;7</th><th align="center">k&#x3D;8</th></tr></thead><tbody><tr><td align="center">2</td><td align="center">1.39</td><td align="center">0.393</td><td align="center">0.400</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">3</td><td align="center">2.08</td><td align="center">0.283</td><td align="center">0.237</td><td align="center">0.253</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">4</td><td align="center">2.77</td><td align="center">0.221</td><td align="center">0.155</td><td align="center">0.147</td><td align="center">0.160</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">5</td><td align="center">3.46</td><td align="center">0.181</td><td align="center">0.109</td><td align="center">0.092</td><td align="center">0.092</td><td align="center">0.101</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">6</td><td align="center">4.16</td><td align="center">0.154</td><td align="center">0.0804</td><td align="center">0.0609</td><td align="center">0.0561</td><td align="center">0.0578</td><td align="center">0.0638</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">7</td><td align="center">4.85</td><td align="center">0.133</td><td align="center">0.0618</td><td align="center">0.0423</td><td align="center">0.0359</td><td align="center">0.0347</td><td align="center">0.0364</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">8</td><td align="center">5.55</td><td align="center">0.118</td><td align="center">0.0489</td><td align="center">0.0306</td><td align="center">0.024</td><td align="center">0.0217</td><td align="center">0.0216</td><td align="center">0.0229</td><td align="center"></td></tr><tr><td align="center">9</td><td align="center">6.24</td><td align="center">0.105</td><td align="center">0.0397</td><td align="center">0.0228</td><td align="center">0.0166</td><td align="center">0.0141</td><td align="center">0.0133</td><td align="center">0.0135</td><td align="center">0.0145</td></tr><tr><td align="center">10</td><td align="center">6.93</td><td align="center">0.0952</td><td align="center">0.0329</td><td align="center">0.0174</td><td align="center">0.0118</td><td align="center">0.00943</td><td align="center">0.00844</td><td align="center">0.00819</td><td align="center">0.00846</td></tr><tr><td align="center">11</td><td align="center">7.62</td><td align="center">0.0869</td><td align="center">0.0276</td><td align="center">0.0136</td><td align="center">0.00864</td><td align="center">0.0065</td><td align="center">0.00552</td><td align="center">0.00513</td><td align="center">0.00509</td></tr><tr><td align="center">12</td><td align="center">8.32</td><td align="center">0.08</td><td align="center">0.0236</td><td align="center">0.0108</td><td align="center">0.00646</td><td align="center">0.00459</td><td align="center">0.00371</td><td align="center">0.00329</td><td align="center">0.00314</td></tr><tr><td align="center">13</td><td align="center">9.01</td><td align="center">0.074</td><td align="center">0.0203</td><td align="center">0.00875</td><td align="center">0.00492</td><td align="center">0.00332</td><td align="center">0.00255</td><td align="center">0.00217</td><td align="center">0.00199</td></tr><tr><td align="center">14</td><td align="center">9.7</td><td align="center">0.0689</td><td align="center">0.0177</td><td align="center">0.00718</td><td align="center">0.00381</td><td align="center">0.00244</td><td align="center">0.00179</td><td align="center">0.00146</td><td align="center">0.00129</td></tr><tr><td align="center">15</td><td align="center">10.4</td><td align="center">0.0645</td><td align="center">0.0156</td><td align="center">0.00596</td><td align="center">0.003</td><td align="center">0.00183</td><td align="center">0.00128</td><td align="center">0.001</td><td align="center">0.000852</td></tr><tr><td align="center">16</td><td align="center">11.1</td><td align="center">0.0606</td><td align="center">0.0138</td><td align="center">0.005</td><td align="center">0.00239</td><td align="center">0.00139</td><td align="center">0.000935</td><td align="center">0.000702</td><td align="center">0.000574</td></tr><tr><td align="center">17</td><td align="center">11.8</td><td align="center">0.0571</td><td align="center">0.0123</td><td align="center">0.00423</td><td align="center">0.00193</td><td align="center">0.00107</td><td align="center">0.000692</td><td align="center">0.000499</td><td align="center">0.000394</td></tr><tr><td align="center">18</td><td align="center">12.5</td><td align="center">0.054</td><td align="center">0.0111</td><td align="center">0.00362</td><td align="center">0.00158</td><td align="center">0.000839</td><td align="center">0.000519</td><td align="center">0.00036</td><td align="center">0.000275</td></tr><tr><td align="center">19</td><td align="center">13.2</td><td align="center">0.0513</td><td align="center">0.00998</td><td align="center">0.00312</td><td align="center">0.0013</td><td align="center">0.000663</td><td align="center">0.000394</td><td align="center">0.000264</td><td align="center">0.000194</td></tr><tr><td align="center">20</td><td align="center">13.9</td><td align="center">0.0488</td><td align="center">0.00906</td><td align="center">0.0027</td><td align="center">0.00108</td><td align="center">0.00053</td><td align="center">0.000303</td><td align="center">0.000196</td><td align="center">0.00014</td></tr><tr><td align="center">21</td><td align="center">14.6</td><td align="center">0.0465</td><td align="center">0.00825</td><td align="center">0.00236</td><td align="center">0.000905</td><td align="center">0.000427</td><td align="center">0.000236</td><td align="center">0.000147</td><td align="center">0.000101</td></tr><tr><td align="center">22</td><td align="center">15.2</td><td align="center">0.0444</td><td align="center">0.00755</td><td align="center">0.00207</td><td align="center">0.000764</td><td align="center">0.000347</td><td align="center">0.000185</td><td align="center">0.000112</td><td align="center">7.46e-05</td></tr><tr><td align="center">23</td><td align="center">15.9</td><td align="center">0.0425</td><td align="center">0.00694</td><td align="center">0.00183</td><td align="center">0.000649</td><td align="center">0.000285</td><td align="center">0.000147</td><td align="center">8.56e-05</td><td align="center">5.55e-05</td></tr><tr><td align="center">24</td><td align="center">16.6</td><td align="center">0.0408</td><td align="center">0.00639</td><td align="center">0.00162</td><td align="center">0.000555</td><td align="center">0.000235</td><td align="center">0.000117</td><td align="center">6.63e-05</td><td align="center">4.17e-05</td></tr><tr><td align="center">25</td><td align="center">17.3</td><td align="center">0.0392</td><td align="center">0.00591</td><td align="center">0.00145</td><td align="center">0.000478</td><td align="center">0.000196</td><td align="center">9.44e-05</td><td align="center">5.18e-05</td><td align="center">3.16e-05</td></tr><tr><td align="center">26</td><td align="center">18</td><td align="center">0.0377</td><td align="center">0.00548</td><td align="center">0.00129</td><td align="center">0.000413</td><td align="center">0.000164</td><td align="center">7.66e-05</td><td align="center">4.08e-05</td><td align="center">2.42e-05</td></tr><tr><td align="center">27</td><td align="center">18.7</td><td align="center">0.0364</td><td align="center">0.0051</td><td align="center">0.00116</td><td align="center">0.000359</td><td align="center">0.000138</td><td align="center">6.26e-05</td><td align="center">3.24e-05</td><td align="center">1.87e-05</td></tr><tr><td align="center">28</td><td align="center">19.4</td><td align="center">0.0351</td><td align="center">0.00475</td><td align="center">0.00105</td><td align="center">0.000314</td><td align="center">0.000117</td><td align="center">5.15e-05</td><td align="center">2.59e-05</td><td align="center">1.46e-05</td></tr><tr><td align="center">29</td><td align="center">20.1</td><td align="center">0.0339</td><td align="center">0.00444</td><td align="center">0.000949</td><td align="center">0.000276</td><td align="center">9.96e-05</td><td align="center">4.26e-05</td><td align="center">2.09e-05</td><td align="center">1.14e-05</td></tr><tr><td align="center">30</td><td align="center">20.8</td><td align="center">0.0328</td><td align="center">0.00416</td><td align="center">0.000862</td><td align="center">0.000243</td><td align="center">8.53e-05</td><td align="center">3.55e-05</td><td align="center">1.69e-05</td><td align="center">9.01e-06</td></tr><tr><td align="center">31</td><td align="center">21.5</td><td align="center">0.0317</td><td align="center">0.0039</td><td align="center">0.000785</td><td align="center">0.000215</td><td align="center">7.33e-05</td><td align="center">2.97e-05</td><td align="center">1.38e-05</td><td align="center">7.16e-06</td></tr><tr><td align="center">32</td><td align="center">22.2</td><td align="center">0.0308</td><td align="center">0.00367</td><td align="center">0.000717</td><td align="center">0.000191</td><td align="center">6.33e-05</td><td align="center">2.5e-05</td><td align="center">1.13e-05</td><td align="center">5.73e-06</td></tr></tbody></table><p><strong>表4：</strong> 在各种 $m&#x2F;n$ 和 $k$ 组合下的误判率。</p><table><thead><tr><th align="center">m&#x2F;n</th><th align="center">k</th><th align="center">k&#x3D;9</th><th align="center">k&#x3D;10</th><th align="center">k&#x3D;11</th><th align="center">k&#x3D;12</th><th align="center">k&#x3D;13</th><th align="center">k&#x3D;14</th><th align="center">k&#x3D;15</th><th align="center">k&#x3D;16</th></tr></thead><tbody><tr><td align="center">11</td><td align="center">7.62</td><td align="center">0.00531</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">12</td><td align="center">8.32</td><td align="center">0.00317</td><td align="center">0.00334</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">13</td><td align="center">9.01</td><td align="center">0.00194</td><td align="center">0.00198</td><td align="center">0.0021</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">14</td><td align="center">9.7</td><td align="center">0.00121</td><td align="center">0.0012</td><td align="center">0.00124</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">15</td><td align="center">10.4</td><td align="center">0.000775</td><td align="center">0.000744</td><td align="center">0.000747</td><td align="center">0.000778</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">16</td><td align="center">11.1</td><td align="center">0.000505</td><td align="center">0.00047</td><td align="center">0.000459</td><td align="center">0.000466</td><td align="center">0.000488</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">17</td><td align="center">11.8</td><td align="center">0.000335</td><td align="center">0.000302</td><td align="center">0.000287</td><td align="center">0.000284</td><td align="center">0.000291</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">18</td><td align="center">12.5</td><td align="center">0.000226</td><td align="center">0.000198</td><td align="center">0.000183</td><td align="center">0.000176</td><td align="center">0.000176</td><td align="center">0.000182</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">19</td><td align="center">13.2</td><td align="center">0.000155</td><td align="center">0.000132</td><td align="center">0.000118</td><td align="center">0.000111</td><td align="center">0.000109</td><td align="center">0.00011</td><td align="center">0.000114</td><td align="center"></td></tr><tr><td align="center">20</td><td align="center">13.9</td><td align="center">0.000108</td><td align="center">8.89e-05</td><td align="center">7.77e-05</td><td align="center">7.12e-05</td><td align="center">6.79e-05</td><td align="center">6.71e-05</td><td align="center">6.84e-05</td><td align="center"></td></tr><tr><td align="center">21</td><td align="center">14.6</td><td align="center">7.59e-05</td><td align="center">6.09e-05</td><td align="center">5.18e-05</td><td align="center">4.63e-05</td><td align="center">4.31e-05</td><td align="center">4.17e-05</td><td align="center">4.16e-05</td><td align="center">4.27e-05</td></tr><tr><td align="center">22</td><td align="center">15.2</td><td align="center">5.42e-05</td><td align="center">4.23e-05</td><td align="center">3.5e-05</td><td align="center">3.05e-05</td><td align="center">2.78e-05</td><td align="center">2.63e-05</td><td align="center">2.57e-05</td><td align="center">2.59e-05</td></tr><tr><td align="center">23</td><td align="center">15.9</td><td align="center">3.92e-05</td><td align="center">2.97e-05</td><td align="center">2.4e-05</td><td align="center">2.04e-05</td><td align="center">1.81e-05</td><td align="center">1.68e-05</td><td align="center">1.61e-05</td><td align="center">1.59e-05</td></tr><tr><td align="center">24</td><td align="center">16.6</td><td align="center">2.86e-05</td><td align="center">2.11e-05</td><td align="center">1.66e-05</td><td align="center">1.38e-05</td><td align="center">1.2e-05</td><td align="center">1.08e-05</td><td align="center">1.02e-05</td><td align="center">9.87e-06</td></tr><tr><td align="center">25</td><td align="center">17.3</td><td align="center">2.11e-05</td><td align="center">1.52e-05</td><td align="center">1.16e-05</td><td align="center">9.42e-06</td><td align="center">8.01e-06</td><td align="center">7.1e-06</td><td align="center">6.54e-06</td><td align="center">6.22e-06</td></tr><tr><td align="center">26</td><td align="center">18</td><td align="center">1.57e-05</td><td align="center">1.1e-05</td><td align="center">8.23e-06</td><td align="center">6.52e-06</td><td align="center">5.42e-06</td><td align="center">4.7e-06</td><td align="center">4.24e-06</td><td align="center">3.96e-06</td></tr><tr><td align="center">27</td><td align="center">18.7</td><td align="center">1.18e-05</td><td align="center">8.07e-06</td><td align="center">5.89e-06</td><td align="center">4.56e-06</td><td align="center">3.7e-06</td><td align="center">3.15e-06</td><td align="center">2.79e-06</td><td align="center">2.55e-06</td></tr><tr><td align="center">28</td><td align="center">19.4</td><td align="center">8.96e-06</td><td align="center">5.97e-06</td><td align="center">4.25e-06</td><td align="center">3.22e-06</td><td align="center">2.56e-06</td><td align="center">2.13e-06</td><td align="center">1.85e-06</td><td align="center">1.66e-06</td></tr><tr><td align="center">29</td><td align="center">20.1</td><td align="center">6.85e-06</td><td align="center">4.45e-06</td><td align="center">3.1e-06</td><td align="center">2.29e-06</td><td align="center">1.79e-06</td><td align="center">1.46e-06</td><td align="center">1.24e-06</td><td align="center">1.09e-06</td></tr><tr><td align="center">30</td><td align="center">20.8</td><td align="center">5.28e-06</td><td align="center">3.35e-06</td><td align="center">2.28e-06</td><td align="center">1.65e-06</td><td align="center">1.26e-06</td><td align="center">1.01e-06</td><td align="center">8.39e-06</td><td align="center">7.26e-06</td></tr><tr><td align="center">31</td><td align="center">21.5</td><td align="center">4.1e-06</td><td align="center">2.54e-06</td><td align="center">1.69e-06</td><td align="center">1.2e-06</td><td align="center">8.93e-07</td><td align="center">7e-07</td><td align="center">5.73e-07</td><td align="center">4.87e-07</td></tr><tr><td align="center">32</td><td align="center">22.2</td><td align="center">3.2e-06</td><td align="center">1.94e-06</td><td align="center">1.26e-06</td><td align="center">8.74e-07</td><td align="center">6.4e-07</td><td align="center">4.92e-07</td><td align="center">3.95e-07</td><td align="center">3.3e-07</td></tr></tbody></table><p><strong>表5：</strong> 在各种 m&#x2F;n 和 $k$ 组合下的误判率。</p><table><thead><tr><th align="center">m&#x2F;n</th><th align="center">k</th><th align="center">k&#x3D;17</th><th align="center">k&#x3D;18</th><th align="center">k&#x3D;19</th><th align="center">k&#x3D;20</th><th align="center">k&#x3D;21</th><th align="center">k&#x3D;22</th><th align="center">k&#x3D;23</th><th align="center">k&#x3D;24</th></tr></thead><tbody><tr><td align="center">22</td><td align="center">15.2</td><td align="center">2.67e-05</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">23</td><td align="center">15.9</td><td align="center">1.61e-05</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">24</td><td align="center">16.6</td><td align="center">9.84e-06</td><td align="center">1e-05</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">25</td><td align="center">17.3</td><td align="center">6.08e-06</td><td align="center">6.11e-06</td><td align="center">6.27e-06</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">26</td><td align="center">18</td><td align="center">3.81e-06</td><td align="center">3.76e-06</td><td align="center">3.8e-06</td><td align="center">3.92e-06</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">27</td><td align="center">18.7</td><td align="center">2.41e-06</td><td align="center">2.34e-06</td><td align="center">2.33e-06</td><td align="center">2.37e-06</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">28</td><td align="center">19.4</td><td align="center">1.54e-06</td><td align="center">1.47e-06</td><td align="center">1.44e-06</td><td align="center">1.44e-06</td><td align="center">1.48e-06</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">29</td><td align="center">20.1</td><td align="center">9.96e-07</td><td align="center">9.35e-07</td><td align="center">9.01e-07</td><td align="center">8.89e-07</td><td align="center">8.96e-07</td><td align="center">9.21e-07</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">30</td><td align="center">20.8</td><td align="center">6.5e-07</td><td align="center">6e-07</td><td align="center">5.69e-07</td><td align="center">5.54e-07</td><td align="center">5.5e-07</td><td align="center">5.58e-07</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">31</td><td align="center">21.5</td><td align="center">4.29e-07</td><td align="center">3.89e-07</td><td align="center">3.63e-07</td><td align="center">3.48e-07</td><td align="center">3.41e-07</td><td align="center">3.41e-07</td><td align="center">3.48e-07</td><td align="center"></td></tr><tr><td align="center">32</td><td align="center">22.2</td><td align="center">2.85e-07</td><td align="center">2.55e-07</td><td align="center">2.34e-07</td><td align="center">2.21e-07</td><td align="center">2.13e-07</td><td align="center">2.1e-07</td><td align="center">2.12e-07</td><td align="center">2.17e-07</td></tr></tbody></table><p><strong>图4</strong> 中的曲线图显示了误判率与分配给每个条目的位数的关系，即比率为： $\alpha &#x3D; n &#x2F; m $ 。上面的曲线是针对 4个哈希函数的情况。下面的曲线是散列函数的最佳数量。标度是对数的，因此观察到的直线对应于指数下降。显然，由于布隆过滤器中每个Key只有很少的存储空间，因此存在一些误报的轻微风险。例如，对于一个比Key数量大10倍的位数组，在使用4个散列函数的情况下，误报率大概为1.2％，使用5个散列函数的情况下，误报的概率为0.9％。 我们也可以通过分配更多的内存来轻松降低误报率。</p><p><img src="/assets/images/summary-cache-figure-4.png" alt="图4：假阳性的概率（对数刻度）。顶部曲线用于4个哈希函数。 底部曲线是散列函数的最佳（整数）个数" loading="lazy" loading='lazy'></p><p>由于在我们的上下文中，每个代理都维护一个本地布隆过滤器来存储自己的缓存文档，因此集合 $A$ 必须支持修改。这是通过为位数组中的每个位置 $\tau$ 维持该位被设置为 $1$ 的次数 $c(\tau)$（即散列值为 $\tau$ 的元素的数量）的计数来完成的。所有的计数最初都为0。当我们插入或者删除键 $a$ （在我们示例的文档中为URL）的时候，计数 $c(h_1(a),h_2(a),…h_k(a))$ 相应的增加或者减少。当计数从0变为1的时候，相应的位被打开，当计数从1变为0的时候，相应的位将关闭，因此，本地的布隆过滤器始终正确反应当前目录。</p><p>由于我们还需要为计数分配内存，因此重要的是要知道它们可以变为多大。 将具有 $k$ 个散列函数的 $n$ 个密钥插入大小为 $m$ 的位数组后，估计预期最大计数为（参见[22，第72页]）</p><p>$T^{-1}{(m)}(1 + \displaystyle \frac{ln(kn&#x2F;m)}{ln T^{-1}(m)} + O(\frac{1}{ln^2 T^{-1}(m)}))$</p><p>并且任何计数大于或等于 $i$ 的概率为：</p><p>$Pr(max(c) \geq i) \leq m({^{nk}_i}){ \displaystyle \frac{1}{m^i}} \leq m({\displaystyle \frac{enk}{im}})^i$</p><p>如前所述，$k$（超过实数）的最优值为 $ln(2m&#x2F;n)$，因此假设哈希函数的数量小于 $ln(2m&#x2F;n)$ ，我们可以进一步限制：</p><p>$Pr(max(c) \geq i) \leq m({\displaystyle \frac{eln2}{i}})^i$</p><p>因此，取 $i &#x3D; 16$，我们得到：</p><p>$Pr(max(c) \geq 16) \leq 1.37 * 10^{-15} * m$</p><p>换句话说，如果我们允许使用4个比特位用于计数，则在表的初始插入过程中，实际 $m$ 值的溢出概率很小。</p><p>在实践中，我们必须考虑到哈希函数并不是真正随机的，并且我们一直在进行插入和删除操作。 不过，<strong>似乎每个计数占用4个比特位就足够了</strong>。 此外，如果计数超过 <code>15</code>，我们可以简单地将其保持在<code>15</code>；否则，它将保持不变。 在多次删除之后，这可能会导致布隆过滤器允许出现假负数（当计数不应为<code>0</code>时该计数变为<code>0</code>）的情况，但是发生此类事件的可能性非常低，以至于同时将重新启动代理服务器，并重建整个结构。</p><h3 id="布隆过滤器作为摘要"><a href="#布隆过滤器作为摘要" class="headerlink" title="布隆过滤器作为摘要"></a>布隆过滤器作为摘要</h3><p><img src="/assets/images/summary-cache-figure-5.png" alt="图5：不同摘要表示形式下的总点击率" loading="lazy" loading='lazy'></p><p><img src="/assets/images/summary-cache-figure-6.png" alt="图6：不同摘要表示下的错误命中率。 请注意，y轴为对数刻度" loading="lazy" loading='lazy'></p><p><img src="/assets/images/summary-cache-figure-7.png" alt="图7： 不同摘要形式下每个用户请求的网络消息数。 请注意，y轴为对数刻度" loading="lazy" loading='lazy'></p><p><img src="/assets/images/summary-cache-figure-8.png" alt="图8： 不同摘要格式下每个用户请求的网络消息字节数" loading="lazy" loading='lazy'></p><p><img src="/assets/images/summary-cache-figure-9.png" alt="图9： 不同摘要表示形式的内存需求" loading="lazy" loading='lazy'></p><p>布隆过滤器提供了一种创建摘要信息的简单机制。代理从缓存文档的URL列表构建Bloom过滤器，然后将位数组以及哈希函数的规范发送给其他代理。 更新摘要信息时，代理可以指定翻转位数组中的哪些位，或发送整个数组，以较小者为准。</p><p>每个代理都维护着布隆过滤器的本地副本，并在将文档添加到缓存或从缓存中替换文档时对其进行更新。正如以上所说明的，为了更新本地的布隆过滤器，代理服务器需要维护一个计数器数组，每个计数器记住相应位设置为1的次数。并将文档添加到高速缓存中时，增加相应位的计数器； 当从缓存中删除它时，减少相应位的计数器。 当计数器从0增加到1或从1下降到0时，相应的位应该分别被设置为1或0，并在列表中添加一条记录更新的记录。</p><p>布隆过滤器的优点是，它们可以在内存需求和误报率（会导致误判命中）之间进行权衡。 因此，代理可以在代理间的通信量略有增加的情况下，为摘要信息分配较少的内存。</p><p>我们针对基于布隆过滤器的摘要信息尝试了三种配置：<strong>位数组的大小为缓存中平均文档数的8倍，16倍和32倍（该比率也称为 “加载因子”）</strong>。 通过将高速缓存大小除以<strong>8K</strong>（平均文档大小）来计算平均文档数。 所有这三种配置都使用<strong>四个哈希函数</strong>。 哈希函数的数量并不是每种配置的最佳选择，但足以证明布隆过滤器的性能。 首先通过计算URL的MD5签名[38]（产生128位），然后<strong>将128位划分为4个32位字</strong>，最后将每个32位字的模数与表大小 $m$ 进行比较。MD5是一种加密消息摘要算法，可将任意长度的字符串散列为128位[38]。我们选择它是因为其众所周知的属性和相对较快的实现。</p><p>这些摘要信息表示，<strong>精确目录方法</strong>和<strong>服务器名称方法的性能</strong>如<strong>图5</strong>到<strong>图9</strong>所示。在<strong>图5</strong>中，我们显示了<strong>总的高速缓存命中率</strong>，在<strong>图6</strong>中，我们显示了<strong>错误命中率</strong>。 请注意，<strong>图6</strong>中的y轴为对数刻度。 <strong>基于布隆过滤器的摘要实际上具有与精确目录方法相同的缓存命中率，并且在位数组较小时具有较高的假命中率</strong>。 服务器名称具有较高的错误命中率。 它具有较高的缓存命中率，可能是因为它的许多错误命中有助于避免错误遗漏。</p><p><strong>图7</strong>显示了代理间网络消息的总数，包括<strong>摘要更新的数量和查询消息的数量（包括远程高速缓存命中，错误命中和远程陈旧命中）</strong>。注意，<strong>图7</strong>中的y轴为对数刻度。为了进行比较，我们还列出了每个跟踪中<code>ICP</code>引起的消息数。假定所有消息都是单播消息。该图通过每个跟踪中的HTTP请求数将消息数标准化。基于精确目录和布隆过滤器的摘要都可以很好地执行，并且服务器名和<code>ICP</code>会生成更多消息。对于布隆过滤器，如预期的那样，需要在位阵列大小和消息数之间进行权衡。但是，一旦错误命中率足够小，错误命中就不再是代理间消息的主要来源。而是，远程缓存命中和远程陈旧命中成为主导。因此，在<code>负载因数16</code>和<code>负载因数32</code>之间在网络消息方面的差异很小。与<code>ICP</code>相比，基于布隆过滤器的摘要将消息数量减少了<code>25</code>到<code>60</code>。</p><p><strong>图8</strong>显示了代理间网络消息的估计总大小（以字节为单位）。我们估计大小是因为更新消息往往大于查询消息。<code>ICP</code>和其他方法中查询消息的平均大小均假定为<code>标头20字节</code>和<code>平均URL为50字节</code>。 精确目录和服务器名称中摘要更新的大小假定为<code>标头20字节</code>，<code>每次更改16字节</code>。 在基于布隆过滤器的摘要中，摘要更新的大小估计为32字节的标头（请参见第6节）加上每个位翻转4字节。 结果表明，<strong>就消息字节而言，基于布隆过滤器的摘要比ICP改善了55％至64％</strong>。 换句话说，摘要缓存偶尔使用大消息突发，以避免连续发送小消息。 查看<strong>表2、6和7</strong>中的CPU开销和网络接口数据包（其中SC-ICP代表摘要缓存方法），我们可以看到这是一个很好的权衡。</p><p>最后，<strong>图9</strong>显示了摘要缓存方法的每个代理的内存，以缓存大小的百分比表示。 三种布隆过滤器配置所消耗的内存比精确目录少得多，但在所有其他方面的性能却与精确目录类似。 负载因数为8时，布隆过滤器摘要与服务器名称方法具有相似或更少的内存要求，并且错误命中率和网络消息更少。 由于<code>DEC</code>，<code>UCB</code>，<code>UPisa</code>，<code>NLANR</code>和<code>Questnet</code>迹线<code>分别具有16、8、8、4和12个代理</code>，因此该图表明内存需求随代理的数量线性增长。</p><p>考虑所有结果，我们看到布隆过滤器的摘要在低网络开销和低内存需求方面提供了最佳性能。 这种方法简单易行。 除MD5之外，其他更快的哈希方法也可用，例如哈希函数可以基于Rabin指纹方法（参见[42]，[7]）中的多项式算法或简单的哈希函数（例如[22]，p. [48]）可用于生成例如32位，并且可以通过对这32位（视为整数）进行随机线性变换来获得其他位。 缺点是这些较快的功能是有效可逆的（也就是说，可以轻松地构建散列到特定位置的URL），这一事实可能被恶意用户用于邪恶目的。</p><h3 id="推荐配置"><a href="#推荐配置" class="headerlink" title="推荐配置"></a>推荐配置</h3><p>结合以上结果，我们建议对摘要缓存方法进行以下配置。<strong>更新阈值应在1％到10％之间，以避免显着降低总缓存命中率</strong>。如果<strong>选择了基于时间的更新方法，则应选择时间间隔，以使新文档的百分比在1％到10％之间</strong>。代理可以广播更改（如果较小，则广播整个位数组），或者让其他代理从中获取更新。摘要应采用布隆过滤器的形式。<strong>8到16之间的负载系数可以很好地工作，但是代理可以根据其内存和网络流量问题降低或提高它</strong>。<strong>基于负载因子，应使用四个或更多哈希函数</strong>。此处和[16]中提供的数据可以用作决策参考。<strong>对于散列函数，我们建议从URL的128位MD5签名中获取不相交的位组</strong>。如果需要更多位，则可以计算与其自身连接的URL的MD5签名。实际上，与缓存文档所引起的用户和系统CPU开销相比，MD5的计算开销可以忽略不计（请参阅第[7]节）。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>尽管我们的模拟是针对<code>4</code>到<code>16</code>个代理完成的，但是我们可以轻松地推断出结果。例如，假设每个代理有<code>8GB的缓存</code>要合作。每个代理平均存储约<code>100万个网页</code>。表示<code>1M页面</code>所需的布隆过滤器内存在<code>负载因子16时为2MB</code>。每个代理大约需要<code>200MB</code>来表示所有摘要，另外需要<code>1MB</code>来表示自己的计数器。代理间消息包括更新消息，错误命中，远程缓存命中和远程陈旧命中。 <strong>1％的阈值对应于更新之间的10K请求，每个更新包含99条消息，每个请求的更新消息数小于0.01</strong>。<strong>带有10个哈希函数的16的负载因子的假命中率约为4.7％（每个摘要的误报率小于0.00047，但其中有100个摘要）</strong>。因此，不计算远程缓存命中和远程陈旧命中（在代理数量上相对稳定）引入的消息，协议引入的开销是每个请求100个代理的0.06条消息以下。在这些消息中，只有更新消息很大，大约数百KB。幸运的是，更新消息可以通过不可靠的多播方案进行传输。我们的仿真预测，在保持较低开销的同时，与ICP的理论命中率相比，该方案将总命中率降低了不到2％。</p><p>尽管没有迹线足够大以至于无法对100个代理进行有意义的仿真，但我们已经使用大量代理进行了仿真，结果验证了这些&#96;&#96;封底’’的计算结果。 因此，我们相信摘要缓存可以很好地扩展。</p><h2 id="摘要缓存增强型ICP的实现"><a href="#摘要缓存增强型ICP的实现" class="headerlink" title="摘要缓存增强型ICP的实现"></a>摘要缓存增强型ICP的实现</h2><p>基于仿真结果，我们提出以下摘要缓存增强的Internet缓存协议作为ICP的优化。 该协议已在<code>Squid 1.1.14</code>之上的原型中实现，并且该原型可公开获得[15]。 在<code>Squid 1.2b20</code>中也实现了我们的方法的一种称为<code>Cache Digest的变体</code>[44]。</p><h3 id="协议书"><a href="#协议书" class="headerlink" title="协议书"></a>协议书</h3><p>我们协议的设计面向较小的延迟阈值。 因此，它假定摘要是通过发送差异来更新的。 如果延迟阈值很大，则发送整个位阵列会更经济； <code>Squid 1.2b20</code> [44]中的<code>Cache Digest原型</code>采用了这种方法。</p><p>我们在ICP版本2[48]中添加了新的操作码，<code>ICP_OP_DIRUPDATE（= 20）</code>，代表目录更新消息。 在更新消息中，常规ICP头后面有一个附加头，该头包括：<code>16位的Function_Num</code>，<code>16位的Function_Bits</code>，<code>32位的BitArray_Size_InBits</code>和<code>32位的Number_of_Updates</code>。 标头完全指定用于探测过滤器的哈希函数。 有散列函数的Function_Num。 通过首先从URL的MD5签名[38,22]中取出位0到M-1，M到2M-1、2M到3M-1等来计算函数，其中M是Function_Bits，然后进行模块化 位由BitArray_Size_InBits决定。 如果128位不够用，则通过计算与其自身连接的URL的MD5签名来生成更多位。</p><p>标头后跟一个32位整数列表。 整数中的最高有效位指定该位应设置为<code>0</code>还是<code>1</code>，其余位指定需要更改的位的索引。 该设计是出于以下考虑：如果消息仅指定应翻转的位，则丢失先前的更新消息将具有级联效应。 该设计使消息可以通过不可靠的多播协议发送。 此外，每个更新消息都带有标头，该标头指定哈希函数，以便接收者可以验证信息。 该设计将哈希表的大小限制为小于20亿，这暂时足够大。</p><h3 id="原型实现"><a href="#原型实现" class="headerlink" title="原型实现"></a>原型实现</h3><p>我们修改了<code>Squid 1.1.4</code>软件以实现上述协议。 一个额外的位数组被添加到每个邻居的数据结构中。 当从邻居接收到第一摘要更新消息时，初始化该结构。 代理还分配了一个字节计数器数组来维护Bloom过滤器的本地副本，并分配一个整数数组来记住过滤器的更改。</p><p>由于<code>ICP</code>是基于<code>UDP</code>构建的，因此当前的原型通过UDP发送更新消息。 该设计的一种变体是通过TCP或多播发送消息。 由于这些消息的大小，最好通过<code>TCP</code>或多播发送它们。 此外，由于协作代理的收集是相对静态的，因此代理可以仅保持彼此之间的永久<code>TCP</code>连接以交换更新消息。 不幸的是，在<code>Squid</code>中实现<code>ICP</code>仅在<code>UDP</code>之上。 因此，原型有悖于5.5节中的建议，并且只要有足够的更改来填充IP数据包，就发送更新。 该实现还利用<code>Squid</code>的内置支持来检测邻居代理的故障和恢复，并在故障邻居恢复时重新初始化它。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>表6：实验2中UPisa迹线的ICP和摘要缓存的性能。括号中的数字表示三个实验之间测量值的差异。</p><table><thead><tr><th>Exp</th><th>Hit Ratio</th><th>Client Latency</th><th>User CPU</th><th>System CPU</th><th>UDP Traffic</th><th>TCP Traffic</th><th>Total Packets</th></tr></thead><tbody><tr><td>no ICP</td><td>16.94</td><td>6.22(0.4%)</td><td>81.72(0.1%)</td><td>115.63(0.1%)</td><td>4718(1%)</td><td>242K(0.1%)</td><td>259K(0.1%)</td></tr><tr><td>ICP</td><td>19.3</td><td>6.31(0.5%)</td><td>116.81(0.1%)</td><td>137.12(0.1%)</td><td>72761(0%)</td><td>245K(0.1%)</td><td>325K(0.2%)</td></tr><tr><td><strong>Overhead</strong></td><td></td><td>1.42%</td><td>43%</td><td>19%</td><td>1400%</td><td>1%</td><td>25%</td></tr><tr><td>SC-ICP</td><td>19.0</td><td>6.07 (0.1%)</td><td>91.53(0.4%)</td><td>121.75(0.5%)</td><td>5765(2%)</td><td>244K(0.1%)</td><td>262K(0.1%)</td></tr><tr><td><strong>Overhead</strong></td><td></td><td>-2.4%</td><td>12%</td><td>5%</td><td>22%</td><td>1%</td><td>1%</td></tr></tbody></table><p>表7：实验3中UPisa跟踪的ICP和摘要缓存的性能</p><table><thead><tr><th>Exp</th><th>Hit Ratio</th><th>Client Latency</th><th>User CPU</th><th>System CPU</th><th>UDP Traffic</th><th>TCP Traffic</th><th>Total Packets</th></tr></thead><tbody><tr><td>no ICP</td><td>9.94</td><td>7.11</td><td>81.75</td><td>119.7</td><td>1608</td><td>248K</td><td>265K</td></tr><tr><td>ICP</td><td>17.9</td><td>7.22</td><td>121.5</td><td>146.4</td><td>75226</td><td>257K</td><td>343K</td></tr><tr><td><strong>Overhead</strong></td><td></td><td>1.6%</td><td>49%</td><td>22%</td><td>4577%</td><td>3.7%</td><td>29%</td></tr><tr><td>SC-ICP</td><td>16.2</td><td>6.80</td><td>90.4</td><td>126.5</td><td>4144</td><td>254K</td><td>274K</td></tr><tr><td><strong>Overhead</strong></td><td></td><td>-4.3%</td><td>11%</td><td>5.7%</td><td>1.6</td><td>2.4%</td><td>3.2%</td></tr></tbody></table><p>我们对该原型进行了三个实验。 第一个实验重复第4节中的测试，结果包含在第4节<strong>表2</strong>中，标题为<code>SC-ICP</code>。改进的协议将UDP流量减少了50倍，并且具有网络流量， CPU时间和客户端等待时间与No-ICP相似。</p><p>我们的第二个实验和第三个实验重播了UPisa跟踪中的前24,000个请求。我们使用在4个工作站上运行的80个客户端进程的集合，并将同一工作站上的客户端进程连接到同一代理服务器。在第二个实验中，我们通过让每个客户端进程通过发出它们的Web请求来模拟一组真实的客户端来重播跟踪。在第三个实验中，我们通过让客户端进程从跟踪文件循环发出请求来重播跟踪，而不管每个请求来自哪个实际客户端。第二个实验保留了客户端及其请求之间的边界，并且客户端的请求都转到同一个代理。但是，它不会保留来自不同客户端的请求之间的顺序。第三个实验不保留请求和客户端之间的边界，但是保留请求之间的时序顺序。与第二个实验相比，第三个实验中的代理负载均衡性更高。</p><p>在这两个实验中，每个请求的URL都在跟踪文件中包含请求的大小，并且服务器以指定的字节数进行答复。 其余配置与第4节中的实验相似。与综合基准不同，该跟踪包含大量的远程命中。 实验2的结果列在表6中，实验3的结果列在<strong>表7</strong>中。</p><p>结果表明，增强的ICP协议可显着减少网络流量和CPU开销，而仅略微降低总命中率。 与No-ICP相比，增强的ICP协议可以稍微降低客户端等待时间，即使它将CPU时间增加了大约12％。 客户端延迟的减少归因于远程缓存命中。 单独的实验表明，大多数CPU时间增加是由于服务于远程命中，而MD5计算导致的CPU时间增加不到5％。 尽管实验没有如实地重播跟踪，但它们确实说明了摘要缓存在实践中的性能。</p><p>我们的结果表明，摘要缓存增强的ICP解决了ICP的开销问题，需要最小的更改，并且可以在广域网上共享可伸缩的Web缓存。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>Web缓存是一个活跃的研究领域。 关于Web客户端访问特性[10,3,14,33,23]，Web缓存算法[49,35,8]以及Web缓存一致性[28,31,34,13]已有许多研究。 我们的研究不涉及缓存算法或缓存一致性维护，但是在我们对Web缓存共享的好处进行调查的过程中，与某些客户端流量研究重叠。</p><p>最近，文献中提出了许多新的缓存共享方法。缓存阵列路由协议[46]在一组松散耦合的代理服务器之间划分URL空间，并让每个代理仅缓存URL哈希到其上的文档。该方法的优点在于，它消除了文档的重复副本。但是，尚不清楚该方法在代理分布在区域网络中的广域缓存共享方面的性能如何。 Relais项目[27]还建议使用本地目录在其他缓存中查找文档，并异步更新目录。这个想法类似于摘要缓存。但是，该项目似乎并未解决内存需求问题。从Relais的出版物中我们可以找到并阅读[4]，我们也不清楚该项目是否解决了目录更新频率的问题。由紧密耦合的群集工作站构建的代理也使用各种哈希和分区方法来利用群集中的内存和磁盘[20]，但是这些方法不适用于广域网。</p><p>我们的研究部分受到名为目录服务器[21]的现有提议的推动。 该方法使用中央服务器来跟踪所有代理的缓存目录，并且所有代理向服务器查询其他代理中的缓存命中。 该方法的缺点是中央服务器很容易成为瓶颈。 优点是，除了远程命中之外，同级代理之间几乎不需要通信。</p><p>关于Web缓存层次结构和缓存共享的研究也很多。 分层Web缓存最早是在Harvest项目[26,12]中提出的，该项目还引入了ICP协议。 当前，Squid代理服务器实现了ICP协议的第2版[48]，我们的摘要缓存增强型ICP以此版本为基础。 自适应Web缓存[50]提出了一种基于多播的自适应缓存基础结构，用于在Web中分发文档。 特别地，该方案试图沿着到服务器的路线将文档放置在正确的缓存中。 我们的研究未解决定位问题。 相反，我们注意到我们的研究是互补的，因为摘要缓存方法可以用作传达缓存内容的机制。</p><p>尽管我们没有模拟这种情况，但是可以在父代理和子代理之间使用摘要缓存增强的ICP。 分层Web缓存不仅包括相邻（同级）代理之间的协作，还包括父代和子代代理之间的协作。 同级代理与父级代理之间的区别在于，代理不能要求同级代理从服务器获取文档，而可以要求父级代理这样做。 尽管我们的模拟仅涉及同级代理之间的协作，但是摘要缓存方法可用于将有关父缓存内容的信息传播到子代理，并消除子代理对父代的ICP查询。 我们对Questnet跟踪的检查表明，子级到父级ICP查询可能是父级必须处理的消息的很大一部分（超过2&#x2F;3）。</p><p>在操作系统的上下文中，已经有很多关于协作文件缓存[11,2]和全局存储系统（GMS）[17]的研究。这些系统中的基本假设是高速局域网比磁盘快，并且工作站应使用彼此的空闲内存来缓存文件页面或虚拟内存页面，以避免流向磁盘。在这方面，问题与Web缓存共享完全不同。另一方面，在这两种情况下，都存在缓存应紧密协作的问题。大多数协作文件缓存和GMS系统都试图模拟全局LRU替换算法，有时还会使用提示[45]。有趣的是，对于是否需要全局替换算法，我们得出了截然不同的结论[17]。原因是在OS上下文中，全局替换算法用于从空闲工作站窃取内存（即负载均衡缓存），而在Web缓存共享中，每个代理一直都在忙。因此，尽管简单的缓存共享在OS上下文中的性能较差，但只要每个代理的资源配置都适合其负载，就足以满足Web代理缓存共享的需要。最后，请注意，基于Bloom筛选器的摘要缓存技术不限于Web代理缓存上下文，而是可以在其他缓存内容的知识有帮助的情况下使用，例如，在集群中的缓存和负载平衡方面。服务器。</p><h2 id="结论与未来工作"><a href="#结论与未来工作" class="headerlink" title="结论与未来工作"></a>结论与未来工作</h2><p>我们提出了摘要缓存增强的ICP，这是一种可扩展的广域Web缓存共享协议。 使用跟踪驱动的模拟和测量，我们演示了Web代理缓存共享的好处，说明了当前缓存共享协议的开销，并表明摘要缓存方法大大降低了开销。 我们研究了这种方法的两个关键方面：延迟更新的影响以及摘要的简洁表示。 我们的解决方案是基于具有更新延迟阈值的Bloom过滤器的摘要，对内存和带宽的需求较低，但实现了与原始ICP协议相似的命中率。 尤其是，跟踪驱动的仿真表明，与ICP相比，新协议将代理间协议消息的数量减少了25到60，将带宽消耗减少了50％以上，同时几乎不降低代理质量。 缓存命中率。 仿真和分析进一步证明了该协议的可扩展性。</p><p>我们已经在<code>Squid 1.1.14</code>中构建了一个原型实现。 合成和跟踪重播实验表明，除了减少网络流量外，新协议还将CPU开销减少了75％至95％之间，并改善了客户端延迟。 原型实现是公开可用的[15]。</p><p>未来的工作还有很多。 我们计划调查该协议对父子代理合作以及给定工作负载的最佳层次结构配置的影响。 我们还计划研究摘要缓存在各种Web缓存一致性协议中的应用。 最后，摘要缓存可用于单个代理实现中以加快缓存查找的速度，我们将通过修改代理实现来量化效果。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] J. Almeida and P. Cao. (1997) Wisconsin proxy benchmark 1.0. [Online]. Available: <a target="_blank" rel="noopener" href="http://www.cs.wisc.edu/~cao/wpbl.0.html">http://www.cs.wisc.edu/~cao/wpbl.0.html</a><br>[2] T.E. Anderson, M. D. Dahlin, J. M. Neefe, D. A. Patterson, D. S. Roselli,and R. Y. Wang,”Serverless network file systems,” in Proc. 15th ACM Syrup. Operating Syst. Principles,Dec. 1995.<br>[3] M. Arlitt, R. Friedrich, and T. Jin, “Performance evaluation of Web proxy cache replacement policies,” in Proc. Performance Tools’98, Lecture Notes in Computer Science, 1998, vol. 1469, pp. 193-206.<br>[4] M. Arlitt and C. Williamson, “Web server workload characterization,” in Proc. 1996 ACM SIGMETRICS Int. Conf. Measurement and Modeling of Computer Systems, May 1996.<br>[5] A. Baggio and G. Pierre. Oleron: Supporting information sharing in large-scale mobile environments, presented at ERSADS Workshop, Mar.[Online]. Available: <a target="_blank" rel="noopener" href="http://www-sor.inria.fr/projects/relais/">http://www-sor.inria.fr/projects/relais/</a><br>[6] K. Beck. Tennessee cache box project, presented at 2nd Web Caching Workshop, Boulder, CO, June 1997. [Online]. Available: <a target="_blank" rel="noopener" href="http://ircache.nlanr.net/Cache/Workshop97/">http://ircache.nlanr.net/Cache/Workshop97/</a><br>[7] B. Bloom, “Space&#x2F;time trade-offs in hash coding with allowable errors,” Commun. ACM, vol. 13, no. 7, pp. 422-426, July 1970.<br>[8] L. Breslau, P. Cao, L. Fan, G. Phillips, and S. Shenker, “Web caching and zipf-like distributions: Evidence and implications,” in Proc. IEEE INFOCOM, 1999.<br>[9] A. Z. Broder, “Some applications of Rabin’s fingerprinting method,” in Sequences 11: Methods in Communications, Security, and Computer Science, R. Capocelli, A. De Santis, and U. Vaccaro, Eds. New York, NY: Springer-Verlag, 1993, pp. 143-152.<br>[10] P. Can and S. Irani, “Cost-aware WWW proxy caching algorithms,” in Proc. 1997 USEN1X Symp. lnternet Technology and Systems, Dec. 1997, <a target="_blank" rel="noopener" href="http://www.cs.wisc.edu/~cao/papers/gd-size.html">http://www.cs.wisc.edu/~cao/papers/gd-size.html</a>, pp. 193-206.<br>[11] M. Crovella and A. Bestavros, “Self-similiarity in world wide web traffic: Evidence and possible causes,” in Proc. 1996 Sigmetrics Conf. Measurement and Modeling of Computer Systems, Philadelphia, PA, May 1996.<br>[12] C. R. Cunha, A. Bestavros, and M. E. Crovella, “Characteristics of WWW client-based traces,” Boston University, Boston, MA, Tech. Rep. BU-CS-96-010, Oct. 1995.<br>[13] M. D. Dahlin, R. Y. Wang, T. E. Anderson, and D. A. Patterson, “Cooperative caching: Using remote client memory to improve file system performance,” in Proc. 1st USENIX Symp. Operating Systems Design and Implementation, Nov. 1994, pp. 267-280.<br>[14] P. B. Danzig, R. S. Hall, and M. E Schwartz, “A case for caching file objects inside internetworks,” in Proc. S1GCOMM, 1993, pp. 239-248.<br>[15] E Douglis, A. Feldmann, B. Krishnamurthy, and J. Mogul, “Rate of change and other metrics: A live study of the world wide web,” in Proc. USENIX Symp. lnternet Technology and Systems, Dec. 1997.<br>[16] B.M. Duska, D. Marwood, and M. J. Feeley, “The measured access characteristics of world-wide-web client proxy caches,” in Proc. USENIX Symp. lnternet Technology and Systems, Dec. 1997.<br>[17] L. Fan, P. Cao, and J. Almeida. (1998, Feb.) A prototype implementation of summary-cache enhanced icp in Squid 1.1.14. [Online]. Available: <a target="_blank" rel="noopener" href="http://www.cs.wisc.edu/~cao/sc-icp.html">http://www.cs.wisc.edu/~cao/sc-icp.html</a><br>[18] L. Fan, P. Cao, J. Almeida, and A. Z. Broder, “Summary cache: A scalable wide-area web cache sharing protocol,” in Proc. ACM SIGCOMM,<br>[19] –, (1998, Feb.) Summary cache: A scalable wide-area web cache sharing protocol. Tech. Rep. 1361, Computer Science Department, University of Wisconsin-Madison. [Online]. Available: <a target="_blank" rel="noopener" href="http://www.cs.wisc.edu/-cao/papers/summarycache.html">http://www.cs.wisc.edu/-cao/papers/summarycache.html</a><br>[20] M.J. Feeley, W. E. Morgan, E H. Pighin, A. R. Karlin, H. M. Levy, and C. A. Thekkath, “Implementing global memory management in a workstation cluster,” in Proc. 15th ACM Symp. Operating Systems Principles, Dec. 1995.<br>[21] ICP working group. (1998). National Lab for Applied Network Research. [Online]. Available: <a target="_blank" rel="noopener" href="http://ircache.nlanr.neticache/ICP/">http://ircache.nlanr.netICache/ICP/</a><br>[22] A. Fox, S. D. Gribhle, Y. Chawathe, E. A. Brewer, and P. Gauthier<br>[23] S. Gadde, M. Rabinovich, and J. Chase. Reduce, reuse, recycle: An approach to building large internet caches, presented at 6th Workshop Hot Topics in Operating Systems (HotOS VI), May 1997. [Online]. Available: <a target="_blank" rel="noopener" href="http://www.research.att.com/-misha/">http://www.research.att.com/-misha/</a><br>[24] G. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data Structures. Reading, MA: Addison-Wesley, 1991.<br>[25] S. Gribble and E. Brewer, “System design issues for intemet middleware service: Deduction from a large client trace,” in Proc. USENIX Symp.Internet Technology and Systems, Dec. 1997.<br>[26] –, (1997, June) UCB home IP HTTP traces. [Online]. Available:<a target="_blank" rel="noopener" href="http://www.cs.berkeley.edu/~gribble/traces/index.html">http://www.cs.berkeley.edu/~gribble/traces/index.html</a><br>[27] C. Grimm. The dfn cache service in B-WiN. presented at 2nd Web Caching Workshop, Boulder, CO, June 1997. [Online]. Available: <a target="_blank" rel="noopener" href="http://www-cache.dfn.de/CacheEN/">http://www-cache.dfn.de/CacheEN/</a><br>[28] The Harvest Group. (1994) Harvest Information Discovery and Access System. [Online]. Available: <a target="_blank" rel="noopener" href="http://excalibur.usc.edu/">http://excalibur.usc.edu/</a><br>[29] The Relais Group. (1998) Relais: Cooperative caches for the world-wide web. [Online]. Available: <a target="_blank" rel="noopener" href="http://www-sor.inria.fr/projects/relais/">http://www-sor.inria.fr/projects/relais/</a><br>[30] J. Gwertzman and M. Seltzer, “World-wide web cache consistency,” in Proc. 1996 USENIX Tech. Conf., San Diego, CA, Jan. 1996.<br>[31] IRCACHE. (1999, Mar.) Benchmarking Proxy Caches with Web Polygraph. [Online].Available: <a target="_blank" rel="noopener" href="http://www.polygraph.ircache.net/slides/">http://www.polygraph.ircache.net/slides/</a><br>[32] V. Jacobson. How to kill the internet, presented at SIGCOMM’95 Middleware Workshop, Aug. 1995. [Online]. Available: <a href="ftp://ftp.ee.lhl/">ftp://ftp.ee.lhl</a> .gov&#x2F;talks&#x2F;vj -webflame.ps.Z<br>[33] J. Jung. Nation-wide caching project in korea, presented at 2nd Web Caching Workshop, Boulder, CO, June 1997. [Online]. Available: <a target="_blank" rel="noopener" href="http://ircache.nlanr.net/Cache/Workshop97/">http://ircache.nlanr.net/Cache/Workshop97/</a><br>[34] B. Krishnamurthy and C. E. Ellis, “Study of piggyback cache validation for proxy caches in the world wide web,” in Proc. USENIX Symp. lnternet Technology and Systems, Dec. 1997.<br>[35] T. M. Kroeger, J. Mogul, and C. Maltzahn. (1996, Aug.) Digital’s web proxy traces. [Online]. Available: <a href="ftp://ftp.digital.com/pub/DEC/traces/proxy/webtraces.html">ftp://ftp.digital.com/pub/DEC/traces/proxy/webtraces.html</a><br>[36] T.M. Kroeger, D. D. E. Long, and J. C. Mogul, “Exploring the bounds of web latency reduction from caching and prefetching,” in Proc. USEN1X Syrup. lnternet Technology and Systems, Dec. 1997.<br>[37] C. Liu and P. Cao, “Maintaining strong cache consistency for the world-wide web,” presented at the 17th Int. Conf. Distributed Computing Systems, May 1997.<br>[38] P. Lorenzetti, L. Rizzo, and L. Vicisano. (1996, Oct.) Replacement policies for a proxy cache. Universita di Pisa, Italy. [Online]. Available: <a target="_blank" rel="noopener" href="http://www.iet.unipi.it/~luigi/caching.ps.gz">http://www.iet.unipi.it/~luigi/caching.ps.gz</a><br>[39] C. Maltzahn, K. Richardson, and D. Grunwald, “Performance issues of enterprise level web proxies,” in Proc. 1997 ACM SIGMETRICS Int. Conf. Measurement and MOdeling of Computer Systems, June 1997, pp. 13-23.<br>[40] J. Marais and K. Bharat. Supporting cooperative and personal surfing with a desktop assistant, presented at ACM UIST’97. [Online]. Available: <a href="ftp://ftp.digital.com/pub/DEC/SRC/publications/marais/uist97paper.pdf">ftp://ftp.digital.com/pub/DEC/SRC/publications/marais/uist97paper.pdf</a>.<br>[41] A. J. Menezes, P. C. van Oorschot, and S. A. Vanstone, Handbook of Applied Cryptography: CRC Press, 1997.<br>[42] J. C. Mogul, E Douglis, A. Feldmann, and B. Krishnamurthy. Potential benefits of delta encoding and data compression for http.presented at ACM SIGCOMM’97. [Online]. Available: <a target="_blank" rel="noopener" href="http://www.research.att.com/~douglis/">http://www.research.att.com/~douglis/</a><br>[43] National Lab of Applied Network Research. (1997, July) Sanitized Access Log.[Online].Available: <a href="ftp://ircache.nlanr.netltraces/">ftp://ircache.nlanr.netlTraces/</a><br>[44] J. Pietsch. Caching in the Washington State k-20 network, presented at2nd Web Caching Workshop, Boulder, CO, June 1997. [Online]. Available: http:&#x2F;lircache.nlanr.net&#x2F;CachelWorkshop97&#x2F;<br>[45] M. O. Rabin, “Fingerprinting by random polynomials,” Center for Research in Computing Technology, Harvard Univ., Tech. Rep. TR-15-81,1981.<br>[46] A. Rousskov. (1998, Apr.) Cache digest. [Online]. Available: <a target="_blank" rel="noopener" href="http://squid.nlanr.net/Squid/CacheDigest/">http://squid.nlanr.net/Squid/CacheDigest/</a><br>[47] P. Sarkar and J. Hartman, “Efficient cooperative caching using hints,”in Proc. USENIX Conf. Operating System Design and Implementations,Oct. 1996.<br>[48] V. Valloppillil and K. W. Ross. (1997) Cache array routing protocol vl.0. [Online]. Available: http:l&#x2F;ircache.nlanr.net&#x2F;CachelICP&#x2F;draftvinod-carp-v 1-02.tx<br>[49] D. Wessels and K. Claffy. (1998) Internet cache protocol (ICP) v.2. [Online]. Available: <a target="_blank" rel="noopener" href="http://ds.internic.net/rfc/rfc2186.txt">http://ds.internic.net/rfc/rfc2186.txt</a><br>[50] S. Williams, M. Abrams, C. R. Stanbridge, G. Abdulla, and E. A. Fox. Removal policies in network caches for world-wide web documents, presented at ACM SIGCOMM’96. [Online]. Available: <a target="_blank" rel="noopener" href="http://ei.cs.vt.edu/~succeed/96sigcomm/">http://ei.cs.vt.edu/~succeed/96sigcomm/</a><br>[51] L. Zhang, S. Floyd, and V. Jacobson. Adaptive web caching, presented at<br>2nd Web Caching Workshop, Boulder, CO, June 1997. [Online]. Available: <a target="_blank" rel="noopener" href="http://ircache.nlanr.net/Cache/Workshop97/Papers/Floyd/floyd">http://ircache.nlanr.net/Cache/Workshop97/Papers/Floyd/floyd</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://bugwz.com">bugwz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://bugwz.com/2020/05/23/bloom-filter-summary-cache-paper/">https://bugwz.com/2020/05/23/bloom-filter-summary-cache-paper/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://bugwz.com" target="_blank">咕咕</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法</a><a class="post-meta__tags" href="/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/">布隆过滤器</a></div><div class="post-share"><div class="social-share" data-image="/assets/images/bg/paper.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="/pluginsSrc/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="/pluginsSrc/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2020/05/21/codis-slots-rebalance/" title="Codis的Slots-Rebalance算法"><img class="cover" src="/assets/images/bg/redis.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post" loading='lazy'><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Codis的Slots-Rebalance算法</div></div><div class="info-2"><div class="info-item-1">Codis 实现了另一种的 Redis 集群方案。在该方案中为了能够实现类似于 RedisCluster 的横向扩缩容的能力，Codis 内部实现了一种 Slot-Rebalance 的算法，该算法中所有的 key 都被哈希到 1024 个 slots 上，在每个 slots 分配均匀的前提下，如果一个分片中的 slots 过多，该分片中存储的 key 的数量也就越多，该分片对应的负载也就越大，在扩缩容之后为了保证集群中各分片的负载均衡，需要调整分片的 slots...</div></div></div></a><a class="pagination-related" href="/2020/05/24/memcached-slab-calcification/" title="Memcached的钙化及相关解决方案"><img class="cover" src="/assets/images/bg/cache.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post" loading='lazy'><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Memcached的钙化及相关解决方案</div></div><div class="info-2"><div class="info-item-1">一、背景由于Memcached存储数据的时候是按照Slabs分类进行存储的，当内存达到Memcached限制的时候，服务进程会执行一系列的内存回收方案，但是，不管是什么内存回收方案，回收的大前提就只有一种：只回收与即将写入数据写入数据块一致的Slabs。因此，这就导致了在业务访问模型变更之后，Memcached对于之前访问模型存储的数据就不会做任何变更，也就是说那部分数据永不会被剔除，因此最终服务可用的内存也会远小于进程启动时的设定，这种情况就被称为Memcached的Slab钙化现象（Slab Calcification）。 在Memcached的 1.4.11 版本之前，官方版本一直存在内存钙化的问题，在这个过程中，Twitter基于Memcached 1.4.4的版本推出了 Twemcache 尝试解决了Slab钙化的问题。在1.4.11版本中，官方引入了 Slab 的 Automove &amp; Rebalance 的策略也解决了内存钙化的问题。 二、 Automove &amp; Rebalance 策略2.1、概念简介通过检测每个Slab...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2022/03/01/scalable-bloom-filters/" title="译 - Scalable Bloom Filters"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-01</div><div class="info-item-2">译 - Scalable Bloom Filters</div></div><div class="info-2"><div class="info-item-1">《Scalable Bloom Filters》 这篇论文讲述了一种布隆过滤器的变体实现方式，通过将预设的误判率分配给多个子布隆过滤器来约束整体的一个误判率情况，并且可以通过新增子布隆过滤器来实现对存储元素数量的调节，以满足初始容量无法准确估计的情况，论文中详细介绍了在不同的误判率变化率以及布隆过滤器容量变化率的情况下，存储空间等的使用情况。目前了解到的，RedisBloom 和 TairBloom 都参考了这篇论文实现了各自的布隆过滤器。 摘要Bloom Filters provide space-efficient storage of sets at the cost of a probability of false positives on membership queries. The size of the filter must be defined a priori based on the number of elements to store and the desired false positive probability, being...</div></div></div></a><a class="pagination-related" href="/2019/10/20/the-rsync-algorithm/" title="译 - The rsync algorithm"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2019-10-20</div><div class="info-item-2">译 - The rsync algorithm</div></div><div class="info-2"><div class="info-item-1">《The rsync algorithm》这篇发表于 1996 年的论文中介绍了一种名为 rsync 的增量同步算法，它能够快速地将两个文件夹中的内容同步。该算法利用了文件的局部性和差异性，通过计算文件的弱校验和和块校验和来确定文件的相似性，并进行增量同步。该算法具有高效性、可靠性和安全性等优点，在实际应用中被广泛使用。 0、摘要This report presents an algorithm for updating a file on one machine to be identical to a file on another machine. We assume that the two machines are connected by a low-bandwidth high-latency bi-directional communications link. The algorithm identifies parts of the source file which are identical to some part of the...</div></div></div></a><a class="pagination-related" href="/2022/11/16/bitcoin/" title="译 - Bitcoin: A Peer-to-Peer Electronic Cash System"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-16</div><div class="info-item-2">译 - Bitcoin: A Peer-to-Peer Electronic Cash System</div></div><div class="info-2"><div class="info-item-1">《Bitcoin: A Peer-to-Peer Electronic Cash System》 翻译过来是《 比特币：一种点对点的电子现金系统》 ，这篇文章是比特币的发明人中本聪于 2008 年发表的比特币白皮书。这篇文章介绍了比特币的设计背景，讲述了比特币的工作原理，是加密货币，区块链领域必读的一篇文章，其中讲述了很多巧妙的构思。作者翻译水平有限，翻译的语句可能会有一些出入，建议有能力的读者还是去阅读一下原文。 0、摘要A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is...</div></div></div></a><a class="pagination-related" href="/2019/10/14/dynamo/" title="转&#x2F;译-Dynamo:Amazon的高可用键值存储"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2019-10-14</div><div class="info-item-2">转&#x2F;译-Dynamo:Amazon的高可用键值存储</div></div><div class="info-2"><div class="info-item-1">本文翻译自 2007 年 Amazon 的分布式存储经典论文：《Dynamo: Amazon’s Highly Available Key-value Store》)，直译为 《Dynamo：Amazon 的高可用键值存储》，这里对排版做了一些调整，以更适合 web 阅读。 Dynamo 是 Amazon 的高可用分布式键值存储（key&#x2F;value storage）系统。这篇论文发表 的时候（2007）它还只是一个内部服务，现在（改名为 DynamoDB）已经发展成 AWS 最核心 的存储产品（服务）之一，与 S3 等并列。据了解，国内某一线大厂的公有云键值 存储服务，也是参考这篇文章设计和实现的。 现在提到键值存储，大家首先想到的可能是 Redis，那么 Dynamo 和 Redis 是不是竞品， 只是一个开源一个是商业的？不是的，二者针对的场景不同，这里非常粗地列举几方面： 使用场景：Dynamo 定位是永远可写（always writable）的持久文件系统，Redis 主要用作（易失）缓存或内存数据库 存储方式：Dynamo 是磁盘，Redis...</div></div></div></a><a class="pagination-related" href="/2023/06/20/crush/" title="译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-20</div><div class="info-item-2">译 - CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</div></div><div class="info-2"><div class="info-item-1">译作: 可控的、可扩展的、分布式的副本数据放置算法，论文原文 。 该论文于 2006 年 11 月发布于 SC2006 。 CRUSH 是一种用于大规模分布式存储系统的数据分布算法，它通过伪随机函数将数据对象映射到存储设备上，无需依赖中央目录。CRUSH 算法设计考虑了系统的动态性，支持在添加或移除存储设备时高效地重组数据，并最小化不必要的数据移动。此外，CRUSH 支持多种数据复制和可靠性机制，并允许根据用户定义的策略进行数据分布，这些策略能够在故障域之间有效地分离副本，增强数据安全性。 CRUSH 的核心是其层级集群图，该图描述了存储集群的物理和逻辑结构，并通过一系列规则来确定数据的放置位置。CRUSH 算法通过将数据均匀分布在加权设备上，保持存储和设备带宽资源的平衡利用。算法还考虑了设备的故障和过载情况，能够在设备发生故障或过载时重新分配数据，避免数据丢失并优化系统性能。 CRUSH 的映射性能高效，计算复杂度为 O(logn) ，适用于管理大规模（多 PB...</div></div></div></a><a class="pagination-related" href="/2022/09/24/gorilla-cn/" title="译 - Gorilla: A Fast, Scalable, In-Memory Time Series Database"><img class="cover" src="/assets/images/bg/paper.jpg" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-24</div><div class="info-item-2">译 - Gorilla: A Fast, Scalable, In-Memory Time Series Database</div></div><div class="info-2"><div class="info-item-1">《Gorilla: A Fast, Scalable, In-Memory Time Series Database》 这篇论文讲述了 Facebook 在存储时序数据模型时的一些实践，重点讲述了他们内部的一款内存型的时序数据库 Gorilla。论文中通过使用 Delta-Of-Delta 和 XOR 方式分别对时序数据的时间戳以及浮点数据进行压缩编码，极大的节省了时序数据的存储开销，这也成为了业界时序数据库主流的数据编码压缩方式。这篇论文是时序数据库领域必读的一篇文章。 摘要Large-scale internet services aim to remain highly available and responsive in the presence of unexpected failures. Providing this service often requires monitoring and analyzing tens of millions of measurements per second across a large number...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/assets/images/bg/avatar-256.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"/ loading='lazy'></div><div class="author-info-name">bugwz</div><div class="author-info-description">持续学习，持续进步</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">134</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">135</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/bugwz" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%A8%E8%BF%B9%E5%92%8C%E6%A8%A1%E6%8B%9F"><span class="toc-text">轨迹和模拟</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%93%E5%AD%98%E5%85%B1%E4%BA%AB%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-text">缓存共享的好处</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ICP%E7%9A%84%E5%BC%80%E9%94%80"><span class="toc-text">ICP的开销</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81%E7%BC%93%E5%AD%98"><span class="toc-text">摘要缓存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0%E5%BB%B6%E8%BF%9F%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">更新延迟的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%91%98%E8%A6%81%E8%A1%A8%E7%A4%BA"><span class="toc-text">摘要表示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-%E6%95%B0%E5%AD%A6"><span class="toc-text">布隆过滤器-数学</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E4%BD%9C%E4%B8%BA%E6%91%98%E8%A6%81"><span class="toc-text">布隆过滤器作为摘要</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E9%85%8D%E7%BD%AE"><span class="toc-text">推荐配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="toc-text">可扩展性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81%E7%BC%93%E5%AD%98%E5%A2%9E%E5%BC%BA%E5%9E%8BICP%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">摘要缓存增强型ICP的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E8%AE%AE%E4%B9%A6"><span class="toc-text">协议书</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">原型实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-text">实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text">相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="toc-text">结论与未来工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-text">参考文献</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/09/cephfs-inode-num/" title="CephFS Inode 编号的申请与释放"><img src="/assets/images/bg/ceph.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="CephFS Inode 编号的申请与释放"/ loading='lazy'></a><div class="content"><a class="title" href="/2025/08/09/cephfs-inode-num/" title="CephFS Inode 编号的申请与释放">CephFS Inode 编号的申请与释放</a><time datetime="2025-08-08T16:00:00.000Z" title="发表于 2025-08-09 00:00:00">2025-08-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析"><img src="/assets/images/bg/ceph.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Ceph Crimson 设计实现深入解析"/ loading='lazy'></a><div class="content"><a class="title" href="/2025/06/01/ceph-cirmson/" title="Ceph Crimson 设计实现深入解析">Ceph Crimson 设计实现深入解析</a><time datetime="2025-05-31T16:00:00.000Z" title="发表于 2025-06-01 00:00:00">2025-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/23/3fs-deploy/" title="3FS 集群部署笔记"><img src="/assets/images/bg/deepseek.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="3FS 集群部署笔记"/ loading='lazy'></a><div class="content"><a class="title" href="/2025/05/23/3fs-deploy/" title="3FS 集群部署笔记">3FS 集群部署笔记</a><time datetime="2025-05-22T16:00:00.000Z" title="发表于 2025-05-23 00:00:00">2025-05-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群部署教程"><img src="/assets/images/bg/ceph.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Ceph Crimson 集群部署教程"/ loading='lazy'></a><div class="content"><a class="title" href="/2025/01/12/ceph-crimson-deploy/" title="Ceph Crimson 集群部署教程">Ceph Crimson 集群部署教程</a><time datetime="2025-01-11T16:00:00.000Z" title="发表于 2025-01-12 00:00:00">2025-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/01/cephfs-samba/" title="CephFS 对接 Samba 使用教程"><img src="/assets/images/bg/ceph.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="CephFS 对接 Samba 使用教程"/ loading='lazy'></a><div class="content"><a class="title" href="/2024/12/01/cephfs-samba/" title="CephFS 对接 Samba 使用教程">CephFS 对接 Samba 使用教程</a><time datetime="2024-11-30T16:00:00.000Z" title="发表于 2024-12-01 00:00:00">2024-12-01</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(/assets/images/bg/paper.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By bugwz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const e=()=>{(()=>{const e=document.querySelectorAll("pre > code.mermaid");0!==e.length&&e.forEach(e=>{const t=document.createElement("pre");t.className="mermaid-src",t.hidden=!0,t.textContent=e.textContent;const n=document.createElement("div");n.className="mermaid-wrap",n.appendChild(t),e.parentNode.replaceWith(n)})})();const e=document.querySelectorAll("#article-container .mermaid-wrap");if(0===e.length)return;const t=()=>(e=>{window.loadMermaid=!0;const t="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";e.forEach((e,n)=>{const d=e.firstElementChild,a=`mermaid-${n}`,r=`%%{init:{ 'theme':'${t}'}}%%\n`+d.textContent,m=mermaid.render(a,r),o=e=>{d.insertAdjacentHTML("afterend",e)};"string"==typeof m?o(m):m.then(({svg:e})=>o(e))})})(e);btf.addGlobalFn("themeChange",t,"mermaid"),window.loadMermaid?t():btf.getScript("/pluginsSrc/mermaid/dist/mermaid.min.js").then(t)};btf.addGlobalFn("encrypt",e,"mermaid"),window.pjax?e():document.addEventListener("DOMContentLoaded",e)})()</script><script>(()=>{const t="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,e=null,n=t=>{const e=document.querySelector("#post-meta .gitalk-comment-count");e&&(e.textContent=t)},i=(i,o)=>{t&&(window.shuoshuoComment.destroyGitalk=()=>{i.children.length&&(i.innerHTML="",i.classList.add("no-comment"))});new Gitalk({clientID:"6af3be16b94cec39bcf6",clientSecret:"13a5202ff773ffcea6300b6c8ff25f455566737c",repo:"bugwz.github.io",owner:"bugwz",admin:["bugwz"],updateCountCallback:n,...e,id:t?o:"23e4ed715f68fe8c5e66628b36c11202"}).render("gitalk-container")},o=async(t,e)=>{"function"==typeof Gitalk||(await btf.getCSS("/pluginsSrc/gitalk/dist/gitalk.css"),await btf.getScript("/pluginsSrc/gitalk/dist/gitalk.min.js")),i(t,e)};t?window.shuoshuoComment={loadComment:o}:o()})()</script></div><div class="docsearch-wrap"><div id="docsearch" style="display:none"></div><script>(()=>{let e=!1;const c=()=>{if(e)return;e=!0;const c=document.createElement("link");c.rel="stylesheet",c.href="/pluginsSrc/@docsearch/css/dist/style.css",c.media="print",c.onload=()=>{c.media="all"},document.head.appendChild(c),btf.getScript("/pluginsSrc/@docsearch/js/dist/umd/index.js").then(()=>{docsearch(Object.assign({appId:"PFB3WGSSCO",apiKey:"3e9cd446e41d93f2f130b91698b699f7",indexName:"bugwz",container:"#docsearch",placeholder:"请输入要搜索的内容"},{maxResultsPerGroup:10}));const e=document.querySelector(".DocSearch-Button");e&&e.click()})},t=()=>{const e=document.querySelector("#search-button > .search");e&&btf.addEventListenerPjax(e,"click",c)};t(),window.addEventListener("pjax:complete",t)})()</script></div></div></body></html>